---
title: "Feature Engineering"
format: 
  html: 
    fontsize: 25px
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

::: {style="color: yellow"}
## Overview of Feature Engineering
:::

Feature engineering in machine learning typically describes the process of creating, transforming, or selecting variables (features) from raw data to improve a model’s performance.

For example, one aspect of feature engineering is feature creation. This often means transforming raw data into meaningful inputs that better capture the underlying patterns the model needs to learn. For example, suppose we are trying to predict medicine adherence (whether or not someone takes their medicine). Our raw data may contain timestamps of when someone actually takes their medicine. We might use this timestamp to create new features for our model. For example, we might add features representing *day of the week*, *holiday* or *weekend* to provide additional context to the model.

**A number of domains fall under feature engineering:**

-   Missing data handling

-   Feature creation

-   Feature transformations

-   Screening or feature selection

-   Dimension reduction

::: {style="color: orange"}
### Feature Engineering and Data Leakage
:::

Data leakage occurs when information from outside the training data set is used to create the model.

Data leakage often occurs during feature engineering.

To minimize data leaking, we will often want to do our feature engineering during the resampling, or data splitting, procedure we are using. To visualize this take a look at the graphic below from @handson where the pre-processing, or data engineering tasks, occur during each iteration. Keep this in mind as we introduce each feature engineering task.

![https://bradleyboehmke.github.io/HOML/images/minimize-leakage.png](images/minimize-leakage.png){width="800"}

::: {style="color: orange"}
### Outline of Remaining Sections
:::

1.  Example Data

2.  Addressing Missing Data

::: {style="color: yellow"}
## Example Data
:::

To demonstrate feature engineering we will use some example data from the [The General Social Survey (GSS)](https://gss.norc.org/).

GSS is a long-running, nationally representative survey of adults in the United States that has been conducted almost every two years since 1972 by the [National Opinion Research Center (NORC)](https://www.norc.org/) at the University of Chicago. The GSS data is often used to measure American's attitudes, behaviors, and beliefs on a wide range of topics—such as politics, religion, crime, race relations, family, work, and technology.

There are two ways to access the GSS data. One is using the [GSS Data Explorer](#0). Another option is using the `gssr` [@gssr] R package. Here we will use the `gssr` [@gssr] package to download some example data.

We will start by choosing `happy` as our target variable. This comes from Question 157 of the 2024 GSS where respondents were asked:

> Taken all together, how would you say things are these days - would you say that you are very happy, pretty happy, or not too happy?

Responses were coded such that 1 indicates "very happy", 2 indicates "pretty happy", and 3 indicates "not too happy", while NA indicates "don’t know."

We can also identify ad bunch of features we think predict self-reported happiness and save our final dataset.

```{r}
library(gssr)

gss24 <- gss_get_yr(2024)

target <- "happy"

features <- c(
  "age",     # age of participant
  "sex",     # sex of participant
  "race",    # race of participant
  "educ",    # highest education completed by participant
  "income",  # income of participant
  "childs",  # number of children participant has
  "wrkstat", # work force status
  "marital", # marital status
  "born",    # whether or not participant was born in USA
  "partyid", # political party of participant
  "adults",  # num of fam members 18 or older
  "earnrs"   # number of earners in family  
)

data <- gss24[,c(target, features)]

table(data$happy, useNA = "always")
```

Next, let's clean up `data` a bit, discarding some of the labels and missing value information we don’t need. The data in `gss24` retains the labeling structure provided by the GSS. Variables are stored numerically with labels attached to them. Often, when using the data in R, it will be convenient to convert the categorical variables we are interested in to character or factor type instead.

Here we can use code from the [gssr package introduction](https://kjhealy.github.io/gssr/articles/overview.html) to simplify this recoding. The only thing we need to do is define the categorical variables in our data.

```{r}
library(dplyr)

cat_vars <- c("sex","race", "educ", "wrkstat", "marital", "born", "partyid")

capwords <- function(x, strict = FALSE) {
    cap <- function(x) paste(toupper(substring(x, 1, 1)),
                  {x <- substring(x, 2); if(strict) tolower(x) else x},
                             sep = "", collapse = " " )
    sapply(strsplit(x, split = " "), cap, USE.NAMES = !is.null(names(x)))
}

data <- data |>
  mutate(
    # Convert all missing to NA
    across(everything(), haven::zap_missing), 
    # Make all categorical variables factors and relabel nicely
    across(all_of(cat_vars), forcats::as_factor),
    across(all_of(cat_vars), \(x) forcats::fct_relabel(x, capwords, strict = TRUE))
  )
```

::: {style="color: yellow"}
## Addressing Missing Data
:::

Dealing with missing data in a consistent manner is one of the most important aspects of feature engineering.

::: {style="color: orange"}
### Missing Data Mechanisms
:::

Much attention is paid to the mechanisms producing missing data in the statistics and applied sciences literature.

Knowing how the missing data came about is critical for knowing how to handle it in a subsequent analysis.

Although attention to missing data mechanisms has historically not been a primary focus in the ML literature, this has been changing.

A common framework for understanding missing data mechanisms was described by Rubin [-@rubin1976]. Here we will briefly describe these mechanisms in the context of our current data example.

::: {style="color: lightyellow"}
#### Missing Completely at Random (MCAR)
:::

**In MCAR, the probability of a value being missing is unrelated to the value itself or any other observed or unobserved variable.** This is a purely random and unsystematic process. 

Imagine a cat opening up your dataset in Excel and walking across the keyboard, randomly deleting different cells.

**Definition:** Missingness in self-reported happiness, for example, is unrelated to the respondent's true happiness level or any other variables.

**Example:** Let's say there is a glitch in the online GSS survey and for some respondent's questions are randomly skipped. This means the probability of missingness is purely random, and it does not depend on other variables (e.g. income, age, or happiness levels).

**Implication:** Dropping these cases (listwise deletion) or using simple imputation will not bias the results, although efficiency is reduced.

::: {style="color: lightyellow"}
#### Missing at Random (MAR)
:::

**In MAR, the probability of a value being missing is systematically related to other observed variables in the dataset, but not to the unobserved value itself.**

**Definition:** Missingness in happiness depends on *other observed variables* but not directly on happiness itself.

**Example:** Suppose in the GSS, individuals with higher incomes were less likely to answer the happiness question. Missingness on happiness is explained by *income*, which is observed. Conditional on income (having income in our model as a predictor), the probability of missingness does not depend how happy one is.

**Implication:** Methods like multiple imputation (MICE), missForest, or regression-based imputation can use income (and other observed covariates like marital status, education) to predict and impute missing happiness values without bias.

::: {style="color: lightyellow"}
#### Missing Not at Random (MNAR)
:::

**Definition:** Missingness in happiness depends on the unobserved happiness score itself.

**Example:** Respondents who are very unhappy may avoid answering the happiness question because it feels too personal, while those who are extremely happy may skip it because they consider it obvious. Missingness is directly tied to the unreported happiness level.

**Implication:** Standard imputation methods will be biased. Handling MNAR requires explicitly modeling the missingness mechanism (e.g., selection models, pattern-mixture models)

::: {style="color: orange"}
### Handling Missing Data in R
:::

Often a first step in handling missing data involves recoding missing values as `NA`. Writing bespoke code to handle the different types of missing data one might encounter is tedious and unnecessary.

The `naniar` [@naniar] package in R contains many convenience functions for managing missing data in R. Here we demonstrate some of that functionality.

::: {style="color: lightyellow"}
#### Recoding Values with `NA`
:::

Now that we have a dataset with missing values we can use `naniar` to recode these values to `NA`. In our current data example this is already done, but this code might be useful for other projects where you import data

```{r}
library(naniar)

gss_na_codes <- c("<NA>", -99, -999, "NA")
  
data <- naniar::replace_with_na_all(
  data, condition = ~.x %in% gss_na_codes
)
```

See the [`naniar` vignette on recoding NA values](https://cran.r-project.org/web/packages/naniar/vignettes/replace-with-na.html) for more detailed information on the package functionality.

::: {style="color: lightyellow"}
#### Missing Data Visualization
:::

Once we have recoded our data in a consistent manner we can use visualizations to explore the missing data. The `vis_miss()` function from `naniar` is a good starting point for visualizing the amount of missing data in our dataset. The plots shows the missing values in black and non-missing values in gray. In addition, percentages of missing data in both the dataset and individual variables are provided.

```{r}
naniar::vis_miss(data)
```

It is often useful to look at combinations of missingness among different variables.

```{r}
naniar::gg_miss_upset(data)
```

We can also look at the percentage of missing data across a factor variable.

```{r}
naniar::gg_miss_fct(x = data, fct = marital)
```

Many missing data visualizations are described in the [`naniar` vignette on missing data visualization](https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html) including plots for exploring missing data mechanisms.

::: {style="color: orange"}
### Missing Data Activity
:::

Below is a small dataset looking at predictors of happiness. Some values are missing.

In small groups please speculate on why each value might be missing and which type of missing data mechanism it represents: MCAR, MAR, or MNAR.

```{r}
# Columns: id, happiness (target), age (feature), income (feature), education (feature)
# NA indicates missing values

example_data <- data.frame(
  id = 1:15,
  happiness = c(NA, 8, 5, NA, 6, 9, 4, 7, NA, 5, 6, 8, 7, NA, 4),
  age = c(25, NA, 30, 40, 22, 35, NA, 29, 31, 28, 34, NA, 27, 33, 26),
  income = c(50000, 55000, 6500, 70000, NA, NA, NA, 62000, NA, 45000, 52000, NA, 58000, NA,61000),
  education = c("Bachelor","Bachelor","Master","Master","HighSchool","Bachelor","HighSchool","Master","Bachelor","HighSchool","Master","Master","Bachelor","HighSchool","Bachelor")
)

example_data

```

::: {style="color: orange"}
### Handling Missing Data
:::

There are a number of ways to handle missing data. Below I will discuss some of the most common ways of addressing missing data.

::: {style="color: lightyellow"}
#### Listwise Deletion
:::

Listwise deletion (also called complete-case analysis) is one of the simplest methods for handling missing data in a dataset.

When performing listwise deletion we remove any row that has one or more missing values across any variable used in the analysis.

After deletion, only rows that are complete for all variables remain.

For example, our example GSS data has `3,309` rows before we address the missing data. If we only kept rows that contained no missing data we would have `2,780` observations. You can perform listwise deletion on your data using the `complete.cases()` function as demonstrated below. Then you can visualize the missing data to ensure there is no missingness on the new dataset.

```{r}
# nrow(data) # 3,3309 rows

data_cc <- data[complete.cases(data),]
nrow(data_cc)
naniar::vis_miss(data_cc)
```

Now, listwise deletion should really only be used if data is missing completely at random (MCAR). In this case it can still provide unbiased results, although they can be less efficient (reduced power and more uncertainty).

::: {style="color: lightyellow"}
#### Imputation
:::

Imputation is the process of filling in missing values in a dataset with estimated or predicted values so that you can perform analyses without dropping incomplete cases.

Unlike listwise deletion, imputation retains all observations, reducing data loss. Imputation can be simple (deterministic, like a mean) or more involved (stochastic, model-based).

**Estimated Statistic**

A simple approach to imputing missing values for a feature is to compute descriptive statistics such as the mean, median, or mode (for categorical) and use that value to replace the `NA` values.

Although computationally efficient, this approach does not consider any other attributes for a given observation when imputing.

The `tidymodels` [@tidymodels] R package has a number of useful functions for machine learning. Here we use the package to perform mean imputation on our dataset.

```{r}
library(tidymodels)

# create a recipe
rec <- recipe(happy ~ ., data = data) %>%
  step_impute_mean(all_numeric_predictors())  %>% 
  step_impute_mode(all_factor_predictors())
  # apply mean imputation to predictors

# prep the recipe (estimates imputation values)
rec_prep <- prep(rec)

# check the imputed values
data_mi <- bake(rec_prep, new_data = NULL)

naniar::vis_miss(data_mi)
```

**K-Nearest Neighbor**

Another popular method for performing imputation is k-nearest neighbor.

Now, instead of just filling in a missing value with a simple number like the mean, KNN looks for other respondents who are most similar (the “nearest neighbors”) and uses their information to fill in the blank.

**How KNN Imputation Works**

1.  **Measure similarity**
    -   For the student with a missing value, KNN looks at the other variables (educ, martial, partyid).\
    -   It finds the *k* students who are most “similar” (closest in the data space).
2.  **Borrow information**
    -   If we set `k = 3`, the algorithm finds the 3 most similar students.\
3.  **Fill in the blank**
    -   It might take the average of those 3 scores and use that as the imputed value.
    -   Or, depending on settings, it could pick a weighted average (closer neighbors count more).

**Better than Mean Imputation?**

Think of guessing someone’s favorite pizza topping:

-   **Mean imputation**: “Most people like pepperoni, so I’ll assume this person does too.”\
-   **KNN imputation**: This person is very similar to Iris, Naomi, Owen, Naveen, and Stella who all like pineapply. Instead, I’ll assume they probably like pineapple.

```{r}
library(tidymodels)

# create a recipe
rec <- recipe(happy ~ ., data = data) %>%
  step_impute_knn(all_predictors())   
  # apply mean imputation to predictors

# prep the recipe (estimates imputation values)
rec_prep <- prep(rec)

# check the imputed values
data_knn <- bake(rec_prep, new_data = NULL)

naniar::vis_miss(data_knn)
```

**Tree-Based Imputation**

Tree-based imputation methods use decision trees (or random forests) to predict missing values based on the other variables in the dataset. Similar to KNN methods, tree-based methods make a tailored prediction using patterns in the data.

Tree-based methods are especially nice for imputation as they handle non-linearities and interactions, and can accomodate mixed data types, like continuous and categorical variables.

**How Tree-Based Imputation Works**

1.  **Treat the missing variable like an outcome**
    -   Suppose some people are missing their *income*.
    -   A tree is trained to predict income from other available variables (like education, age, occupation).
2.  **Learn patterns in the data**
    -   The tree splits the dataset into groups that have similar income levels.
    -   For example:
        -   *If education \> 16 years → higher income group*
        -   *If education ≤ 16 and occupation = retail → lower income group*
3.  **Predict the missing values**

-   For a person with missing income, the model uses their other information (education, age, etc.) to drop them into the right “leaf” of the tree.
-   The average (or majority class, if categorical) in that leaf is used as the imputed value.

**Better than Mean Imputation?**

Think of tree-based imputation like asking:

> *“Given your age, job, and education, people like you usually make about X — so we’ll use that as your missing value.”*

This is smarter than saying *“everyone makes the same average income”* (mean imputation) or even *“let’s just look at your 3 closest neighbors”* (KNN).

::: {style="color: yellow"}
## Feature Filtering
:::

Feature filtering is a feature selection technique in machine learning where features are evaluated prior to the model fitting based on statistical or heuristic criteria. Features are then kept or discarded based on those criteria.

It is called filtering because it is a preprocessing step, part of a feature engineering pipeline, done before we actually train the model.

Typically, the goals of feature filtering are:

1.  **Reduce dimensionality:** Fewer features means simpler models, faster training, and less risk of overfitting.

2.  **Remove irrelevant or redundant features:** Avoids feeding the model noisy or useless inputs.

3.  **Improve interpretability:** Models are easier to understand when they focus only on meaningful features.

4.  **Boost performance:** Better generalization on new data when unnecessary features are excluded.

In practice, one of the filtering tasks we will typically conduct involves weeding out low variance features.

::: {style="color:orange"}
### Removing Low-Variance Features {style="color:orange"}
:::

Zero and near-zero variance variables are low-hanging fruit to eliminate.

-   **Zero Variance Features**: variable only contains a single unique value

-   **Near-Zero Variance Features**: variable contains only a few unique values

Zero and near-zero variance variables typically offer little to no information for model building. Furthermore, resampling (data-splitting) further complicates this picture because a given fold or sample may only contain a single value if the variable itself only contains a few unique values.

@handson suggest the following rule-of-thumb for removing low-variance features:

**Remove a variable if:**

-   The fraction of unique values over the sample size is low (e.g. 10%).

-   The ratio of the frequency of the most prevalent value to the frequency of the second most prevalent value is large (e.g. ≥20).

We can use the `caret` [@caret] package in R to look at these different metrics for our example data.

```{r}
caret::nearZeroVar(data, saveMetrics = TRUE) %>% 
  tibble::rownames_to_column() 
```

::: {style="color: yellow"}
## Numeric Feature Engineering
:::

Feature engineering on continuous or numeric features involves transforming the raw data to make it more useful for modeling. Common motivations include capturing nonlinear relationships (e.g., log or polynomial transforms), improving distributional properties (reducing skew or outlier impact), and putting features on comparable scales through standardization.

In some cases, continuous variables are binned into categories or combined into interactions to highlight patterns. Overall, these transformations help models detect structure in the data more effectively and improve both performance and interpretability.

::: {style="color: orange"}
### Skewness
:::

Parametric models with distributional assumptions (e.g., GLMs, and some regularized models) can benefit from minimizing the skewness of numeric features. One popular transformation is the [Yeo-Johnson transformation](https://en.wikipedia.org/wiki/Power_transform#Yeo%E2%80%93Johnson_transformation).

The Yeo-Johnsontransformation is a statistical technique used to stabilize variance and make data more normally distributed, similar to the Box-Cox transformation but more flexible. Unlike Box-Cox, it can handle both positive and negative values, making it useful for real-world data that span zero.

```{r}
recipe(happy ~ ., data = data) %>%
  step_YeoJohnson(all_numeric()) 
```

::: {style="color: orange"}
### Standardization
:::

We will often want to standardize variables prior to our model fitting to put them on a common scale, typically with mean of zero and a standard deviation of one.

This prevents features with larger numeric ranges (e.g., income in dollars vs. age in years) from dominating distance-based methods (like k-NN, SVMs, or clustering). It can also help with optimization problems, may improve convergence and can help ensure that regularization penalties (like in ridge or lasso regression) are applied fairly across predictors.

```{r,eval=Fve}
data %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes())
```

::: {style="color: yellow"}
## Categorical Feature Engineering
:::

Many models require that the predictors take numeric form. There are exceptionssuch as tree-based models, however, even tree-based methods can benefit from preprocessing categorical features. The following sections will discuss a few of the more common approaches to engineer categorical features.

::: {style="color: orange"}
### Lumping
:::

Sometimes features will contain levels that have very few observations. For example, take a look at the work status variable `wrkstat`. There are 8 unique levels and some have relatively few observations. For example, `With A Job, But Not At Work Because Of Temporary Illness, Vacation, Strike`.

```{r}
count(data, wrkstat) %>% arrange(n)
```

Sometimes we can benefit from collapsing, or “lumping” these into a lesser number of categories. In the above examples, we may want to collapse all levels that are observed in less than 5% of the training sample into an `Other` category. We can use `step_other()` to do so.

```{r}
lumping <- recipe(happy ~ ., data = data) %>%
  step_other(wrkstat, threshold = 0.05, 
             other = "Other")

# Apply this blue print --> you will learn about this at 
# the end of the chapter
apply_2_training <- prep(lumping, training = data) %>%
  bake(data)

# New distribution of Neighborhood
count(apply_2_training, wrkstat) %>% arrange(n)
```

::: {style="color: orange"}
### One-Hot and Dummy Encoding
:::

As mentioned previously many models require that all features be numeric. Consequently, we need to intelligently transform any categorical variables into numeric representations so that these algorithms will work.

There are many ways to recode categorical variables as numeric (e.g., one-hot, ordinal, binary, sum, and Helmert).

One-hot encoding is a common method for converting categorical variables into a numerical format that machine learning algorithms can work with. Instead of assigning arbitrary numbers to categories (which could incorrectly imply an order), one-hot encoding creates a new binary (0/1) column for each category level. For a given observation, the column corresponding to its category is set to 1, and all others are set to 0.

![https://towardsdatascience.com/encoding-categorical-variables-one-hot-vs-dummy-encoding-6d5b9c46e2db/](images/one_hot.webp){width="800"}

However, this creates perfect collinearity which causes problems with some predictive modeling algorithms (e.g., ordinary linear regression and neural networks). Alternatively, we can create a full-rank encoding by dropping one of the levels (level `blue` has been dropped). This is referred to as dummy coding.

![https://towardsdatascience.com/encoding-categorical-variables-one-hot-vs-dummy-encoding-6d5b9c46e2db/](images/dummy.webp){width="800"}

Below is an example of one-hot encoding the predictors in our model.

```{r}
recipe(happy ~ ., data = data) %>%
  step_dummy(all_factor_predictors(), one_hot = TRUE)
```

::: {style="color: yellow"}
## Workflow {style="color: yellow"}
:::

When setting up your feature engineering workflow there is often a sensible order in which to perform these steps.

One possible sequence that avoids headaches is suggested by @handson:

1.  Filter out zero or near-zero variance features.

2.  Perform imputation if required.

3.  Normalize to resolve numeric feature skewness.

4.  Standardize (center and scale) numeric features.

5.  Perform dimension reduction (e.g., PCA) on numeric features.

6.  One-hot or dummy encode categorical features.

::: {style="color: orange"}
### Workflow in R
:::

To illustrate how this process works together in R code, let’s do a simple analysis using our example data, **starting from scratch.**

The steps below simply re-downloads our data, selects the variables we want to keep, cleans up the missing data codes and does some basic relabeling.

```{r}
library(gssr)

gss24 <- gss_get_yr(2024)

target <- "happy"

features <- c(
  "age",     # age of participant
  "sex",     # sex of participant
  "race",    # race of participant
  "educ",    # highest education completed by participant
  "income",  # income of participant
  "childs",  # number of children participant has
  "wrkstat", # work force status
  "marital", # marital status
  "born",    # whether or not participant was born in USA
  "partyid", # political party of participant
  "adults",  # num of fam members 18 or older
  "earnrs"   # number of earners in family  
)

data <- gss24[,c(target, features)]


# define which varibles are categorical and continuous
cat_vars <- c("sex","race", "educ", "wrkstat", "marital", "born", "partyid")
con_vars <- c("age","income","childs","adults","earnrs","happy")

capwords <- function(x, strict = FALSE) {
    cap <- function(x) paste(toupper(substring(x, 1, 1)),
                  {x <- substring(x, 2); if(strict) tolower(x) else x},
                             sep = "", collapse = " " )
    sapply(strsplit(x, split = " "), cap, USE.NAMES = !is.null(names(x)))
}

data <- data |>
  mutate(
    # Convert all missing to NA
    across(everything(), haven::zap_missing), 
    # Make all categorical variables factors and relabel nicely
    across(all_of(cat_vars), forcats::as_factor),
    across(all_of(con_vars), as.numeric),
    across(all_of(cat_vars), \(x) forcats::fct_relabel(x, capwords, strict = TRUE))
  )

data <- data[!is.na(data$happy),]
```

We can now separate our data into a training and test set.

```{r}
# install.packages("rsample")
library(rsample)
# Stratified sampling with the rsample package
set.seed(123)
split <- initial_split(data, prop = 0.7, strata = "happy")
data_train  <- training(split)
data_test   <- testing(split)
```

Now, we will formally introduce the `recipes` package.

The **`recipes`** package is part of the `tidymodels` framework and is designed for feature engineering. In machine learning, raw data usually isn’t ready to be used directly in a model—you might need to do all the things we discussed in these notes.

Instead of doing all these steps manually, `recipes` lets us define a sequence of preprocessing steps (called a recipe) that can be applied consistently to training and test data.

A recipe typically goes through three main stages:

#### 1. Define the recipe

-   Write down the *blueprint* of preprocessing steps you want to apply.\
-   Example: “Impute missing values, standardize numeric predictors, and one-hot encode categorical variables.”\
-   At this stage, the recipe only **records what to do**, not how to do it.

#### 2. Prep

-   Use the training data to **learn any parameters needed** for preprocessing.\
-   Example: Calculate means and standard deviations for standardization, determine category levels for encoding, or find values to impute.\
-   After prepping, the recipe is ready to be applied consistently to new data.

#### 3. Bake

-   Apply the recipe to a dataset (training, validation, or test).\
-   This step actually transforms the data using the information learned during the `prep` stage.\
-   The output is a processed dataset that can be used directly in a machine learning model.

For example, the following defines `happy` as the target variable and then uses all the remaining columns as features based on `data_train`. We then:

1.  Remove near-zero variance features that are categorical (aka nominal).

2.  Impute missing data

3.  Dummy encode our categorical features.

4.  Center and scale (i.e., standardize) all numeric features

```{r}
blueprint <- recipe(happy ~ ., data = data_train) %>%
  step_nzv(all_nominal())  %>%
  step_impute_bag(all_predictors()) %>%
  step_dummy(all_factor_predictors(), one_hot = FALSE) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) 

blueprint

# these are example steps you don't need to run for any reason other
# than troubleshooting

# prepare <- prep(blueprint, training = data_train)
# prepare

# baked_train <- bake(prepare, new_data = data_train)
# baked_test  <- bake(prepare, new_data = data_test)
# baked_train
```

Next, we can fit a model using the `caret` pacakge, using our blueprint as the first argument and then `caret` takes care of the rest.

```{r}
library(caret)

# Specify cross-validation plan
cv <- trainControl(
  method = "cv", # k-folds cross-validation", 
  number = 10  # 10 folds
)

# Create grid of hyperparameter values
hyper_grid <- expand.grid(k = seq(2, 25, by = 1))

# Tune a knn model using grid search
knn_fit <- train(
  blueprint, 
  data = data_train, 
  method = "knn", 
  trControl = cv, 
  tuneGrid = hyper_grid,
  metric = "RMSE"
)

knn_fit

ggplot(knn_fit)
```
