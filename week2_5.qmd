---
title: "Feature Engineering"
format: 
  html: 
    fontsize: 25px
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

Feature engineering in ML typically describes the process of creating, transforming, or selecting variables (features) from raw data to improve a model’s performance. This often means transforming raw data into meaningful inputs that better capture the underlying patterns the model needs to learn.

For example, suppose we are trying to predict medicine adherence (whether or not someone takes their medicine). Our raw data might be timestamps of when someone actually takes their medicine. We may use that timestamps to create variables for *day of the week*, *holiday* or *weekend* to provide additional context to the model.

Domains considered to be part of a feature engineering pipeline can include:

-   Missing data handling

-   Feature Creation

-   Feature Transformations

-   Screening or Feature Selection

-   Dimesion Reduction

::: {style="color: yellow"}
## Example Data
:::

In this example we will use data from the General Social Survey. [The General Social Survey (GSS)](https://gss.norc.org/) is a long-running, nationally representative survey of adults in the United States that has been conducted almost every two years since 1972 by the [National Opinion Research Center (NORC)](https://www.norc.org/) at the University of Chicago. The GSS data is often used to measure American's attitudes, behaviors, and beliefs on a wide range of topics—such as politics, religion, crime, race relations, family, work, and technology.

There are two ways to access the GSS data. One is using the [GSS Data Explorer](#0). Another option is using the `gssr` [@gssr] R package. Here we will use the `gssr` [@gssr] package to download some example data.

We will start by choosing `happy` as our target variable. This comes from Question 157 of the 2024 GSS where respondents were asked:

> Taken all together, how would you say things are these days - would you say that you are very happy, pretty happy, or not too happy?

Responses were coded such that 1 indicates "very happy", 2 indicates "pretty happy", and 3 indicates "not too happy", while NA indicates "don’t know."

We can also identify ad bunch of features we think predict self-reported happiness and save our final dataset.

```{r}
library(gssr)

gss24 <- gss_get_yr(2024)

target <- "happy"

features <- c(
  "age",     # age of participant
  "sex",     # sex of participant
  "race",    # race of participant
  "educ",    # highest education completed by participant
  "income",  # income of participant
  "childs",  # number of children participant has
  "wrkstat", # work force status
  "marital", # marital status
  "born",    # whether or not participant was born in USA
  "partyid"  # political party of participant
)

data <- gss24[,c(target, features)]

table(data$happy, useNA = "always")
```

::: {style="color: yellow"}
## Missing Data
:::

Dealing with missing data in a consistent manner is one of the most important aspects of feature engineering.

::: {style="color: orange"}
### Missing Data Mechnisms
:::

Much attention is paid to the mechisms producing missing data in the statistics and applied sciences literature. Knowing how the missing data came about is critical for knowing how to handle it in a subsequent analysis.

Although attention to missing data mechanisms has historically not been a primary focus in the ML literature, this has been changing. A common framework for understanding missing data mechanisms was described by Rubin [-@rubin1976]. Here we will briefly describe these mechanisms in the context of our current data example.

::: {style="color: lightyellow"}
#### Missing Completely at Random (MCAR)
:::

In MCAR, the probability of a value being missing is unrelated to the value itself or any other observed or unobserved variable. This is a purely random and unsystematic process. Imagine a cat opening up your dataset in Excel and walking across the keyboard, randomly deleting different cells.

**Definition:** Missingness in self-reported happiness, for example, is unrelated to the respondent's true happiness level or any other variables.

**Example:** Let's say there is a glitch in the online GSS survey and for some respondent's questions are randomly skipped. This means the probability of missingness is purely random, and it does not depend on other variables (e.g. income, age, or happiness levels).

**Implication:** Dropping these cases (listwise deletion) or using simple imputation will not bias the results, although efficiency is reduced.

::: {style="color: lightyellow"}
#### Missing at Random (MAR)
:::

In MAR, the probability of a value being missing is systematically related to other observed variables in the dataset, but not to the unobserved value itself.

**Definition:** Missingness in the one of our predictors variables (income) is unrelated to the respondent's true happiness level or any other variables.

**Example:** Suppose in the GSS, individuals with higher incomes were less likely to answer the happiness question. Missingness on happiness is explained by *income*, which is observed. Conditional on income (having income in our model as a predictor), the probability of missingness does not depend how happy one is.

**Implication:** Methods like multiple imputation (MICE), missForest, or regression-based imputation can use income (and other observed covariates like marital status, education) to predict and impute missing happiness values without bias.

::: {style="color: lightyellow"}
#### Missing Not at Random (MNAR)
:::

**Definition:** Missingness in happiness depends on the unobserved happiness score itself.

**Example:** Respondents who are very unhappy may avoid answering the happiness question because it feels too personal, while those who are extremely happy may skip it because they consider it obvious. Missingness is directly tied to the unreported happiness level.

**Implication:** Standard imputation methods will be biased. Handling MNAR requires explicitly modeling the missingness mechanism (e.g., selection models, pattern-mixture models, sensitivity analysis, or generative deep models)

::: {style="color: orange"}
### 

Handling Missing Data in R
:::

Often a first step in handling missing data involves recoding missing values as `NA`. Writing bespoke code to handle the different types of missing data one might encounter is tedious and unnecessary.

`naniar` [@naniar] is a useful package with many convenience functions for managing missing data in R. Here we demonstrate some of this functionality.

::: {style="color: lightyellow"}
#### Recoding Values with `NA`
:::

Now that we have a dataset with missing values we can use `naniar` to recode these values to `NA`.

```{r}
library(naniar)

na_strings <- c("NA", "N/A", -99)
  
data <- naniar::replace_with_na_all(
  data, condition = ~.x %in% na_strings
)
```

See the [`naniar` vignette on recoding NA values](https://cran.r-project.org/web/packages/naniar/vignettes/replace-with-na.html) for more detailed information on the package functionality.

::: {style="color: lightyellow"}
#### Missing Data Visualization
:::

Once we have recoded our data in a consistent manner we can use visualizations to explore the missing data. The `vis_miss()` function from `naniar` is a good starting point for visualizing the amount of missing data in our dataset. The plots shows the missing values in black and non-missing values in gray. In addition, percentages of missing data in both the dataset and individual variables are provided.

```{r}
naniar::vis_miss(data)
```

It is often useful to look at combinations of missingness among different variables.

```{r}
naniar::gg_miss_upset(data)
```

We can also look at the percentage of missing data across a factor variable.

```{r}
naniar::gg_miss_fct(x = data, fct = marital)
```

Many missing data visualizations are described in the [`naniar` vignette on missing data visualization](https://cran.r-project.org/web/packages/naniar/vignettes/naniar-visualisation.html) including plots for exploring missing data mechanisms.

::: {style="color: orange"}
### Missing Data Activity
:::

Below is a small dataset looking at predictors of happiness. Some values are missing. Your job is to speculate on why each value might be missing and which type of missing data mechanism it represents: MCAR, MAR, or MNAR.

```{r}
# Columns: id, happiness (target), age (feature), income (feature), education (feature)
# NA indicates missing values

example_data <- data.frame(
  id = 1:15,
  happiness = c(NA, 8, 5, NA, 6, 9, 4, 7, NA, 5, 6, 8, 7, NA, 4),
  age = c(25, NA, 30, 40, 22, 35, NA, 29, 31, 28, 34, NA, 27, 33, 26),
  income = c(50000, 55000, 6500, 70000, NA, NA, NA, 62000, NA, 45000, 52000, NA, 58000, NA,61000),
  education = c("Bachelor","Bachelor","Master","Master","HighSchool","Bachelor","HighSchool","Master","Bachelor","HighSchool","Master","Master","Bachelor","HighSchool","Bachelor")
)

example_data

```

::: {style="color: orange"}
### Handling Missing Data
:::

There are a number of ways to handle missing data. Below I will discuss some of the most common ways of addressing missing data.

::: {style="color: lightyellow"}
#### Listwise Deletion
:::

Listwise deletion (also called complete-case analysis) is one of the simplest methods for handling missing data in a dataset.

When performing listwise deletion we remove any row that has one or more missing values across any variable used in the analysis.

After deletion, only rows that are complete for all variables remain.

For example, our example GSS data has `3,309` rows before we address the missing data. If we only kept rows that contained no missing data we would have `2,780` observations. You can perform listwise deletion on your data using the `complete.cases()` function as demonstrated below. Then you can visualize the missing data to ensure there is no missingness on the new dataset.

```{r}
# nrow(data) # 3,3309 rows

data_cc <- data[complete.cases(data),]
nrow(data_cc)
naniar::vis_miss(data_cc)
```

Now, listwise deletion should really only be used if data is missing completely at random (MCAR). In this case it can still provide unbiased results, although they can be less efficient (reduced power and more uncertainty).

::: {style="color: lightyellow"}
#### Imputation
:::

Imputation is the process of filling in missing values in a dataset with estimated or predicted values so that you can perform analyses without dropping incomplete cases.

Unlike listwise deletion, imputation retains all observations, reducing data loss. Imputation can be simple (deterministic, like a mean) or more involved (stochastic, model-based).

**Estimated Statistic**

A simple approach to imputing missing values for a feature is to compute descriptive statistics such as the mean, median, or mode (for categorical) and use that value to replace the `NA` values.

Although computationally efficient, this approach does not consider any other attributes for a given observation when imputing.

The `tidymodels` [@tidymodels] R package has a number of useful functions for machine learning. Here we use the package to perform mean imputation on our dataset.

```{r}
library(tidymodels)

# create a recipe
rec <- recipe(happy ~ ., data = data) %>%
  step_impute_mean(all_predictors())   
  # apply mean imputation to predictors

# prep the recipe (estimates imputation values)
rec_prep <- prep(rec)

# check the imputed values
data_mi <- bake(rec_prep, new_data = NULL)

naniar::vis_miss(data_mi)
```

**K-Nearest Neighbor**

Another popular method for performing imputation is k-nearest neighbor.

Now, instead of just filling in a missing value with a simple number like the mean, KNN looks for other respondents who are most similar (the “nearest neighbors”) and uses their information to fill in the blank.

**How KNN Imputation Works**

1.  **Measure similarity**
    -   For the student with a missing value, KNN looks at the other variables (educ, martial, partyid).\
    -   It finds the *k* students who are most “similar” (closest in the data space).
2.  **Borrow information**
    -   If we set `k = 3`, the algorithm finds the 3 most similar students.\
3.  **Fill in the blank**
    -   It might take the average of those 3 scores and use that as the imputed value.
    -   Or, depending on settings, it could pick a weighted average (closer neighbors count more).

**Better than Mean Imputation?**

Think of guessing someone’s favorite pizza topping:

-   **Mean imputation**: “Most people like pepperoni, so I’ll assume this person does too.”\
-   **KNN imputation**: This person is very similar to Iris, Naomi, Owen, Naveen, and Stella who all like pineapply. Instead, I’ll assume they probably like pineapple.

```{r}
library(tidymodels)

# create a recipe
rec <- recipe(happy ~ ., data = data) %>%
  step_impute_knn(all_predictors())   
  # apply mean imputation to predictors

# prep the recipe (estimates imputation values)
rec_prep <- prep(rec)

# check the imputed values
data_knn <- bake(rec_prep, new_data = NULL)

naniar::vis_miss(data_knn)
```

**Tree-Based Imputation**

Tree-based imputation methods use decision trees (or random forests) to predict missing values based on the other variables in the dataset. Similar to KNN methods, tree-based methods make a tailored prediction using patterns in the data.

Tree-based methods are especially nice for imputation as they handle non-linearities and interactions, and can accomodate mixed data types, like continuous and categorical variables.

**How Tree-Based Imputation Works**

1.  **Treat the missing variable like an outcome**
    -   Suppose some people are missing their *income*.
    -   A tree is trained to predict income from other available variables (like education, age, occupation).
2.  **Learn patterns in the data**
    -   The tree splits the dataset into groups that have similar income levels.
    -   For example:
        -   *If education \> 16 years → higher income group*
        -   *If education ≤ 16 and occupation = retail → lower income group*
3.  **Predict the missing values**

-   For a person with missing income, the model uses their other information (education, age, etc.) to drop them into the right “leaf” of the tree.
-   The average (or majority class, if categorical) in that leaf is used as the imputed value.

**Better than Mean Imputation?**

Think of tree-based imputation like asking:

> *“Given your age, job, and education, people like you usually make about X — so we’ll use that as your missing value.”*

This is smarter than saying *“everyone makes the same average income”* (mean imputation) or even *“let’s just look at your 3 closest neighbors”* (KNN).

::: {style="color: yellow"}
## Feature Filtering
:::

Feature filtering is a feature selection technique in machine learning where features are evaluated individually (typically prior to the model fitting) using statistical or heuristic criteria, and then kept or discarded based on those criteria.

It is called filtering because the method acts as a preprocessing step, part of a feature engineering pipeline, before actually training the model.

Typically, the goals of feature filtering are:

1.  **Reduce dimensionality:** Fewer features means simpler models, faster training, and less risk of overfitting.

2.  **Remove irrelevant or redundant features:** Avoids feeding the model noisy or useless inputs.

3.  **Improve interpretability:** Models are easier to understand when they focus only on meaningful features.

4.  **Boost performance:** Better generalization on new data when unnecessary features are excluded.

## 
