[
  {
    "objectID": "week3_2.html",
    "href": "week3_2.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Multiple linear regression is a statistical method used to model the relationship between two variables. As such, multiple regression extends the simple linear regression framework to include more than one predictor variable:\n- more than one feature (also called the predictors or independent variables),\n- one target (also called the outcome or dependent variable).\nMultiple linear regression allows us to model the relationship between a single outcome variable \\(Y_i\\) and multiple predictors \\(X_{1i}, X_{2i}, \\dots, X_{pi}\\) for each observation \\(i = 1, \\dots, n\\).\nAgain let’s use the Ames dataset to illustrate multiple linear regression. We will look to predict the sale price of a home (Sale_Price) using two predictors: above ground living area (Gr_Liv_Area) and the year the house was built (Year_Built).\n# Install and load package\n# install.packages(\"AmesHousing\")\nlibrary(AmesHousing)\n\n# Load data\names &lt;- make_ames()\nThe general form of the multiple linear regression model is:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_p X_{pi} + \\varepsilon_i\n\\]\nwhere:\n- \\(Y_i\\): the dependent (outcome) variable for observation \\(i\\).\n- \\(X_{ji}\\): the value of predictor \\(j\\) for observation \\(i\\).\n- \\(\\beta_0\\): the intercept (expected value of \\(Y_i\\) when all \\(X_{ji} = 0\\)).\n- \\(\\beta_j\\): the regression coefficient for predictor \\(X_{ji}\\), representing the expected change in \\(Y_i\\) for a one-unit increase in \\(X_{ji}\\), holding all other predictors constant.\n- \\(\\varepsilon_i\\): the error term for observation \\(i\\), capturing variation in \\(Y_i\\) not explained by the predictors.\nMultiple regression relies on several key assumptions:\nThe coefficients \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) are estimated using ordinary least squares (OLS).\nOLS chooses coefficient values that minimize the residual sum of square:\n\\[\n\\text{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\nwhere:\n- \\(y_i\\) is the observed outcome for observation \\(i\\),\n- \\(\\hat{y}_i\\) is the predicted value of \\(Y_i\\) from the model.\nIn R, we fit a multiple regression model with the lm() function:\nfit_mlr &lt;- lm(\n  Sale_Price ~ Gr_Liv_Area + Year_Built, \n  data = ames\n)\nsummary(fit_mlr)\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-458172  -26758   -2236   18514  306986 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.106e+06  5.734e+04  -36.74   &lt;2e-16 ***\nGr_Liv_Area  9.597e+01  1.758e+00   54.60   &lt;2e-16 ***\nYear_Built   1.087e+03  2.938e+01   37.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 46660 on 2927 degrees of freedom\nMultiple R-squared:  0.6591,    Adjusted R-squared:  0.6588 \nF-statistic:  2829 on 2 and 2927 DF,  p-value: &lt; 2.2e-16\nWe interpret the intercept, \\(\\beta_0=-2,106,000\\), as the estimated mean sale price when both Gr_Liv_Area and Year_Built are 0. While this interpretation may not be meaningful in a real-world context (since a house with 0 living area and built in year 0 is not realistic), it serves as a baseline for our model.\nWe interpret the slope coefficient for Gr_Liv_Area, \\(\\beta_1=96\\), as the estimated change in mean sale price for each additional square foot of living area, holding Year_Built constant. Specifically, for each one-square-foot increase in Gr_Liv_Area, the mean sale price is estimated to increase by \\(96\\) dollars, holding Year_Built constant.\nWe interpret the slope coefficient for Year_Built, \\(\\beta_2=1,087\\), as the estimated change in mean sale price for each additional year the house was built, holding Gr_Liv_Area constant. Specifically, for each one-year increase in Year_Built, the mean sale price is estimated to increase by roughly a thousand dollars, holding Gr_Liv_Area constant.\nIn multiple linear regression, an interaction occurs when the effect of one predictor on the response variable depends on the level of another predictor.\nIn other words, the impact of one variable is modified or moderated by another variable.\nFor two predictors, \\(X_1\\) and \\(X_2\\), an interaction term can be included as:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 (X_{1i} \\cdot X_{2i}) + \\varepsilon_i\n\\]\nwhere:\n- \\(\\beta_3\\) captures the interaction effect between \\(X_1\\) and \\(X_2\\).\n- The term \\((X_{1i} \\cdot X_{2i})\\) is the product of the two predictors.\n- If \\(\\beta_3 \\neq 0\\), the effect of \\(X_1\\) on \\(Y\\) changes depending on \\(X_2\\), and vice versa.\nInteractions allow the model to represent non-additive relationships between predictors and the response. Without interactions, the model assumes each predictor contributes independently, which may oversimplify reality.\nfit_interaction &lt;- lm(\n  Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, \n  data = ames\n)\nsummary(fit_interaction)\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, \n    data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-541888  -25275   -1849   17227  292527 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -3.252e+05  1.757e+05  -1.851   0.0643 .  \nGr_Liv_Area            -1.033e+03  1.055e+02  -9.792   &lt;2e-16 ***\nYear_Built              1.825e+02  8.931e+01   2.043   0.0411 *  \nGr_Liv_Area:Year_Built  5.727e-01  5.351e-02  10.703   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45780 on 2926 degrees of freedom\nMultiple R-squared:  0.6719,    Adjusted R-squared:  0.6716 \nF-statistic:  1998 on 3 and 2926 DF,  p-value: &lt; 2.2e-16\nWe could interpret the interaction coefficient, \\(\\beta_3=0.057\\), as follows: For each additional square foot of living area, the effect of Year_Built on Sale_Price increases by about six cents, holding Gr_Liv_Area constant. Conversely, for each additional year the house was built, the effect of Gr_Liv_Area on Sale_Price increases by about six cents, holding Year_Built constant.\nTo assess how accurate our models are we will go back to our cross-validation framework. First, let’s split our data into test and train sets.\nset.seed(123)\nsplit &lt;- initial_split(\n  ames, \n  prop = 0.7, \n  strata = \"Sale_Price\"\n)\names_train  &lt;- training(split)\names_test   &lt;- testing(split)\nNow, let’s consider three different models we may want to compare: 1. Model 1: A simple linear regression using only Gr_Liv_Area as a predictor.\n2. Model 2: A multiple linear regression using Gr_Liv_Area and Year_Built as predictors.\n3. Model 3: A full model using all available predictors.\nlibrary(caret)\n# Train model using 10-fold cross-validation\nset.seed(123)  # for reproducibility\n(cv_model1 &lt;- train(\n  form = Sale_Price ~ Gr_Liv_Area, \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nLinear Regression \n\n2049 samples\n   1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1843, 1844, 1844, 1844, 1844, 1844, ... \nResampling results:\n\n  RMSE      Rsquared  MAE     \n  56644.76  0.510273  38851.99\n\nTuning parameter 'intercept' was held constant at a value of TRUE\nThe resulting cross-validated RMSE is $56,644.76 (this is the average RMSE across the 10 CV folds). How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about $56,644.76 off from the actual sale price.\n# model 2 CV\nset.seed(123)\n(cv_model2 &lt;- train(\n  Sale_Price ~ Gr_Liv_Area + Year_Built, \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nLinear Regression \n\n2049 samples\n   2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1843, 1844, 1844, 1844, 1844, 1844, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  46865.68  0.6631008  31695.48\n\nTuning parameter 'intercept' was held constant at a value of TRUE\nThe resulting cross-validated RMSE is now a bit lower at $46,865.68 (this is the average RMSE across the 10 CV folds). How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about $46,865.68 off from the actual sale price.\n# model 3 CV\nset.seed(123)\n(cv_model3 &lt;- train(\n  Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area*Year_Built, \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nLinear Regression \n\n2049 samples\n   2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1843, 1844, 1844, 1844, 1844, 1844, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  46523.38  0.6691386  30746.99\n\nTuning parameter 'intercept' was held constant at a value of TRUE\nAgain we see a (very) small improvement when adding all the available features. The resulting cross-validated RMSE is now lower at $46,523.38 (this is the average RMSE across the 10 CV folds). How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about $46,523.38 off from the actual sale price.\n# Extract out of sample performance measures\nsummary(resamples(list(\n  model1 = cv_model1, \n  model2 = cv_model2, \n  model3 = cv_model3\n)))\n\n\nCall:\nsummary.resamples(object = resamples(list(model1 = cv_model1, model2\n = cv_model2, model3 = cv_model3)))\n\nModels: model1, model2, model3 \nNumber of resamples: 10 \n\nMAE \n           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nmodel1 34076.73 37656.23 39785.18 38851.99 40200.92 42058.68    0\nmodel2 29227.14 30885.17 32003.59 31695.48 32710.41 33942.26    0\nmodel3 27811.34 30036.68 31084.19 30746.99 31639.78 33142.02    0\n\nRMSE \n           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nmodel1 45604.65 55896.58 57000.74 56644.76 59544.08 66198.59    0\nmodel2 37174.26 42650.00 46869.84 46865.68 51155.14 55780.47    0\nmodel3 35825.06 41986.84 46598.42 46523.38 50897.45 58150.97    0\n\nRsquared \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.4230788 0.4621034 0.5090642 0.5102730 0.5681246 0.5996400    0\nmodel2 0.5829425 0.6075293 0.6865871 0.6631008 0.6976664 0.7254572    0\nmodel3 0.5653745 0.6087785 0.6898757 0.6691386 0.7134740 0.7453157    0\nAs discussed earlier, linear regression remains one of the most widely used modeling techniques, primarily because the regression coefficients are straightforward to interpret. Each coefficient quantifies the expected change in the response variable for a one-unit change in a predictor, holding all other predictors constant.\nHowever, linear regression relies on several strong assumptions, and these assumptions are often violated as we increase the number of predictors in the model. Violations can lead to biased estimates, misleading interpretations of coefficients, and inaccurate predictions.\nLinear regression assumes a linear relationship between each predictor and the response variable. This means that the change in the response associated with a one-unit change in a predictor is constant across all levels of that predictor.\nIn practice, many relationships are non-linear. Fortunately, we can often address this issue by applying transformations to the response and/or predictors to make the relationship approximately linear.\nFor example, the relationship between home sale price and the year the home was built is shown:\np1 &lt;- ggplot(ames_train, aes(Year_Built, Sale_Price)) + \n  geom_point(size = 1, alpha = .4) +\n  geom_smooth(se = FALSE) +\n  scale_y_continuous(\"Sale price\", labels = scales::dollar) +\n  xlab(\"Year built\") +\n  ggtitle(paste(\"Non-transformed variables with a\\n\",\n                \"non-linear relationship.\"))\n\np2 &lt;- ggplot(ames_train, aes(Year_Built, Sale_Price)) + \n  geom_point(size = 1, alpha = .4) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_y_log10(\"Sale price\", labels = scales::dollar, \n                breaks = seq(0, 400000, by = 100000)) +\n  xlab(\"Year built\") +\n  ggtitle(paste(\"Transforming variables can provide a\\n\",\n                \"near-linear relationship.\"))\n\ngridExtra::grid.arrange(p1, p2, nrow = 1)\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n`geom_smooth()` using formula = 'y ~ x'\nHeteroskedasticity occurs when the variance of the errors is not constant across all levels of the predictors. This violates the homoscedasticity assumption of linear regression.\nLooking at the plot of residuals versus fitted values for Model 1, we can see a funnel shape, indicating that the variance of the residuals increases with the fitted values. This suggests heteroskedasticity, which can lead to inefficient estimates and invalid inference.\ndf1 &lt;- broom::augment(cv_model1$finalModel, data = ames_train)\n\nggplot(df1, aes(.fitted, .std.resid)) + \n  geom_point(size = 1, alpha = .4) +\n  xlab(\"Predicted values\") +\n  ylab(\"Residuals\") +\n  ggtitle(\"Model 1\", subtitle = \"Sale_Price ~ Gr_Liv_Area\")\nTo address heteroskedasticity, we can consider transforming the response variable (e.g., using a log transformation) or using robust standard errors that adjust for non-constant variance.\nWe might also be concerned with checking the residuals for autocorrelation, especially if our data has a time component. Autocorrelation occurs when the residuals are correlated with each other, violating the independence assumption. This can lead to underestimated standard errors and overly optimistic p-values. Since our current data does not have a time component, we will not explore this further here.\nThe vip() function from the vip package in R (Variable Importance Plots) is commonly used to visualize the relative importance of predictors in a model.\n# Load package\nlibrary(vip)\nThe vip() function takes any fitted model object, such as lm() for linear regression, glm(), random forests, or xgboost.\nFor linear regression models, vip() typically uses the absolute value of the t-statistics of the coefficients as a measure of importance. Predictors with larger absolute t-values are considered more influential on the response.\n# model 4 CV\nset.seed(1234)\n(cv_model4 &lt;- train(\n  Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Overall_Qual + Total_Bsmt_SF + Garage_Area + Full_Bath + TotRms_AbvGrd + Fireplaces + Garage_Cars + Wood_Deck_SF + Open_Porch_SF + Year_Remod_Add ,\n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nLinear Regression \n\n2049 samples\n  13 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1844, 1845, 1843, 1844, 1844, 1844, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  32145.44  0.8392331  19868.31\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\nvip(cv_model4, num_features = 10, method = \"model\")",
    "crumbs": [
      "Linear Regression",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#example-data",
    "href": "week3_2.html#example-data",
    "title": "Multiple Linear Regression",
    "section": "Example Data",
    "text": "Example Data",
    "crumbs": [
      "Linear Regression",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#the-model",
    "href": "week3_2.html#the-model",
    "title": "Multiple Linear Regression",
    "section": "The Model",
    "text": "The Model",
    "crumbs": [
      "Linear Regression",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#assumptions",
    "href": "week3_2.html#assumptions",
    "title": "Multiple Linear Regression",
    "section": "Assumptions",
    "text": "Assumptions",
    "crumbs": [
      "Linear Regression",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#estimating-the-model",
    "href": "week3_2.html#estimating-the-model",
    "title": "Multiple Linear Regression",
    "section": "Estimating the Model",
    "text": "Estimating the Model",
    "crumbs": [
      "Linear Regression",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#interpreting-coefficients",
    "href": "week3_2.html#interpreting-coefficients",
    "title": "Multiple Linear Regression",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients",
    "crumbs": [
      "Linear Regression",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#implementation-in-r",
    "href": "week3_2.html#implementation-in-r",
    "title": "Multiple Linear Regression",
    "section": "Implementation in R",
    "text": "Implementation in R",
    "crumbs": [
      "Linear Regression",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week2_5g.html",
    "href": "week2_5g.html",
    "title": "Workflow",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ ggplot2      3.5.2      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.0 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.1.0      ✔ yardstick    1.3.2 \n✔ recipes      1.3.1      \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\n\n\nWorkflow in R\n\nTo illustrate how this process works together in R code, let’s do a simple analysis using our example data, starting from scratch.\nThe steps below simply re-downloads our data, selects the variables we want to keep, cleans up the missing data codes and does some basic relabeling.\n\nlibrary(gssr)\n\nPackage loaded. To attach the GSS data, type data(gss_all) at the console.\nFor the panel data and documentation, type e.g. data(gss_panel08_long) and data(gss_panel_doc).\nFor help on a specific GSS variable, type ?varname at the console.\n\ngss24 &lt;- gss_get_yr(2024)\n\nFetching: https://gss.norc.org/documents/stata/2024_stata.zip\n\ntarget &lt;- \"happy\"\n\nfeatures &lt;- c(\n  \"age\",     # age of participant\n  \"sex\",     # sex of participant\n  \"race\",    # race of participant\n  \"educ\",    # highest education completed by participant\n  \"income\",  # income of participant\n  \"childs\",  # number of children participant has\n  \"wrkstat\", # work force status\n  \"marital\", # marital status\n  \"born\",    # whether or not participant was born in USA\n  \"partyid\", # political party of participant\n  \"adults\",  # num of fam members 18 or older\n  \"earnrs\"   # number of earners in family  \n)\n\ndata &lt;- gss24[,c(target, features)]\n\n\n# define which varibles are categorical and continuous\ncat_vars &lt;- c(\"sex\",\"race\", \"educ\", \"wrkstat\", \"marital\", \"born\", \"partyid\")\ncon_vars &lt;- c(\"age\",\"income\",\"childs\",\"adults\",\"earnrs\",\"happy\")\n\ncapwords &lt;- function(x, strict = FALSE) {\n    cap &lt;- function(x) paste(toupper(substring(x, 1, 1)),\n                  {x &lt;- substring(x, 2); if(strict) tolower(x) else x},\n                             sep = \"\", collapse = \" \" )\n    sapply(strsplit(x, split = \" \"), cap, USE.NAMES = !is.null(names(x)))\n}\n\ndata &lt;- data |&gt;\n  mutate(\n    # Convert all missing to NA\n    across(everything(), haven::zap_missing), \n    # Make all categorical variables factors and relabel nicely\n    across(all_of(cat_vars), forcats::as_factor),\n    across(all_of(con_vars), as.numeric),\n    across(all_of(cat_vars), \\(x) forcats::fct_relabel(x, capwords, strict = TRUE))\n  )\n\ndata &lt;- data[!is.na(data$happy),]\n\nWe can now separate our data into a training and test set.\n\n# install.packages(\"rsample\")\nlibrary(rsample)\n# Stratified sampling with the rsample package\nset.seed(123)\nsplit &lt;- initial_split(data, prop = 0.7, strata = \"happy\")\ndata_train  &lt;- training(split)\ndata_test   &lt;- testing(split)\n\nNow, we will formally introduce the recipes package.\nThe recipes package is part of the tidymodels framework and is designed for feature engineering. In machine learning, raw data usually isn’t ready to be used directly in a model—you might need to do all the things we discussed in these notes.\nInstead of doing all these steps manually, recipes lets us define a sequence of preprocessing steps (called a recipe) that can be applied consistently to training and test data.\nA recipe typically goes through three main stages:\n\n1. Define the recipe\n\nWrite down the blueprint of preprocessing steps you want to apply.\n\nExample: “Impute missing values, standardize numeric predictors, and one-hot encode categorical variables.”\n\nAt this stage, the recipe only records what to do, not how to do it.\n\n\n\n2. Prep\n\nUse the training data to learn any parameters needed for preprocessing.\n\nExample: Calculate means and standard deviations for standardization, determine category levels for encoding, or find values to impute.\n\nAfter prepping, the recipe is ready to be applied consistently to new data.\n\n\n\n3. Bake\n\nApply the recipe to a dataset (training, validation, or test).\n\nThis step actually transforms the data using the information learned during the prep stage.\n\nThe output is a processed dataset that can be used directly in a machine learning model.\n\nFor example, the following defines happy as the target variable and then uses all the remaining columns as features based on data_train. We then:\n\nRemove near-zero variance features that are categorical (aka nominal).\nImpute missing data\nDummy encode our categorical features.\nCenter and scale (i.e., standardize) all numeric features\n\n\nblueprint &lt;- recipe(happy ~ ., data = data_train) %&gt;%\n  step_nzv(all_nominal())  %&gt;%\n  step_impute_bag(all_predictors()) %&gt;%\n  step_dummy(all_factor_predictors(), one_hot = FALSE) %&gt;%\n  step_center(all_numeric(), -all_outcomes()) %&gt;%\n  step_scale(all_numeric(), -all_outcomes()) \n\nblueprint\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 12\n\n\n\n\n\n── Operations \n\n\n• Sparse, unbalanced variable filter on: all_nominal()\n\n\n• Bagged tree imputation for: all_predictors()\n\n\n• Dummy variables from: all_factor_predictors()\n\n\n• Centering for: all_numeric() -all_outcomes()\n\n\n• Scaling for: all_numeric() -all_outcomes()\n\n# these are example steps you don't need to run for any reason other\n# than troubleshooting\n\n# prepare &lt;- prep(blueprint, training = data_train)\n# prepare\n\n# baked_train &lt;- bake(prepare, new_data = data_train)\n# baked_test  &lt;- bake(prepare, new_data = data_test)\n# baked_train\n\nNext, we can fit a model using the caret pacakge, using our blueprint as the first argument and then caret takes care of the rest.\n\nlibrary(caret)\n\nLoading required package: lattice\n\n\n\nAttaching package: 'caret'\n\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\n\nThe following object is masked from 'package:rsample':\n\n    calibration\n\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n# Specify cross-validation plan\ncv &lt;- trainControl(\n  method = \"cv\", # k-folds cross-validation\", \n  number = 10  # 10 folds\n)\n\n# Create grid of hyperparameter values\nhyper_grid &lt;- expand.grid(k = seq(2, 25, by = 1))\n\n# Tune a knn model using grid search\nknn_fit &lt;- train(\n  blueprint, \n  data = data_train, \n  method = \"knn\", \n  trControl = cv, \n  tuneGrid = hyper_grid,\n  metric = \"RMSE\"\n)\n\nWarning: !  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X4th.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n\n\nWarning: !  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n!  The following column has zero variance so scaling cannot be used:\n  educ_X1st.Grade.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n\nknn_fit\n\nk-Nearest Neighbors \n\n2295 samples\n  12 predictor\n\nRecipe steps: nzv, impute_bag, dummy, center, scale \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 2066, 2066, 2066, 2064, 2066, 2065, ... \nResampling results across tuning parameters:\n\n  k   RMSE       Rsquared    MAE      \n   2  0.7418063  0.03097989  0.5719579\n   3  0.7052823  0.03088297  0.5546278\n   4  0.6847273  0.03149323  0.5390046\n   5  0.6771450  0.02897065  0.5329401\n   6  0.6684619  0.03192580  0.5233915\n   7  0.6594812  0.03810636  0.5159846\n   8  0.6550279  0.03945877  0.5112249\n   9  0.6529972  0.03903932  0.5081061\n  10  0.6514761  0.03798039  0.5039809\n  11  0.6495920  0.03878046  0.5006377\n  12  0.6464377  0.04135895  0.4983083\n  13  0.6449913  0.04153260  0.4959262\n  14  0.6457968  0.03833580  0.4952235\n  15  0.6455656  0.03708429  0.4940333\n  16  0.6445642  0.03761307  0.4920747\n  17  0.6428354  0.03999850  0.4895426\n  18  0.6415509  0.04156203  0.4877497\n  19  0.6410869  0.04199466  0.4861644\n  20  0.6408229  0.04204207  0.4847307\n  21  0.6409284  0.04098073  0.4844582\n  22  0.6412205  0.04035344  0.4837011\n  23  0.6408491  0.04051976  0.4822852\n  24  0.6410824  0.03919338  0.4819184\n  25  0.6411540  0.03858298  0.4816823\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was k = 20.\n\nggplot(knn_fit)",
    "crumbs": [
      "Feature Engineering",
      "Workflow"
    ]
  },
  {
    "objectID": "week2_5e.html",
    "href": "week2_5e.html",
    "title": "Numeric Feature Engineering",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ ggplot2      3.5.2      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.0 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.1.0      ✔ yardstick    1.3.2 \n✔ recipes      1.3.1      \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\nFeature engineering on continuous or numeric features involves transforming the raw data to make it more useful for modeling. Common motivations include capturing nonlinear relationships (e.g., log or polynomial transforms), improving distributional properties (reducing skew or outlier impact), and putting features on comparable scales through standardization.\nIn some cases, continuous variables are binned into categories or combined into interactions to highlight patterns. Overall, these transformations help models detect structure in the data more effectively and improve both performance and interpretability.\nParametric models with distributional assumptions (e.g., GLMs, and some regularized models) can benefit from minimizing the skewness of numeric features. One popular transformation is the Yeo-Johnson transformation.\nThe Yeo-Johnsontransformation is a statistical technique used to stabilize variance and make data more normally distributed, similar to the Box-Cox transformation but more flexible. Unlike Box-Cox, it can handle both positive and negative values, making it useful for real-world data that span zero.\nrecipe(happy ~ ., data = data) %&gt;%\n  step_YeoJohnson(all_numeric()) \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 12\n\n\n\n\n\n── Operations \n\n\n• Yeo-Johnson transformation on: all_numeric()\nWe will often want to standardize variables prior to our model fitting to put them on a common scale, typically with mean of zero and a standard deviation of one.\nThis prevents features with larger numeric ranges (e.g., income in dollars vs. age in years) from dominating distance-based methods (like k-NN, SVMs, or clustering). It can also help with optimization problems, may improve convergence and can help ensure that regularization penalties (like in ridge or lasso regression) are applied fairly across predictors.\ndata %&gt;%\n  step_center(all_numeric(), -all_outcomes()) %&gt;%\n  step_scale(all_numeric(), -all_outcomes())",
    "crumbs": [
      "Feature Engineering",
      "Numeric Feature Engineering"
    ]
  },
  {
    "objectID": "week2_5e.html#numeric-feature-engineering",
    "href": "week2_5e.html#numeric-feature-engineering",
    "title": "Numeric Feature Engineering",
    "section": "Numeric Feature Engineering",
    "text": "Numeric Feature Engineering",
    "crumbs": [
      "Feature Engineering",
      "Numeric Feature Engineering"
    ]
  },
  {
    "objectID": "week2_5c.html",
    "href": "week2_5c.html",
    "title": "Missing Data",
    "section": "",
    "text": "Dealing with missing data in a consistent manner is one of the most important aspects of feature engineering.\nTo understand modern approaches to handle missing data it is critical to have a basic understanding of missing data mechanisms, and the methodological approaches used for addressing each mechanism.\nMuch attention is paid to the mechanisms producing missing data in the statistics and applied sciences literature.\nKnowing how the missing data came about is critical for knowing how to handle it in a subsequent analysis.\nAlthough attention to missing data mechanisms has historically not been a primary focus in the ML literature, this has been changing.\nA common framework for understanding missing data mechanisms was described by Rubin (1976). Here we will briefly describe these mechanisms in the context of our current data example.\nIn MCAR, the probability of a value being missing is unrelated to the value itself or any other observed or unobserved variable. This is a purely random and unsystematic process.\nImagine a cat opening up your dataset in Excel and walking across the keyboard, randomly deleting different cells.\nDefinition: Missingness in self-reported happiness, for example, is unrelated to the respondent’s true happiness level or any other variables.\nExample: Let’s say there is a glitch in the online GSS survey and for some respondent’s questions are randomly skipped. This means the probability of missingness is purely random, and it does not depend on other variables (e.g. income, age, or happiness levels).\nImplication: Dropping these cases (listwise deletion) or using simple imputation will not bias the results, although efficiency is reduced.\nIn MAR, the probability of a value being missing is systematically related to other observed variables in the dataset, but not to the unobserved value itself.\nDefinition: Missingness in happiness depends on other observed variables but not directly on happiness itself.\nExample: Suppose in the GSS, individuals with higher incomes were less likely to answer the happiness question. Missingness on happiness is explained by income, which is observed. Conditional on income (having income in our model as a predictor), the probability of missingness does not depend how happy one is.\nImplication: Methods like multiple imputation (MICE), missForest, or regression-based imputation can use income (and other observed covariates like marital status, education) to predict and impute missing happiness values without bias.\nDefinition: Missingness in happiness depends on the unobserved happiness score itself.\nExample: Respondents who are very unhappy may avoid answering the happiness question because it feels too personal, while those who are extremely happy may skip it because they consider it obvious. Missingness is directly tied to the unreported happiness level.\nImplication: Standard imputation methods will be biased. Handling MNAR requires explicitly modeling the missingness mechanism (e.g., selection models, pattern-mixture models)\nOften a first step in handling missing data involves recoding missing values as NA. Writing bespoke code to handle the different types of missing data one might encounter is tedious and unnecessary.\nThe naniar (Tierney and Cook 2023) package in R contains many convenience functions for managing missing data in R. Here we demonstrate some of that functionality.\nNow that we have a dataset with missing values we can use naniar to recode these values to NA. In our current data example this is already done, but this code might be useful for other projects where you import data\nlibrary(naniar)\ngss_na_codes &lt;- c(-99, -999, \"NA\")\n  \ndata &lt;- naniar::replace_with_na_all(\n  data, condition = ~.x %in% gss_na_codes\n)\nSee the naniar vignette on recoding NA values for more detailed information on the package functionality.\nBelow is a small dataset looking at predictors of happiness. Some values are missing.\nIn small groups please speculate on why each value might be missing and which type of missing data mechanism it represents: MCAR, MAR, or MNAR.\n# Columns: id, happiness (target), age (feature), income (feature), education (feature)\n# NA indicates missing values\n\nexample_data &lt;- data.frame(\n  id = 1:15,\n  happiness = c(NA, 8, 5, NA, 6, 9, 4, 7, NA, 5, 6, 8, 7, NA, 4),\n  age = c(25, NA, 30, 40, 22, 35, NA, 29, 31, 28, 34, NA, 27, 33, 26),\n  income = c(50000, 55000, 6500, 70000, NA, NA, NA, 62000, NA, 45000, 52000, NA, 58000, NA,61000),\n  education = c(\"Bachelor\",\"Bachelor\",\"Master\",\"Master\",\"HighSchool\",\"Bachelor\",\"HighSchool\",\"Master\",\"Bachelor\",\"HighSchool\",\"Master\",\"Master\",\"Bachelor\",\"HighSchool\",\"Bachelor\")\n)\n\nexample_data\n\n   id happiness age income  education\n1   1        NA  25  50000   Bachelor\n2   2         8  NA  55000   Bachelor\n3   3         5  30   6500     Master\n4   4        NA  40  70000     Master\n5   5         6  22     NA HighSchool\n6   6         9  35     NA   Bachelor\n7   7         4  NA     NA HighSchool\n8   8         7  29  62000     Master\n9   9        NA  31     NA   Bachelor\n10 10         5  28  45000 HighSchool\n11 11         6  34  52000     Master\n12 12         8  NA     NA     Master\n13 13         7  27  58000   Bachelor\n14 14        NA  33     NA HighSchool\n15 15         4  26  61000   Bachelor\nThere are a number of ways to handle missing data. Below I will discuss some of the most common ways of addressing missing data.\nListwise deletion (also called complete-case analysis) is one of the simplest methods for handling missing data in a dataset.\nWhen performing listwise deletion we remove any row that has one or more missing values across any variable used in the analysis.\nAfter deletion, only rows that are complete for all variables remain.\nFor example, our example GSS data has 3,309 rows before we address the missing data. If we only kept rows that contained no missing data we would have 2,780 observations. You can perform listwise deletion on your data using the complete.cases() function as demonstrated below. Then you can visualize the missing data to ensure there is no missingness on the new dataset.\n# nrow(data) # 3,3309 rows\n\ndata_cc &lt;- data[complete.cases(data),]\nnrow(data_cc)\n\n[1] 2746\n\nnaniar::vis_miss(data_cc)\nNow, listwise deletion should really only be used if data is missing completely at random (MCAR). In this case it can still provide unbiased results, although they can be less efficient (reduced power and more uncertainty).\nImputation is the process of filling in missing values in a dataset with estimated or predicted values so that you can perform analyses without dropping incomplete cases.\nUnlike listwise deletion, imputation retains all observations, reducing data loss. Imputation can be simple (deterministic, like a mean) or more involved (stochastic, model-based).\nA simple approach to imputing missing values for a feature is to compute descriptive statistics such as the mean, median, or mode (for categorical) and use that value to replace the NA values.\nAlthough computationally efficient, this approach does not consider any other attributes for a given observation when imputing.\nThe tidymodels (Kuhn and Wickham 2020) R package has a number of useful functions for machine learning. Here we use the package to perform mean imputation on our dataset.\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ recipes      1.3.1 \n✔ dials        1.4.2      ✔ rsample      1.3.1 \n✔ dplyr        1.1.4      ✔ tailor       0.1.0 \n✔ ggplot2      3.5.2      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.0 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.1.0      ✔ yardstick    1.3.2 \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\n# create a recipe\nrec &lt;- recipe(happy ~ ., data = data) %&gt;%\n  step_impute_mean(all_numeric_predictors())  %&gt;% \n  step_impute_mode(all_factor_predictors())\n  # apply mean imputation to predictors\n\n# prep the recipe (estimates imputation values)\nrec_prep &lt;- prep(rec)\n\n# check the imputed values\ndata_mi &lt;- bake(rec_prep, new_data = NULL)\n\nnaniar::vis_miss(data_mi)\nAnother popular method for performing imputation is k-nearest neighbor.\nNow, instead of just filling in a missing value with a simple number like the mean, KNN looks for other respondents who are most similar (the “nearest neighbors”) and uses their information to fill in the blank.\nHow KNN Imputation Works\nBetter than Mean Imputation?\nThink of guessing someone’s favorite pizza topping:\nlibrary(tidymodels)\n\n# create a recipe\nrec &lt;- recipe(happy ~ ., data = data) %&gt;%\n  step_impute_knn(all_predictors())   \n  # apply mean imputation to predictors\n\n# prep the recipe (estimates imputation values)\nrec_prep &lt;- prep(rec)\n\n# check the imputed values\ndata_knn &lt;- bake(rec_prep, new_data = NULL)\n\nnaniar::vis_miss(data_knn)\nTree-based imputation methods use decision trees (or random forests) to predict missing values based on the other variables in the dataset. Similar to KNN methods, tree-based methods make a tailored prediction using patterns in the data.\nTree-based methods are especially nice for imputation as they handle non-linearities and interactions, and can accomodate mixed data types, like continuous and categorical variables.\nHow Tree-Based Imputation Works\nBetter than Mean Imputation?\nThink of tree-based imputation like asking:\nThis is smarter than saying “everyone makes the same average income” (mean imputation) or even “let’s just look at your 3 closest neighbors” (KNN).",
    "crumbs": [
      "Feature Engineering",
      "Missing Data"
    ]
  },
  {
    "objectID": "week2_5c.html#missing-data-mechanisms",
    "href": "week2_5c.html#missing-data-mechanisms",
    "title": "Missing Data",
    "section": "Missing Data Mechanisms",
    "text": "Missing Data Mechanisms",
    "crumbs": [
      "Feature Engineering",
      "Missing Data"
    ]
  },
  {
    "objectID": "week2_5c.html#handling-missing-data-in-r",
    "href": "week2_5c.html#handling-missing-data-in-r",
    "title": "Missing Data",
    "section": "Handling Missing Data in R",
    "text": "Handling Missing Data in R",
    "crumbs": [
      "Feature Engineering",
      "Missing Data"
    ]
  },
  {
    "objectID": "week2_5c.html#missing-data-activity",
    "href": "week2_5c.html#missing-data-activity",
    "title": "Missing Data",
    "section": "Missing Data Activity",
    "text": "Missing Data Activity",
    "crumbs": [
      "Feature Engineering",
      "Missing Data"
    ]
  },
  {
    "objectID": "week2_5c.html#handling-missing-data",
    "href": "week2_5c.html#handling-missing-data",
    "title": "Missing Data",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data",
    "crumbs": [
      "Feature Engineering",
      "Missing Data"
    ]
  },
  {
    "objectID": "week2_5a.html",
    "href": "week2_5a.html",
    "title": "Overview",
    "section": "",
    "text": "Feature engineering in machine learning typically describes the process of creating, transforming, or selecting variables (features) from raw data to improve a model’s performance.\nFor example, one aspect of feature engineering is feature creation. This often means transforming raw data into meaningful inputs that better capture the underlying patterns the model needs to learn. For example, suppose we are trying to predict medicine adherence (whether or not someone takes their medicine). Our raw data may contain timestamps of when someone actually takes their medicine. We might use this timestamp to create new features for our model. For example, we might add features representing day of the week, holiday or weekend to provide additional context to the model.\nA number of domains fall under feature engineering:\nData leakage occurs when information from outside the training data set is used to create the model.\nData leakage often occurs during feature engineering.\nTo minimize data leaking, we will often want to do our feature engineering during the resampling, or data splitting, procedure we are using. To visualize this take a look at the graphic below from Boehmke and Greenwell (2019) where the pre-processing, or data engineering tasks, occur during each iteration. Keep this in mind as we introduce each feature engineering task.",
    "crumbs": [
      "Feature Engineering",
      "Overview"
    ]
  },
  {
    "objectID": "week2_5a.html#feature-engineering-and-data-leakage",
    "href": "week2_5a.html#feature-engineering-and-data-leakage",
    "title": "Overview",
    "section": "Feature Engineering and Data Leakage",
    "text": "Feature Engineering and Data Leakage",
    "crumbs": [
      "Feature Engineering",
      "Overview"
    ]
  },
  {
    "objectID": "week2_3.html",
    "href": "week2_3.html",
    "title": "Data Splitting",
    "section": "",
    "text": "Data splitting (e.g., dividing data into training, validation, and test samples) is crucial because it protects us from fooling ourselves about how well a model actually performs.\nIn our discussion of the training and validation samples we introduce the idea of parameters and hyperparameters. The distinction between these terms can often can trip people up, so it’s worth explaining the difference.\nIn groups of 3–4, discuss:\nSo, what does all this data splitting mean for us in practice? Let’s take a look at one of the more popular data-splitting methods used in practice: K-Folds Cross-Validation",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#parameters-vs.-hyperparameters",
    "href": "week2_3.html#parameters-vs.-hyperparameters",
    "title": "Data Splitting",
    "section": "Parameters vs. Hyperparameters",
    "text": "Parameters vs. Hyperparameters\n\nParameters are values learned automatically (or estimated) from the training data. The model parameters are often the thing we are interested in estimating in a given algorithm.\nFor example. the slope of the line in a simple linear regression prediction problem relating amount of treatment (feature) to depression score (target).\nHyperparameters are set before training begins, and control how the learning process works. The selection of hyperparameters is often critical to model performance, as in procedures like k-folds cross-validation, playing a critical role in how the model generalizes.\nFor example, the the number of clusters used to partition the data",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#role-of-parameters-in-data-splitting",
    "href": "week2_3.html#role-of-parameters-in-data-splitting",
    "title": "Data Splitting",
    "section": "Role of Parameters in Data Splitting",
    "text": "Role of Parameters in Data Splitting\nWhen we split data, each portion plays a distinct role:\n\nTraining set → Fit the model parameters.\n\nValidation set → Used to compare different hyperparameter choices.\n\nTest set → Used only once, at the very end, to estimate generalization performance.\n\nImportant note: Hyperparameters should be tuned using the validation set, not the test set. If we adjust hyperparameters based on the test set, we are indirectly training on it and lose the ability to measure real-world performance.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#example-k-means-clustering",
    "href": "week2_3.html#example-k-means-clustering",
    "title": "Data Splitting",
    "section": "Example: k-Means Clustering",
    "text": "Example: k-Means Clustering\n\nParameters (learned from the data)\n\nCluster centroids: the coordinates of the cluster centers.\n\nThese are calculated by the algorithm during training and adjusted iteratively until convergence.\n\nYou do not set them manually — the algorithm figures them out.\n\n\n\nHyperparameters (set before training)\n\nNumber of clusters (k): chosen by the user before running the algorithm.\n\nInitialization method (e.g., random, k-means++).\n\nMaximum number of iterations allowed.\n\nThese control how the algorithm runs, but are not learned from the data itself.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#prevents-overfitting-to-the-training-data",
    "href": "week2_3.html#prevents-overfitting-to-the-training-data",
    "title": "Data Splitting",
    "section": "Prevents Overfitting to the Training Data",
    "text": "Prevents Overfitting to the Training Data\n\nWhen a model is trained, it adapts to patterns in the training set.\n\nIf we only check performance on that same data, we might think the model is excellent — but it may just be memorizing instead of generalizing.\n\nA separate test set lets us see how the model behaves on new, unseen data.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#simulates-real-world-performance",
    "href": "week2_3.html#simulates-real-world-performance",
    "title": "Data Splitting",
    "section": "Simulates Real-World Performance",
    "text": "Simulates Real-World Performance\n\nIn real life, the model will be applied to data it hasn’t seen before.\n\nBy holding out a test set, we simulate that scenario, giving us a better sense of expected performance in practice.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#helps-with-model-selection-tuning",
    "href": "week2_3.html#helps-with-model-selection-tuning",
    "title": "Data Splitting",
    "section": "Helps with Model Selection & Tuning",
    "text": "Helps with Model Selection & Tuning\n\nA validation set (or cross-validation) is used to choose hyperparameters (like tree depth, learning rate, regularization strength).\n\nIf we tuned on the test set, we’d essentially be “leaking” information, and performance estimates would be biased upward.\n\nProper splitting ensures that the test set remains untouched until the very end.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#detects-data-leakage",
    "href": "week2_3.html#detects-data-leakage",
    "title": "Data Splitting",
    "section": "Detects Data Leakage",
    "text": "Detects Data Leakage\n\nSometimes information from the future or from labels sneaks into the features.\n\nIf this happens, the model might look perfect on the training set but fail on a clean split.\n\nData splitting may not actually help data leakage though. When might it help and when might it not?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#provides-a-fair-benchmark",
    "href": "week2_3.html#provides-a-fair-benchmark",
    "title": "Data Splitting",
    "section": "Provides a Fair Benchmark",
    "text": "Provides a Fair Benchmark\n\nIn research and industry, different models need to be compared on a common, untouched test set.\n\nWithout splitting, comparisons aren’t meaningful, since each model might overfit differently.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_1.html",
    "href": "week2_1.html",
    "title": "Important Terms and Distinctions",
    "section": "",
    "text": "The amount of jargon one finds when digging into the machine learning (ML) literature is a common source of confusion for new learners. Many ML terms are borrowed from statistics, computer science, or everyday language, but they can carry subtly or even radically different meanings in the context of ML.\nIn the following overview I will provide a high-level summary of some common terms used in machine learning, and try to highlight the differences between concepts that may seem familiar to you, but can represent distinct ideas.\nTwo important distinction I will try to highlight are the differences between:\nArtificial Intelligence (AI) and Machine Learning (ML) are related but distinct concepts in computer science and data analysis.\nML approaches rely heavily on accurate and efficient prediction algorithms.\nSo, what exactly are algorithms?\nAn algorithm is a step-by-step, well-defined procedure for solving a problem or completing a task.\nIn computing and machine learning, algorithms are generally a finite sequence of instructions that takes some input, follows a set of rules, and produces an output.\nA model in machine learning is the learned representation of patterns in data, produced by applying a learning algorithm to a dataset.\nA model in ML is the vehicle to make predictions, classify new examples, or infer relationships.\nUnderstanding what algorithm to choose for a specific problem or application is often difficult. In this class we will devote a lot of time on how to make these decisions, and communicate the results of an analysis. That said, we often have minimal knowledge of the problem or data at hand when we first approach an applied problem, and it is difficult to know which ML method will perform best. This idea is generally known as the no free lunch theorem\nAn algorithm is just a set of step-by-step instructions to solve a problem. Machine learning algorithms are no different—they’re just systematic ways of finding patterns in data. Let’s explore what that means.\nIn groups of 3–4, imagine you are writing an algorithm for a simple real-world problem:\nEach group should discuss (and prepare to share):",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#artificial-intelligence-ai",
    "href": "week2_1.html#artificial-intelligence-ai",
    "title": "Important Terms and Distinctions",
    "section": "Artificial Intelligence (AI)",
    "text": "Artificial Intelligence (AI)\n\nAI is the broad field of creating systems that can perform tasks that normally require human intelligence.\nTypically, the goal of AI is to create programs that can simulate intelligent behavior, whether or not they learn from data.\nCommon Examples of AI include:\n\nDigital assistants, LLMs\n\n\nCan you think of any examples of AI you encounter in your everyday life?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#machine-learning-ml",
    "href": "week2_1.html#machine-learning-ml",
    "title": "Important Terms and Distinctions",
    "section": "Machine Learning (ML)",
    "text": "Machine Learning (ML)\n\nML is a branch of AI focused on systems that learn patterns from data and improve automatically from experience.\nWhen doing ML we are generally interested in creating models that generalize well enough to make accurate predictions on unseen inputs.\nCommon use cases for ML in everyday life include:\n\nRecommendation systems, image recognition\n\n\nCan you think of any examples of ML you encounter in your everyday life?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#differences-between-ai-and-ml",
    "href": "week2_1.html#differences-between-ai-and-ml",
    "title": "Important Terms and Distinctions",
    "section": "Differences Between AI and ML",
    "text": "Differences Between AI and ML\n\n\n\n\n\n\n\n\n\n\nAspect\nArtificial Intelligence (AI)\nMachine Learning (ML)\n\n\n\n\nFocus\nSimulating intelligent behavior\nLearning patterns from data\n\n\nGoal\nBroader human-like intelligence\nSpecific predictive or decision-making tasks\n\n\nExamples\nChess engines, self-driving cars, expert systems\nRegression, neural networks, clustering\n\n\n\nBroadly speaking, ML is a branch of AI focused on training models to recognize patterns and make predictions using algorithms. AI encompasses a broader set of techniques, some of which do not involve learning from data at all.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#characteristics-of-an-algorithm",
    "href": "week2_1.html#characteristics-of-an-algorithm",
    "title": "Important Terms and Distinctions",
    "section": "Characteristics of an Algorithm",
    "text": "Characteristics of an Algorithm\n\nInput – Data the algorithm operates on\nOutput – The result produced after execution\nFiniteness – Must finish in a finite number of steps\nDefiniteness – Each step is clear and unambiguous\nEffectiveness – Each operation can actually be carried out",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#algorithms-in-machine-learning",
    "href": "week2_1.html#algorithms-in-machine-learning",
    "title": "Important Terms and Distinctions",
    "section": "Algorithms in Machine Learning",
    "text": "Algorithms in Machine Learning\n\nIn ML, an algorithm is the procedure used to train a model from data.\nExample: Linear regression algorithm finds the best-fit line by minimizing error.\nExample: Decision tree algorithm splits data into branches by checking features step by step.\n\nOne can think of an algorithm as the recipe one follows when cooking a meal. The ingredients of the dish are analogous to the input data, and the finished dish is equivalent to the model.\n\n\n\nhttps://xkcd.com/1667/\n\n\nSo, how does an algorithm differ from a model? What is a model in the context of ML?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#definition",
    "href": "week2_1.html#definition",
    "title": "Important Terms and Distinctions",
    "section": "Definition",
    "text": "Definition\nA model is the outcome of training a machine learning algorithm on data.It encapsulates the patterns, relationships, or structure discovered in the training data.\n\nInput: Features from the training data\n\nProcess: Algorithm (e.g., linear regression, decision tree, neural network)\n\nOutput: Parameters, structure, or rules that allow prediction",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#examples",
    "href": "week2_1.html#examples",
    "title": "Important Terms and Distinctions",
    "section": "Examples",
    "text": "Examples\n\n\n\n\n\n\n\n\nAlgorithm\nResulting Model\nWhat It Represents\n\n\n\n\nLinear Regression\nA line (y = mx + b)\nRelationship between input and output variables\n\n\nDecision Tree\nTree of splits\nHow features split to predict classes or values\n\n\nNeural Network\nLayers of neurons & weights\nComplex non-linear mappings between inputs and outputs\n\n\nk-Means Clustering\nCluster centroids\nGrouping of data points into clusters",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#things-to-remember",
    "href": "week2_1.html#things-to-remember",
    "title": "Important Terms and Distinctions",
    "section": "Things to Remember",
    "text": "Things to Remember\n\nThe algorithm is the procedure for learning\n\nThe model is the trained artifact used for prediction or inference\nDifferent algorithms can produce different models from the same data\nA model generalizes patterns from training data to unseen data\nAlgorithm = recipe (instructions for learning)\nModel = finished dish (learned representation ready to use)\n\nThe model is the end product of learning — the part that actually “knows” something about the data and can be used to make predictions. In ML: Data + Algorithm → Model → Predictions.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week1_3.html",
    "href": "week1_3.html",
    "title": "Describing Data Numerically",
    "section": "",
    "text": "Once you have read in a data frame it can be useful to know how the variables are understood by R. For example, let’s look at some Kaggle Marketing Analytics Data. You can download the raw data here.",
    "crumbs": [
      "Introduction to R",
      "Describing Data Numerically"
    ]
  },
  {
    "objectID": "1_1b.html",
    "href": "1_1b.html",
    "title": "R Packages",
    "section": "",
    "text": "One of the main reasons to be excited about R is the package library system. In R, packages are like apps. R packages extend the functionality of base R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded freely without much hassle (compared to many other package libraries). For example, we will often use the ggplot2 package for plotting and visualizing data and the psych package for describing data numerically.\nTo install R packages you can use the install.packages() function directly in the console as follows:\ninstall.packages(\"ggplot2\")\nor you can do so from the Packages tab of the Files pane in RStudio using the following steps:\nOnce a package is installed you can load it into your environment using the library() function. Once loaded you have access to all the R functions supplied by that package.\nlibrary(\"ggplot2\")\nThere are many ways to find useful R packages on the internet simply by googling the type of data or analysis you are interested in. Another way is to look at the CRAN Task Views. For example, there is an interesting Task View for Machine Learning that contains many relevant packages you might be interested in using. Some class members expressed interest in F1 racing. In the Sports Analytics Task View there is a f1dataR package that provides historical data from the beginning of Formula 1.",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "1_1b.html#using-r-packages",
    "href": "1_1b.html#using-r-packages",
    "title": "R Packages",
    "section": "Using R Packages",
    "text": "Using R Packages",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "1_1b.html#finding-useful-r-packages",
    "href": "1_1b.html#finding-useful-r-packages",
    "title": "R Packages",
    "section": "Finding Useful R Packages",
    "text": "Finding Useful R Packages",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "1_1.html",
    "href": "1_1.html",
    "title": "Basic R Programming",
    "section": "",
    "text": "Before getting started we will need to install R and RStudio.",
    "crumbs": [
      "Introduction to R",
      "Basic R Programming"
    ]
  },
  {
    "objectID": "1_2.html",
    "href": "1_2.html",
    "title": "Reading in Data",
    "section": "",
    "text": "One of the most important initial tasks you’ll face in R is reading in data. Let’s walk through some basics.\nBy reading in data we are generally refering to the process of importing data from a local directory on your computer, or from the web, into R. When we read a data file into R, we often read it in as a tabularobject where columns representing variables and rows representing cases. This is not always the case but covers the most basic use case.\nMany different data file formats can be read into R as data frames, such as .csv (comma separated values), .xlsx (Excel workbook), .txt (text), .sas7bdat (SAS), and .sav (SPSS) can be read into R. We will mostly focus on the .csv case in this class.",
    "crumbs": [
      "Introduction to R",
      "Reading in Data"
    ]
  },
  {
    "objectID": "week1_4.html",
    "href": "week1_4.html",
    "title": "Describing Data Visually",
    "section": "",
    "text": "This section will briefly introduce you to visualizing data using the ggplot2 package. R has a number of systems for making graphs but ggplot2 is by far the most developed option. ggplot2 implements the grammar of graphics, and if you’d like to learn more about the motivation for this work take a look at The Layered Grammar of Graphics.",
    "crumbs": [
      "Introduction to R",
      "Describing Data Visually"
    ]
  },
  {
    "objectID": "week2_2.html",
    "href": "week2_2.html",
    "title": "Problem Classes in ML",
    "section": "",
    "text": "Machine learning applications correspond to a wide variety of learning problems. Some major classes include:\nWe will now complete a short in-class exercise demonstrating how one might build a binary classifier to solve a classic classification problem.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#classification",
    "href": "week2_2.html#classification",
    "title": "Problem Classes in ML",
    "section": "Classification",
    "text": "Classification\n\nThe goal of classification is often toassign a category or label to each observation.\nThe music genre problem is a good example of a classification problem\nNote: The number of categories can be small, large, or even unbounded (e.g., optical character recognition, text classification, speech recognition).\n\n\n\n\nhttps://datahacker.rs/008-machine-learning-multiclass-classification-and-softmax-function/\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve classification problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#regression",
    "href": "week2_2.html#regression",
    "title": "Problem Classes in ML",
    "section": "Regression",
    "text": "Regression\n\nIn regression problems we are typically concerned with predicting a real-valued number for each item.\nFor example, we might be interested in predicting stock prices, or how .\nstressed out someone is.\nNotes: The penalty for prediction errors depends on the magnitude of the difference between true and predicted values, unlike classification where categories are discrete and there is often no notion of distance between various categories.\n\n\n\nhttps://builtin.com/data-science/regression-machine-learning\n\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve regression-type problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#ranking",
    "href": "week2_2.html#ranking",
    "title": "Problem Classes in ML",
    "section": "Ranking",
    "text": "Ranking\n\nWith ranking problems we are often tasked with ordering items according to a specific criterion.\nA well-known example of a ranking algorithm is Google’s PageRank algorithm.\nNotes: Ranking problems focus on relative order rather than exact category or numeric value.\n\n\n\n\nhttps://towardsdatascience.com/wp-content/uploads/2023/08/1ZFmd2Q-G95ArY93od20Adw.png\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve ranking problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#clustering",
    "href": "week2_2.html#clustering",
    "title": "Problem Classes in ML",
    "section": "Clustering",
    "text": "Clustering\n\nIn clustering problems we are typically looking topartition items into similar groups or regions.\nFor example, in social network analysis we are interested in identifying communities within large groups of people\nNotes: Clustering is often applied to very large datasets to reveal underlying structure.\n\n\n\n\nhttps://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\n\n\nHow do the classification and clustering problems discussed thus far differ?\nCan you think of any examples of algorithms you encounter in your everyday life that solve clustering problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#dimensionality-reduction",
    "href": "week2_2.html#dimensionality-reduction",
    "title": "Problem Classes in ML",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nWhen looking to solve dimension reduction problems we are typically interested in transforming high-dimensional data into a lower-dimensional representation while preserving important properties.\nCommon examples in psychology include identifying constructs in survey data or scales.\nNotes: Dimension reduction is also useful for feature extraction, noise reduction, and speeding up downstream learning tasks.\n\n\n\n\nhttps://www.sthda.com/english/sthda-upload/figures/principal-component-methods/006-principal-component-analysis-pca-variable-cos2-corrplot-1.png\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve dimension reduction problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_4.html",
    "href": "week2_4.html",
    "title": "Bias and Variance",
    "section": "",
    "text": "Prediction errors can be broken down into two main parts: bias and variance.\nThese represent different sources of mistakes a model can make.\nUsually, there’s a tradeoff between keeping bias low and keeping variance low. It can be exceptionally hard to minimize both at the same time.\nBy understanding where bias and variance come from, we can make better choices when building models and end up with more accurate predictions.\nLet’s approach each concept in turn.\nBias is the difference between the average prediction from our model and the true value which we are trying to predict.\nIn machine learning, bias often refers to the systematic error in a model.\nBias in ML can lead to incorrect or unfair outcomes, possibly from flawed or incomplete training data, or perhaps from poor assumptions made by the algorithm.\nA biased model fails to accurately reflect the true relationship in the data, leading to poor generalization and skewed predictions.\nIn statistics, variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value.\nIn machine learning, variance typically refers to the sensitivity of a model’s predictions to small changes in the training data.\nSo, error due to variance is defined as the variability of a model prediction for a given data point.\nBias and variance are often introduced together in the context of the bias-variance tradeoff that occurs when models overfit, or underfit the data.\nWhen a model overfits the data, it means the model has learned too much from the training data, including random noise or minor fluctuations, rather than just the true underlying patterns.\nWhen a model underfits the data, it means the model is too simple to capture the underlying patterns in the data.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_4.html#variance-in-machine-learning",
    "href": "week2_4.html#variance-in-machine-learning",
    "title": "Bias and Variance",
    "section": "Variance in Machine Learning",
    "text": "Variance in Machine Learning",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_4.html#the-bias-variance-tradeoff",
    "href": "week2_4.html#the-bias-variance-tradeoff",
    "title": "Bias and Variance",
    "section": "The Bias Variance Tradeoff",
    "text": "The Bias Variance Tradeoff",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_5b.html",
    "href": "week2_5b.html",
    "title": "Example Data",
    "section": "",
    "text": "To demonstrate feature engineering we will use some example data from the The General Social Survey (GSS).\nGSS is a long-running, nationally representative survey of adults in the United States that has been conducted almost every two years since 1972 by the National Opinion Research Center (NORC) at the University of Chicago. The GSS data is often used to measure American’s attitudes, behaviors, and beliefs on a wide range of topics—such as politics, religion, crime, race relations, family, work, and technology.\nThere are two ways to access the GSS data. One is using the GSS Data Explorer. Another option is using the gssr (Healy 2023) R package. Here we will use the gssr (Healy 2023) package to download some example data.\nWe will start by choosing happy as our target variable. This comes from Question 157 of the 2024 GSS where respondents were asked:\nResponses were coded such that 1 indicates “very happy”, 2 indicates “pretty happy”, and 3 indicates “not too happy”, while NA indicates “don’t know.”\nWe can also identify ad bunch of features we think predict self-reported happiness and save our final dataset.\nlibrary(gssr)\n\nPackage loaded. To attach the GSS data, type data(gss_all) at the console.\nFor the panel data and documentation, type e.g. data(gss_panel08_long) and data(gss_panel_doc).\nFor help on a specific GSS variable, type ?varname at the console.\n\ngss24 &lt;- gss_get_yr(2024)\n\nFetching: https://gss.norc.org/documents/stata/2024_stata.zip\n\ntarget &lt;- \"happy\"\n\nfeatures &lt;- c(\n  \"age\",     # age of participant\n  \"sex\",     # sex of participant\n  \"race\",    # race of participant\n  \"educ\",    # highest education completed by participant\n  \"income\",  # income of participant\n  \"childs\",  # number of children participant has\n  \"wrkstat\", # work force status\n  \"marital\", # marital status\n  \"born\",    # whether or not participant was born in USA\n  \"partyid\", # political party of participant\n  \"adults\",  # num of fam members 18 or older\n  \"earnrs\"   # number of earners in family  \n)\n\ndata &lt;- gss24[,c(target, features)]\n\ntable(data$happy, useNA = \"always\")\n\n\n   1    2    3 &lt;NA&gt; \n 684 1892  705   28\nNext, let’s clean up data a bit, discarding some of the labels and missing value information we don’t need. The data in gss24 retains the labeling structure provided by the GSS. Variables are stored numerically with labels attached to them. Often, when using the data in R, it will be convenient to convert the categorical variables we are interested in to character or factor type instead.\nHere we can use code from the gssr package introduction to simplify this recoding. The only thing we need to do is define the categorical variables in our data.\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ncat_vars &lt;- c(\"sex\",\"race\", \"educ\", \"wrkstat\", \"marital\", \"born\", \"partyid\")\ncont_vars &lt;- c(\"happy\", \"age\", \"income\", \"childs\", \"adults\", \"earnrs\")\n\ncapwords &lt;- function(x, strict = FALSE) {\n    cap &lt;- function(x) paste(toupper(substring(x, 1, 1)),\n                  {x &lt;- substring(x, 2); if(strict) tolower(x) else x},\n                             sep = \"\", collapse = \" \" )\n    sapply(strsplit(x, split = \" \"), cap, USE.NAMES = !is.null(names(x)))\n}\n\ndata &lt;- data |&gt;\n  mutate(\n    # Convert all missing to NA\n    across(everything(), haven::zap_missing), \n    across(all_of(cont_vars), as.numeric),\n    # Make all categorical variables factors and relabel nicely\n    across(all_of(cat_vars), forcats::as_factor),\n    across(all_of(cat_vars), \\(x) forcats::fct_relabel(x, capwords, strict = TRUE))\n  )",
    "crumbs": [
      "Feature Engineering",
      "Example Data"
    ]
  },
  {
    "objectID": "week2_5d.html",
    "href": "week2_5d.html",
    "title": "Feature Filtering",
    "section": "",
    "text": "Feature filtering is a feature selection technique in machine learning where features are evaluated prior to the model fitting based on statistical or heuristic criteria. Features are then kept or discarded based on those criteria.\nIt is called filtering because it is a preprocessing step, part of a feature engineering pipeline, done before we actually train the model.\nTypically, the goals of feature filtering are:\nIn practice, one of the filtering tasks we will typically conduct involves weeding out low variance features.\nZero and near-zero variance variables are low-hanging fruit to eliminate.\nZero and near-zero variance variables typically offer little to no information for model building. Furthermore, resampling (data-splitting) further complicates this picture because a given fold or sample may only contain a single value if the variable itself only contains a few unique values.\nBoehmke and Greenwell (2019) suggest the following rule-of-thumb for removing low-variance features:\nRemove a variable if:\nWe can use the caret (Kuhn and Max 2008) package in R to look at these different metrics for our example data.\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(caret)\n\nLoading required package: ggplot2\n\n\nLoading required package: lattice\n\ncaret::nearZeroVar(data, saveMetrics = TRUE) %&gt;% \n  tibble::rownames_to_column() \n\n   rowname freqRatio percentUnique zeroVar   nzv\n1    happy  2.683688    0.09066183   FALSE FALSE\n2      age  1.014085    2.17588395   FALSE FALSE\n3      sex  1.242672    0.06044122   FALSE FALSE\n4     race  3.986014    0.09066183   FALSE FALSE\n5     educ  1.191964    0.63463282   FALSE FALSE\n6   income 12.948571    0.36264733   FALSE FALSE\n7   childs  1.209166    0.27198549   FALSE FALSE\n8  wrkstat  1.834171    0.24176488   FALSE FALSE\n9  marital  1.287063    0.15110305   FALSE FALSE\n10    born  6.618056    0.06044122   FALSE FALSE\n11 partyid  1.558271    0.24176488   FALSE FALSE\n12  adults  1.075205    0.18132366   FALSE FALSE\n13  earnrs  1.560570    0.12088244   FALSE FALSE",
    "crumbs": [
      "Feature Engineering",
      "Feature Filtering"
    ]
  },
  {
    "objectID": "week2_5d.html#removing-low-variance-features",
    "href": "week2_5d.html#removing-low-variance-features",
    "title": "Feature Filtering",
    "section": "Removing Low-Variance Features",
    "text": "Removing Low-Variance Features",
    "crumbs": [
      "Feature Engineering",
      "Feature Filtering"
    ]
  },
  {
    "objectID": "week2_5f.html",
    "href": "week2_5f.html",
    "title": "Categorical Feature Engineering",
    "section": "",
    "text": "Attaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.10     ✔ rsample      1.3.1 \n✔ dials        1.4.2      ✔ tailor       0.1.0 \n✔ ggplot2      3.5.2      ✔ tidyr        1.3.1 \n✔ infer        1.0.9      ✔ tune         2.0.0 \n✔ modeldata    1.5.1      ✔ workflows    1.3.0 \n✔ parsnip      1.3.3      ✔ workflowsets 1.1.1 \n✔ purrr        1.1.0      ✔ yardstick    1.3.2 \n✔ recipes      1.3.1      \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\n\nMany models require that the predictors take numeric form. There are exceptionssuch as tree-based models, however, even tree-based methods can benefit from preprocessing categorical features. The following sections will discuss a few of the more common approaches to engineer categorical features.\n\nLumping\n\nSometimes features will contain levels that have very few observations. For example, take a look at the work status variable wrkstat. There are 8 unique levels and some have relatively few observations. For example, With A Job, But Not At Work Because Of Temporary Illness, Vacation, Strike.\n\ncount(data, wrkstat) %&gt;% arrange(n)\n\n# A tibble: 9 × 2\n  wrkstat                                                                      n\n  &lt;fct&gt;                                                                    &lt;int&gt;\n1 &lt;NA&gt;                                                                        10\n2 With A Job, But Not At Work Because Of Temporary Illness, Vacation, Str…    61\n3 In School                                                                   90\n4 Other                                                                      113\n5 Unemployed, Laid Off, Looking For Work                                     168\n6 Keeping House                                                              292\n7 Working Part Time                                                          319\n8 Retired                                                                    796\n9 Working Full Time                                                         1460\n\n\nSometimes we can benefit from collapsing, or “lumping” these into a lesser number of categories. In the above examples, we may want to collapse all levels that are observed in less than 5% of the training sample into an Other category. We can use step_other() to do so.\n\nlumping &lt;- recipe(happy ~ ., data = data) %&gt;%\n  step_other(wrkstat, threshold = 0.05, \n             other = \"Other\")\n\n# Apply this blue print --&gt; you will learn about this at \n# the end of the chapter\napply_2_training &lt;- prep(lumping, training = data) %&gt;%\n  bake(data)\n\n# New distribution of Neighborhood\ncount(apply_2_training, wrkstat) %&gt;% arrange(n)\n\n# A tibble: 7 × 2\n  wrkstat                                    n\n  &lt;fct&gt;                                  &lt;int&gt;\n1 &lt;NA&gt;                                      10\n2 Unemployed, Laid Off, Looking For Work   168\n3 Other                                    264\n4 Keeping House                            292\n5 Working Part Time                        319\n6 Retired                                  796\n7 Working Full Time                       1460\n\n\n\nOne-Hot and Dummy Encoding\n\nAs mentioned previously many models require that all features be numeric. Consequently, we need to intelligently transform any categorical variables into numeric representations so that these algorithms will work.\nThere are many ways to recode categorical variables as numeric (e.g., one-hot, ordinal, binary, sum, and Helmert).\nOne-hot encoding is a common method for converting categorical variables into a numerical format that machine learning algorithms can work with. Instead of assigning arbitrary numbers to categories (which could incorrectly imply an order), one-hot encoding creates a new binary (0/1) column for each category level. For a given observation, the column corresponding to its category is set to 1, and all others are set to 0.\n\n\n\nhttps://towardsdatascience.com/encoding-categorical-variables-one-hot-vs-dummy-encoding-6d5b9c46e2db/\n\n\nHowever, this creates perfect collinearity which causes problems with some predictive modeling algorithms (e.g., ordinary linear regression and neural networks). Alternatively, we can create a full-rank encoding by dropping one of the levels (level blue has been dropped). This is referred to as dummy coding.\n\n\n\nhttps://towardsdatascience.com/encoding-categorical-variables-one-hot-vs-dummy-encoding-6d5b9c46e2db/\n\n\nBelow is an example of one-hot encoding the predictors in our model.\n\nrecipe(happy ~ ., data = data) %&gt;%\n  step_dummy(all_factor_predictors(), one_hot = TRUE)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 12\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_factor_predictors()",
    "crumbs": [
      "Feature Engineering",
      "Categorical Feature Engineering"
    ]
  },
  {
    "objectID": "week3_1.html",
    "href": "week3_1.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Simple linear regression is a statistical method used to model the relationship between two variables:\n- one feature (also called the predictor or independent variable),\n- one target (also called the outcome or dependent variable).\nThe general form of a simple linear regression model is:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\quad \\quad \\text{for}\\:i=1,2,\\dots,n\n\\]\nwhere:\n- \\(Y_i\\) : the ith observation on the target variable we want to predict.\n- \\(X\\) : the ith observation of the feature (or predictor) variable.\n- \\(\\beta_0\\): the intercept, which represents the expected value of \\(Y\\) when \\(X=0\\).\n- \\(\\beta_1\\): the slope, which tells us how much \\(Y\\) changes for a one-unit increase in \\(X\\)\n- \\(\\varepsilon_i\\): the ith observation of the error term, which captures random variation not explained by \\(X\\).\nIn practice, we estimate the parameters \\(\\beta_0\\) and \\(\\beta_1\\) using data by minimizing the residual sum of squares (RSS). It can be helpful to visualize what is meant by residual sum of squares and the fitted or optimal regression line.\nOnce we have estimated the coefficients by finding the coefficients that minimize the RSS, the fitted equation can then be written as:\n\\[\n\\hat{Y}_i = b_0 + b_1 X_i\n\\]\nwhere \\(\\hat{Y}_i\\) is the predicted value of \\(Y\\) for the ith case (or person), and \\(b_0, b_1\\) are the estimated coefficients from the sample data.\nThis simple framework allows us to understand how two variables are related and to make predictions about \\(Y\\) given new values of \\(X\\).\nThe Ames Housing dataset is a widely used dataset in regression and machine learning tutorials. It contains detailed information about residential homes in Ames, Iowa, and is often used to predict sale prices of homes based on various characteristics. For the regression chapters we will use the Ames Housing dataset to illustrate concepts in linear regression.\nThe dataset includes a mix of numerical, categorical, and ordinal variables covering:\n# Install and load package\n# install.packages(\"AmesHousing\")\nlibrary(AmesHousing)\n\n# Load data\names &lt;- make_ames()\n\n# View first few rows\nhead(ames)\n\n# A tibble: 6 × 81\n  MS_SubClass             MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape\n  &lt;fct&gt;                   &lt;fct&gt;            &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;    \n1 One_Story_1946_and_New… Resident…          141    31770 Pave   No_A… Slightly…\n2 One_Story_1946_and_New… Resident…           80    11622 Pave   No_A… Regular  \n3 One_Story_1946_and_New… Resident…           81    14267 Pave   No_A… Slightly…\n4 One_Story_1946_and_New… Resident…           93    11160 Pave   No_A… Regular  \n5 Two_Story_1946_and_New… Resident…           74    13830 Pave   No_A… Slightly…\n6 Two_Story_1946_and_New… Resident…           78     9978 Pave   No_A… Slightly…\n# ℹ 74 more variables: Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;, Lot_Config &lt;fct&gt;,\n#   Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;,\n#   Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, Overall_Qual &lt;fct&gt;, Overall_Cond &lt;fct&gt;,\n#   Year_Built &lt;int&gt;, Year_Remod_Add &lt;int&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;,\n#   Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;,\n#   Mas_Vnr_Area &lt;dbl&gt;, Exter_Qual &lt;fct&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;,\n#   Bsmt_Qual &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;, …\n\n# Summary of key variables\nsummary(ames$Sale_Price)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12789  129500  160000  180796  213500  755000 \n\nsummary(ames$Gr_Liv_Area)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    334    1126    1442    1500    1743    5642 \n\ntable(ames$Neighborhood)\n\n\n                             North_Ames                           College_Creek \n                                    443                                     267 \n                               Old_Town                                 Edwards \n                                    239                                     194 \n                               Somerset                      Northridge_Heights \n                                    182                                     166 \n                                Gilbert                                  Sawyer \n                                    165                                     151 \n                         Northwest_Ames                             Sawyer_West \n                                    131                                     125 \n                               Mitchell                               Brookside \n                                    114                                     108 \n                               Crawford                  Iowa_DOT_and_Rail_Road \n                                    103                                      93 \n                             Timberland                              Northridge \n                                     72                                      71 \n                            Stone_Brook South_and_West_of_Iowa_State_University \n                                     51                                      48 \n                            Clear_Creek                          Meadow_Village \n                                     44                                      37 \n                              Briardale                     Bloomington_Heights \n                                     30                                      28 \n                                Veenker                         Northpark_Villa \n                                     24                                      23 \n                                Blueste                                  Greens \n                                     10                                       8 \n                            Green_Hills                                Landmark \n                                      2                                       1 \n                            Hayden_Lake \n                                      0\nWe can fit a simple linear regression model using the lm() function in R. Note we are not using cross-validation here, simply fitting the training data to illustrate the concept of simple linear regression.\nFor example, if we want to predict happiness (Sale_Price) based on house square footage (Gr_Liv_Area), we can use the following code:\nfit_slr &lt;- lm(\n  Sale_Price ~ Gr_Liv_Area, \n  data = ames\n)\nsummary(fit_slr)\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-483467  -30219   -1966   22728  334323 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13289.634   3269.703   4.064 4.94e-05 ***\nGr_Liv_Area   111.694      2.066  54.061  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56520 on 2928 degrees of freedom\nMultiple R-squared:  0.4995,    Adjusted R-squared:  0.4994 \nF-statistic:  2923 on 1 and 2928 DF,  p-value: &lt; 2.2e-16\nWe can interpret the results from summary in the following way:\nThe estimated coefficients from our model are \\(\\beta_0=13,289\\) and \\(\\beta_1=111.7\\).\nWe interpret the intercept, \\(\\beta_0=13,289\\), as the estimated mean sale price when the square footage is 0. While this interpretation may not be meaningful in a real-world context (since a house with 0 square footage does not exist), it serves as a baseline for our model.\nWe interpret the slope coefficient, \\(\\beta_1=111.7\\), as the estimated change in mean sale price for each additional square foot of living area. Specifically, for each one-unit increase in square footage, the mean sale price is estimated to increase by $111.7, holding all else constant. This indicates a positive relationship between square footage and sale price in our sample.\nIn addition to estimating the coefficients, we can also perform hypothesis tests and construct confidence intervals to assess the statistical significance and precision of our estimates.\nIn the simple linear regression model\n\\[\nY_i = \\beta_0 + \\beta_1 X + \\varepsilon_i\n\\]\nthe error term \\(\\varepsilon\\) is at the heart of the model.\nClassical linear regression (the Gauss–Markov framework) makes several key assumptions about \\(\\varepsilon\\):\nIn practice, \\(\\sigma^2\\) is unknown, so we estimate it with the residual variance:\n\\[\ns^2 = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n - 2}\n\\]\nwhere:\n- \\(y_i\\): the observed outcome,\n- \\(\\hat{y}_i\\): the predicted outcome,\n- \\(n\\): the sample size,\n- denominator \\(n-2\\) accounts for the two estimated parameters (\\(\\beta_0, \\beta_1\\)).",
    "crumbs": [
      "Linear Regression",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "week3_1.html#example-data-ames-housing",
    "href": "week3_1.html#example-data-ames-housing",
    "title": "Simple Linear Regression",
    "section": "Example Data: Ames Housing",
    "text": "Example Data: Ames Housing",
    "crumbs": [
      "Linear Regression",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "week3_1.html#simple-linear-regression-in-r",
    "href": "week3_1.html#simple-linear-regression-in-r",
    "title": "Simple Linear Regression",
    "section": "Simple Linear Regression in R",
    "text": "Simple Linear Regression in R",
    "crumbs": [
      "Linear Regression",
      "Simple Linear Regression"
    ]
  }
]