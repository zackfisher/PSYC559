[
  {
    "objectID": "week1_1b.html",
    "href": "week1_1b.html",
    "title": "R Packages",
    "section": "",
    "text": "One of the main reasons to be excited about R is the package library system. In R, packages are like apps. R packages extend the functionality of base R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded freely without much hassle (compared to many other package libraries). For example, we will often use the ggplot2 package for plotting and visualizing data and the psych package for describing data numerically.\nTo install R packages you can use the install.packages() function directly in the console as follows:\ninstall.packages(\"ggplot2\")\nor you can do so from the Packages tab of the Files pane in RStudio using the following steps:\nOnce a package is installed you can load it into your environment using the library() function. Once loaded you have access to all the R functions supplied by that package.\nlibrary(\"ggplot2\")\nThere are many ways to find useful R packages on the internet simply by googling the type of data or analysis you are interested in. Another way is to look at the CRAN Task Views. For example, there is an interesting Task View for Machine Learning that contains many relevant packages you might be interested in using. Some class members expressed interest in F1 racing. In the Sports Analytics Task View there is a f1dataR package that provides historical data from the beginning of Formula 1.",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "week1_1b.html#using-r-packages",
    "href": "week1_1b.html#using-r-packages",
    "title": "R Packages",
    "section": "Using R Packages",
    "text": "Using R Packages",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "week1_1b.html#finding-useful-r-packages",
    "href": "week1_1b.html#finding-useful-r-packages",
    "title": "R Packages",
    "section": "Finding Useful R Packages",
    "text": "Finding Useful R Packages",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "week2_2b.html#examples-items-instances-or-cases",
    "href": "week2_2b.html#examples-items-instances-or-cases",
    "title": "Terminology Notes",
    "section": "Examples, Items, Instances or Cases",
    "text": "Examples, Items, Instances or Cases\n\nDefinition: Individual data points used for learning or evaluation.\n\nExample: Each participant in a study, with their responses to a cognitive task or survey questionnaire."
  },
  {
    "objectID": "week2_2b.html#features",
    "href": "week2_2b.html#features",
    "title": "Terminology Notes",
    "section": "Features",
    "text": "Features\n\nDefinition: Attributes or variables associated with each example, often represented as a vector.\n\nExample: Participant age, gender, response times, scores on survey items, or physiological measures (e.g., heart rate, skin conductance)."
  },
  {
    "objectID": "week2_2b.html#labels",
    "href": "week2_2b.html#labels",
    "title": "Terminology Notes",
    "section": "Labels",
    "text": "Labels\n\nDefinition: The outcome, value, or category assigned to each example.\n\nExample:\n\nClassification: Diagnosed vs. non-diagnosed for depression.\n\nRegression: Continuous depression score on a scale from 0–30."
  },
  {
    "objectID": "week2_2b.html#training-sample",
    "href": "week2_2b.html#training-sample",
    "title": "Terminology Notes",
    "section": "Training Sample",
    "text": "Training Sample\n\nDefinition: The subset of examples used to train the learning algorithm.\n\nExample: 100 participants’ data with known depression scores used to train a predictive model."
  },
  {
    "objectID": "week2_2b.html#validation-sample",
    "href": "week2_2b.html#validation-sample",
    "title": "Terminology Notes",
    "section": "Validation Sample",
    "text": "Validation Sample\n\nDefinition: Examples used to tune algorithm parameters (e.g., regularization strength, number of neurons in a network).\n\nExample: 30 participants’ data used to select the optimal model settings or tuning parameters before final evaluation."
  },
  {
    "objectID": "week2_2b.html#test-sample",
    "href": "week2_2b.html#test-sample",
    "title": "Terminology Notes",
    "section": "Test Sample",
    "text": "Test Sample\n\nDefinition: Examples used to evaluate model performance, separate from training and validation. Examples used to evaluate the performance of a learning algorithm. The test sample is not made available in the learning stage.\nExample: 50 new participants whose depression scores the model predicts; predicted scores are compared to actual scores to assess accuracy."
  },
  {
    "objectID": "week2_2b.html#loss-function",
    "href": "week2_2b.html#loss-function",
    "title": "Terminology Notes",
    "section": "Loss Function",
    "text": "Loss Function\n\nDefinition: A function that measures the difference between predicted labels and true labels.\n\nExample:\nRegression: Squared loss: L(y, y′) = (y′ − y)², e.g., predicted vs. actual depression score.\n\nClassification: Zero-one loss: L(y, y′) = 1 if prediction ≠ true label, e.g., predicted diagnosis vs. actual diagnosis."
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "PSYC559: Applied Machine Learning in Psychology",
    "section": "",
    "text": "https://zackfisher.github.io/PSYC559/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSC559: Applied Machine Learning in Psychology",
    "section": "",
    "text": "Welcome to the PSC559: Applied Machine Learning in Psychology!\nAs the class progresses I will update this front matter with a general course overview and important announcements.\nIf you find errors or typos in any of these materials please let me know."
  },
  {
    "objectID": "week2_4.html",
    "href": "week2_4.html",
    "title": "Bias and Variance",
    "section": "",
    "text": "Prediction errors can be broken down into two main parts: bias and variance.\nThese represent different sources of mistakes a model can make.\nUsually, there’s a tradeoff between keeping bias low and keeping variance low. It can be exceptionally hard to minimize both at the same time.\nBy understanding where bias and variance come from, we can make better choices when building models and end up with more accurate predictions.\nLet’s approach each concept in turn.\nBias is the difference between the average prediction from our model and the true value which we are trying to predict.\nIn machine learning, bias often refers to the systematic error in a model.\nBias in ML can lead to incorrect or unfair outcomes, possibly from flawed or incomplete training data, or perhaps from poor assumptions made by the algorithm.\nA biased model fails to accurately reflect the true relationship in the data, leading to poor generalization and skewed predictions.\nIn statistics, variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value.\nIn machine learning, variance typically refers to the sensitivity of a model’s predictions to small changes in the training data.\nSo, error due to variance is defined as the variability of a model prediction for a given data point.\nBias and variance are often introduced together in the context of the bias-variance tradeoff that occurs when models overfit, or underfit the data.\nWhen a model overfits the data, it means the model has learned too much from the training data, including random noise or minor fluctuations, rather than just the true underlying patterns.\nWhen a model underfits the data, it means the model is too simple to capture the underlying patterns in the data.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_4.html#variance-in-machine-learning",
    "href": "week2_4.html#variance-in-machine-learning",
    "title": "Bias and Variance",
    "section": "Variance in Machine Learning",
    "text": "Variance in Machine Learning",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_4.html#the-bias-variance-tradeoff",
    "href": "week2_4.html#the-bias-variance-tradeoff",
    "title": "Bias and Variance",
    "section": "The Bias Variance Tradeoff",
    "text": "The Bias Variance Tradeoff",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_3.html",
    "href": "week2_3.html",
    "title": "Data Splitting",
    "section": "",
    "text": "Data splitting (e.g., dividing data into training, validation, and test samples) is crucial because it protects us from fooling ourselves about how well a model actually performs.\nIn our discussion of the training and validation samples we introduce the idea of parameters and hyperparameters. The distinction between these terms can often can trip people up, so it’s worth explaining the difference.\nIn groups of 3–4, discuss:\nSo, what does all this data splitting mean for us in practice? Let’s take a look at one of the more popular data-splitting methods used in practice: K-Folds Cross-Validation",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#parameters-vs.-hyperparameters",
    "href": "week2_3.html#parameters-vs.-hyperparameters",
    "title": "Data Splitting",
    "section": "Parameters vs. Hyperparameters",
    "text": "Parameters vs. Hyperparameters\n\nParameters are values learned automatically (or estimated) from the training data. The model parameters are often the thing we are interested in estimating in a given algorithm.\nFor example. the slope of the line in a simple linear regression prediction problem relating amount of treatment (feature) to depression score (target).\nHyperparameters are set before training begins, and control how the learning process works. The selection of hyperparameters is often critical to model performance, as in procedures like k-folds cross-validation, playing a critical role in how the model generalizes.\nFor example, the the number of clusters used to partition the data",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#role-of-parameters-in-data-splitting",
    "href": "week2_3.html#role-of-parameters-in-data-splitting",
    "title": "Data Splitting",
    "section": "Role of Parameters in Data Splitting",
    "text": "Role of Parameters in Data Splitting\nWhen we split data, each portion plays a distinct role:\n\nTraining set → Fit the model parameters.\n\nValidation set → Used to compare different hyperparameter choices.\n\nTest set → Used only once, at the very end, to estimate generalization performance.\n\nImportant note: Hyperparameters should be tuned using the validation set, not the test set. If we adjust hyperparameters based on the test set, we are indirectly training on it and lose the ability to measure real-world performance.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#example-k-means-clustering",
    "href": "week2_3.html#example-k-means-clustering",
    "title": "Data Splitting",
    "section": "Example: k-Means Clustering",
    "text": "Example: k-Means Clustering\n\nParameters (learned from the data)\n\nCluster centroids: the coordinates of the cluster centers.\n\nThese are calculated by the algorithm during training and adjusted iteratively until convergence.\n\nYou do not set them manually — the algorithm figures them out.\n\n\n\nHyperparameters (set before training)\n\nNumber of clusters (k): chosen by the user before running the algorithm.\n\nInitialization method (e.g., random, k-means++).\n\nMaximum number of iterations allowed.\n\nThese control how the algorithm runs, but are not learned from the data itself.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#prevents-overfitting-to-the-training-data",
    "href": "week2_3.html#prevents-overfitting-to-the-training-data",
    "title": "Data Splitting",
    "section": "Prevents Overfitting to the Training Data",
    "text": "Prevents Overfitting to the Training Data\n\nWhen a model is trained, it adapts to patterns in the training set.\n\nIf we only check performance on that same data, we might think the model is excellent — but it may just be memorizing instead of generalizing.\n\nA separate test set lets us see how the model behaves on new, unseen data.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#simulates-real-world-performance",
    "href": "week2_3.html#simulates-real-world-performance",
    "title": "Data Splitting",
    "section": "Simulates Real-World Performance",
    "text": "Simulates Real-World Performance\n\nIn real life, the model will be applied to data it hasn’t seen before.\n\nBy holding out a test set, we simulate that scenario, giving us a better sense of expected performance in practice.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#helps-with-model-selection-tuning",
    "href": "week2_3.html#helps-with-model-selection-tuning",
    "title": "Data Splitting",
    "section": "Helps with Model Selection & Tuning",
    "text": "Helps with Model Selection & Tuning\n\nA validation set (or cross-validation) is used to choose hyperparameters (like tree depth, learning rate, regularization strength).\n\nIf we tuned on the test set, we’d essentially be “leaking” information, and performance estimates would be biased upward.\n\nProper splitting ensures that the test set remains untouched until the very end.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#detects-data-leakage",
    "href": "week2_3.html#detects-data-leakage",
    "title": "Data Splitting",
    "section": "Detects Data Leakage",
    "text": "Detects Data Leakage\n\nSometimes information from the future or from labels sneaks into the features.\n\nIf this happens, the model might look perfect on the training set but fail on a clean split.\n\nData splitting may not actually help data leakage though. When might it help and when might it not?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#provides-a-fair-benchmark",
    "href": "week2_3.html#provides-a-fair-benchmark",
    "title": "Data Splitting",
    "section": "Provides a Fair Benchmark",
    "text": "Provides a Fair Benchmark\n\nIn research and industry, different models need to be compared on a common, untouched test set.\n\nWithout splitting, comparisons aren’t meaningful, since each model might overfit differently.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week1_4.html",
    "href": "week1_4.html",
    "title": "Describing Data Visually",
    "section": "",
    "text": "This section will briefly introduce you to visualizing data using the ggplot2 package. R has a number of systems for making graphs but ggplot2 is by far the most developed option. ggplot2 implements the grammar of graphics, and if you’d like to learn more about the motivation for this work take a look at The Layered Grammar of Graphics.",
    "crumbs": [
      "Introduction to R",
      "Describing Data Visually"
    ]
  },
  {
    "objectID": "week1_2.html",
    "href": "week1_2.html",
    "title": "Reading in Data",
    "section": "",
    "text": "One of the most important initial tasks you’ll face in R is reading in data. Let’s walk through some basics.\nBy reading in data we are generally refering to the process of importing data from a local directory on your computer, or from the web, into R. When we read a data file into R, we often read it in as a tabularobject where columns representing variables and rows representing cases. This is not always the case but covers the most basic use case.\nMany different data file formats can be read into R as data frames, such as .csv (comma separated values), .xlsx (Excel workbook), .txt (text), .sas7bdat (SAS), and .sav (SPSS) can be read into R. We will mostly focus on the .csv case in this class.",
    "crumbs": [
      "Introduction to R",
      "Reading in Data"
    ]
  },
  {
    "objectID": "week1_3.html",
    "href": "week1_3.html",
    "title": "Describing Data Numerically",
    "section": "",
    "text": "Once you have read in a data frame it can be useful to know how the variables are understood by R. For example, let’s look at some Kaggle Marketing Analytics Data. You can download the raw data here.",
    "crumbs": [
      "Introduction to R",
      "Describing Data Numerically"
    ]
  },
  {
    "objectID": "week1_1.html",
    "href": "week1_1.html",
    "title": "Basic R Programming",
    "section": "",
    "text": "Before getting started we will need to install R and RStudio.",
    "crumbs": [
      "Introduction to R",
      "Basic R Programming"
    ]
  },
  {
    "objectID": "week2_2.html",
    "href": "week2_2.html",
    "title": "Problem Classes in ML",
    "section": "",
    "text": "Machine learning applications correspond to a wide variety of learning problems. Some major classes include:\nWe will now complete a short in-class exercise demonstrating how one might build a binary classifier to solve a classic classification problem.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#classification",
    "href": "week2_2.html#classification",
    "title": "Problem Classes in ML",
    "section": "Classification",
    "text": "Classification\n\nThe goal of classification is often toassign a category or label to each observation.\nThe music genre problem is a good example of a classification problem\nNote: The number of categories can be small, large, or even unbounded (e.g., optical character recognition, text classification, speech recognition).\n\n\n\n\nhttps://datahacker.rs/008-machine-learning-multiclass-classification-and-softmax-function/\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve classification problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#regression",
    "href": "week2_2.html#regression",
    "title": "Problem Classes in ML",
    "section": "Regression",
    "text": "Regression\n\nIn regression problems we are typically concerned with predicting a real-valued number for each item.\nFor example, we might be interested in predicting stock prices, or how .\nstressed out someone is.\nNotes: The penalty for prediction errors depends on the magnitude of the difference between true and predicted values, unlike classification where categories are discrete and there is often no notion of distance between various categories.\n\n\n\nhttps://builtin.com/data-science/regression-machine-learning\n\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve regression-type problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#ranking",
    "href": "week2_2.html#ranking",
    "title": "Problem Classes in ML",
    "section": "Ranking",
    "text": "Ranking\n\nWith ranking problems we are often tasked with ordering items according to a specific criterion.\nA well-known example of a ranking algorithm is Google’s PageRank algorithm.\nNotes: Ranking problems focus on relative order rather than exact category or numeric value.\n\n\n\n\nhttps://towardsdatascience.com/wp-content/uploads/2023/08/1ZFmd2Q-G95ArY93od20Adw.png\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve ranking problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#clustering",
    "href": "week2_2.html#clustering",
    "title": "Problem Classes in ML",
    "section": "Clustering",
    "text": "Clustering\n\nIn clustering problems we are typically looking topartition items into similar groups or regions.\nFor example, in social network analysis we are interested in identifying communities within large groups of people\nNotes: Clustering is often applied to very large datasets to reveal underlying structure.\n\n\n\n\nhttps://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\n\n\nHow do the classification and clustering problems discussed thus far differ?\nCan you think of any examples of algorithms you encounter in your everyday life that solve clustering problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#dimensionality-reduction",
    "href": "week2_2.html#dimensionality-reduction",
    "title": "Problem Classes in ML",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nWhen looking to solve dimension reduction problems we are typically interested in transforming high-dimensional data into a lower-dimensional representation while preserving important properties.\nCommon examples in psychology include identifying constructs in survey data or scales.\nNotes: Dimension reduction is also useful for feature extraction, noise reduction, and speeding up downstream learning tasks.\n\n\n\n\nhttps://www.sthda.com/english/sthda-upload/figures/principal-component-methods/006-principal-component-analysis-pca-variable-cos2-corrplot-1.png\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve dimension reduction problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_1.html",
    "href": "week2_1.html",
    "title": "Important Terms and Distinctions",
    "section": "",
    "text": "The amount of jargon one finds when digging into the machine learning (ML) literature is a common source of confusion for new learners. Many ML terms are borrowed from statistics, computer science, or everyday language, but they can carry subtly or even radically different meanings in the context of ML.\nIn the following overview I will provide a high-level summary of some common terms used in machine learning, and try to highlight the differences between concepts that may seem familiar to you, but can represent distinct ideas.\nTwo important distinction I will try to highlight are the differences between:\nArtificial Intelligence (AI) and Machine Learning (ML) are related but distinct concepts in computer science and data analysis.\nML approaches rely heavily on accurate and efficient prediction algorithms.\nSo, what exactly are algorithms?\nAn algorithm is a step-by-step, well-defined procedure for solving a problem or completing a task.\nIn computing and machine learning, algorithms are generally a finite sequence of instructions that takes some input, follows a set of rules, and produces an output.\nA model in machine learning is the learned representation of patterns in data, produced by applying a learning algorithm to a dataset.\nA model in ML is the vehicle to make predictions, classify new examples, or infer relationships.\nUnderstanding what algorithm to choose for a specific problem or application is often difficult. In this class we will devote a lot of time on how to make these decisions, and communicate the results of an analysis. That said, we often have minimal knowledge of the problem or data at hand when we first approach an applied problem, and it is difficult to know which ML method will perform best. This idea is generally known as the no free lunch theorem\nAn algorithm is just a set of step-by-step instructions to solve a problem. Machine learning algorithms are no different—they’re just systematic ways of finding patterns in data. Let’s explore what that means.\nIn groups of 3–4, imagine you are writing an algorithm for a simple real-world problem:\nEach group should discuss (and prepare to share):",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#artificial-intelligence-ai",
    "href": "week2_1.html#artificial-intelligence-ai",
    "title": "Important Terms and Distinctions",
    "section": "Artificial Intelligence (AI)",
    "text": "Artificial Intelligence (AI)\n\nAI is the broad field of creating systems that can perform tasks that normally require human intelligence.\nTypically, the goal of AI is to create programs that can simulate intelligent behavior, whether or not they learn from data.\nCommon Examples of AI include:\n\nDigital assistants, LLMs\n\n\nCan you think of any examples of AI you encounter in your everyday life?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#machine-learning-ml",
    "href": "week2_1.html#machine-learning-ml",
    "title": "Important Terms and Distinctions",
    "section": "Machine Learning (ML)",
    "text": "Machine Learning (ML)\n\nML is a branch of AI focused on systems that learn patterns from data and improve automatically from experience.\nWhen doing ML we are generally interested in creating models that generalize well enough to make accurate predictions on unseen inputs.\nCommon use cases for ML in everyday life include:\n\nRecommendation systems, image recognition\n\n\nCan you think of any examples of ML you encounter in your everyday life?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#differences-between-ai-and-ml",
    "href": "week2_1.html#differences-between-ai-and-ml",
    "title": "Important Terms and Distinctions",
    "section": "Differences Between AI and ML",
    "text": "Differences Between AI and ML\n\n\n\n\n\n\n\n\n\n\nAspect\nArtificial Intelligence (AI)\nMachine Learning (ML)\n\n\n\n\nFocus\nSimulating intelligent behavior\nLearning patterns from data\n\n\nGoal\nBroader human-like intelligence\nSpecific predictive or decision-making tasks\n\n\nExamples\nChess engines, self-driving cars, expert systems\nRegression, neural networks, clustering\n\n\n\nBroadly speaking, ML is a branch of AI focused on training models to recognize patterns and make predictions using algorithms. AI encompasses a broader set of techniques, some of which do not involve learning from data at all.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#characteristics-of-an-algorithm",
    "href": "week2_1.html#characteristics-of-an-algorithm",
    "title": "Important Terms and Distinctions",
    "section": "Characteristics of an Algorithm",
    "text": "Characteristics of an Algorithm\n\nInput – Data the algorithm operates on\nOutput – The result produced after execution\nFiniteness – Must finish in a finite number of steps\nDefiniteness – Each step is clear and unambiguous\nEffectiveness – Each operation can actually be carried out",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#algorithms-in-machine-learning",
    "href": "week2_1.html#algorithms-in-machine-learning",
    "title": "Important Terms and Distinctions",
    "section": "Algorithms in Machine Learning",
    "text": "Algorithms in Machine Learning\n\nIn ML, an algorithm is the procedure used to train a model from data.\nExample: Linear regression algorithm finds the best-fit line by minimizing error.\nExample: Decision tree algorithm splits data into branches by checking features step by step.\n\nOne can think of an algorithm as the recipe one follows when cooking a meal. The ingredients of the dish are analogous to the input data, and the finished dish is equivalent to the model.\n\n\n\nhttps://xkcd.com/1667/\n\n\nSo, how does an algorithm differ from a model? What is a model in the context of ML?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#definition",
    "href": "week2_1.html#definition",
    "title": "Important Terms and Distinctions",
    "section": "Definition",
    "text": "Definition\nA model is the outcome of training a machine learning algorithm on data.It encapsulates the patterns, relationships, or structure discovered in the training data.\n\nInput: Features from the training data\n\nProcess: Algorithm (e.g., linear regression, decision tree, neural network)\n\nOutput: Parameters, structure, or rules that allow prediction",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#examples",
    "href": "week2_1.html#examples",
    "title": "Important Terms and Distinctions",
    "section": "Examples",
    "text": "Examples\n\n\n\n\n\n\n\n\nAlgorithm\nResulting Model\nWhat It Represents\n\n\n\n\nLinear Regression\nA line (y = mx + b)\nRelationship between input and output variables\n\n\nDecision Tree\nTree of splits\nHow features split to predict classes or values\n\n\nNeural Network\nLayers of neurons & weights\nComplex non-linear mappings between inputs and outputs\n\n\nk-Means Clustering\nCluster centroids\nGrouping of data points into clusters",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#things-to-remember",
    "href": "week2_1.html#things-to-remember",
    "title": "Important Terms and Distinctions",
    "section": "Things to Remember",
    "text": "Things to Remember\n\nThe algorithm is the procedure for learning\n\nThe model is the trained artifact used for prediction or inference\nDifferent algorithms can produce different models from the same data\nA model generalizes patterns from training data to unseen data\nAlgorithm = recipe (instructions for learning)\nModel = finished dish (learned representation ready to use)\n\nThe model is the end product of learning — the part that actually “knows” something about the data and can be used to make predictions. In ML: Data + Algorithm → Model → Predictions.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_5.html",
    "href": "week2_5.html",
    "title": "Feature Engineering",
    "section": "",
    "text": "Feature engineering in machine learning typically describes the process of creating, transforming, or selecting variables (features) from raw data to improve a model’s performance.\nFor example, one aspect of feature engineering is feature creation. This often means transforming raw data into meaningful inputs that better capture the underlying patterns the model needs to learn. For example, suppose we are trying to predict medicine adherence (whether or not someone takes their medicine). Our raw data may contain timestamps of when someone actually takes their medicine. We might use this timestamp to create new features for our model. For example, we might add features representing day of the week, holiday or weekend to provide additional context to the model.\nA number of domains fall under feature engineering:\nData leakage occurs when information from outside the training data set is used to create the model.\nData leakage often occurs during feature engineering.\nTo minimize data leaking, we will often want to do our feature engineering during the resampling, or data splitting, procedure we are using. To visualize this take a look at the graphic below from Boehmke and Greenwell (2019) where the pre-processing, or data engineering tasks, occur during each iteration. Keep this in mind as we introduce each feature engineering task.\nTo demonstrate feature engineering we will use some example data from the The General Social Survey (GSS).\nGSS is a long-running, nationally representative survey of adults in the United States that has been conducted almost every two years since 1972 by the National Opinion Research Center (NORC) at the University of Chicago. The GSS data is often used to measure American’s attitudes, behaviors, and beliefs on a wide range of topics—such as politics, religion, crime, race relations, family, work, and technology.\nThere are two ways to access the GSS data. One is using the GSS Data Explorer. Another option is using the gssr (Healy 2023) R package. Here we will use the gssr (Healy 2023) package to download some example data.\nWe will start by choosing happy as our target variable. This comes from Question 157 of the 2024 GSS where respondents were asked:\nResponses were coded such that 1 indicates “very happy”, 2 indicates “pretty happy”, and 3 indicates “not too happy”, while NA indicates “don’t know.”\nWe can also identify ad bunch of features we think predict self-reported happiness and save our final dataset.\nlibrary(gssr)\n\nPackage loaded. To attach the GSS data, type data(gss_all) at the console.\nFor the panel data and documentation, type e.g. data(gss_panel08_long) and data(gss_panel_doc).\nFor help on a specific GSS variable, type ?varname at the console.\n\ngss24 &lt;- gss_get_yr(2024)\n\nFetching: https://gss.norc.org/documents/stata/2024_stata.zip\n\ntarget &lt;- \"happy\"\n\nfeatures &lt;- c(\n  \"age\",     # age of participant\n  \"sex\",     # sex of participant\n  \"race\",    # race of participant\n  \"educ\",    # highest education completed by participant\n  \"income\",  # income of participant\n  \"childs\",  # number of children participant has\n  \"wrkstat\", # work force status\n  \"marital\", # marital status\n  \"born\",    # whether or not participant was born in USA\n  \"partyid\", # political party of participant\n  \"adults\",  # num of fam members 18 or older\n  \"earnrs\"   # number of earners in family  \n)\n\ndata &lt;- gss24[,c(target, features)]\n\ntable(data$happy, useNA = \"always\")\n\n\n   1    2    3 &lt;NA&gt; \n 684 1892  705   28\nNext, let’s clean up data a bit, discarding some of the labels and missing value information we don’t need. The data in gss24 retains the labeling structure provided by the GSS. Variables are stored numerically with labels attached to them. Often, when using the data in R, it will be convenient to convert the categorical variables we are interested in to character or factor type instead.\nHere we can use code from the gssr package introduction to simplify this recoding. The only thing we need to do is define the categorical variables in our data.\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ncat_vars &lt;- c(\"sex\",\"race\", \"educ\", \"wrkstat\", \"marital\", \"born\", \"partyid\")\n\ncapwords &lt;- function(x, strict = FALSE) {\n    cap &lt;- function(x) paste(toupper(substring(x, 1, 1)),\n                  {x &lt;- substring(x, 2); if(strict) tolower(x) else x},\n                             sep = \"\", collapse = \" \" )\n    sapply(strsplit(x, split = \" \"), cap, USE.NAMES = !is.null(names(x)))\n}\n\ndata &lt;- data |&gt;\n  mutate(\n    # Convert all missing to NA\n    across(everything(), haven::zap_missing), \n    # Make all categorical variables factors and relabel nicely\n    across(all_of(cat_vars), forcats::as_factor),\n    across(all_of(cat_vars), \\(x) forcats::fct_relabel(x, capwords, strict = TRUE))\n  )\nDealing with missing data in a consistent manner is one of the most important aspects of feature engineering.\nMuch attention is paid to the mechanisms producing missing data in the statistics and applied sciences literature.\nKnowing how the missing data came about is critical for knowing how to handle it in a subsequent analysis.\nAlthough attention to missing data mechanisms has historically not been a primary focus in the ML literature, this has been changing.\nA common framework for understanding missing data mechanisms was described by Rubin (1976). Here we will briefly describe these mechanisms in the context of our current data example.\nIn MCAR, the probability of a value being missing is unrelated to the value itself or any other observed or unobserved variable. This is a purely random and unsystematic process.\nImagine a cat opening up your dataset in Excel and walking across the keyboard, randomly deleting different cells.\nDefinition: Missingness in self-reported happiness, for example, is unrelated to the respondent’s true happiness level or any other variables.\nExample: Let’s say there is a glitch in the online GSS survey and for some respondent’s questions are randomly skipped. This means the probability of missingness is purely random, and it does not depend on other variables (e.g. income, age, or happiness levels).\nImplication: Dropping these cases (listwise deletion) or using simple imputation will not bias the results, although efficiency is reduced.\nIn MAR, the probability of a value being missing is systematically related to other observed variables in the dataset, but not to the unobserved value itself.\nDefinition: Missingness in happiness depends on other observed variables but not directly on happiness itself.\nExample: Suppose in the GSS, individuals with higher incomes were less likely to answer the happiness question. Missingness on happiness is explained by income, which is observed. Conditional on income (having income in our model as a predictor), the probability of missingness does not depend how happy one is.\nImplication: Methods like multiple imputation (MICE), missForest, or regression-based imputation can use income (and other observed covariates like marital status, education) to predict and impute missing happiness values without bias.\nDefinition: Missingness in happiness depends on the unobserved happiness score itself.\nExample: Respondents who are very unhappy may avoid answering the happiness question because it feels too personal, while those who are extremely happy may skip it because they consider it obvious. Missingness is directly tied to the unreported happiness level.\nImplication: Standard imputation methods will be biased. Handling MNAR requires explicitly modeling the missingness mechanism (e.g., selection models, pattern-mixture models)\nOften a first step in handling missing data involves recoding missing values as NA. Writing bespoke code to handle the different types of missing data one might encounter is tedious and unnecessary.\nThe naniar (Tierney and Cook 2023) package in R contains many convenience functions for managing missing data in R. Here we demonstrate some of that functionality.\nNow that we have a dataset with missing values we can use naniar to recode these values to NA. In our current data example this is already done, but this code might be useful for other projects where you import data\nlibrary(naniar)\n\ngss_na_codes &lt;- c(\"&lt;NA&gt;\", -99, -999, \"NA\")\n  \ndata &lt;- naniar::replace_with_na_all(\n  data, condition = ~.x %in% gss_na_codes\n)\nSee the naniar vignette on recoding NA values for more detailed information on the package functionality.\nOnce we have recoded our data in a consistent manner we can use visualizations to explore the missing data. The vis_miss() function from naniar is a good starting point for visualizing the amount of missing data in our dataset. The plots shows the missing values in black and non-missing values in gray. In addition, percentages of missing data in both the dataset and individual variables are provided.\nnaniar::vis_miss(data)\nIt is often useful to look at combinations of missingness among different variables.\nnaniar::gg_miss_upset(data)\nWe can also look at the percentage of missing data across a factor variable.\nnaniar::gg_miss_fct(x = data, fct = marital)\nMany missing data visualizations are described in the naniar vignette on missing data visualization including plots for exploring missing data mechanisms.\nBelow is a small dataset looking at predictors of happiness. Some values are missing.\nIn small groups please speculate on why each value might be missing and which type of missing data mechanism it represents: MCAR, MAR, or MNAR.\n# Columns: id, happiness (target), age (feature), income (feature), education (feature)\n# NA indicates missing values\n\nexample_data &lt;- data.frame(\n  id = 1:15,\n  happiness = c(NA, 8, 5, NA, 6, 9, 4, 7, NA, 5, 6, 8, 7, NA, 4),\n  age = c(25, NA, 30, 40, 22, 35, NA, 29, 31, 28, 34, NA, 27, 33, 26),\n  income = c(50000, 55000, 6500, 70000, NA, NA, NA, 62000, NA, 45000, 52000, NA, 58000, NA,61000),\n  education = c(\"Bachelor\",\"Bachelor\",\"Master\",\"Master\",\"HighSchool\",\"Bachelor\",\"HighSchool\",\"Master\",\"Bachelor\",\"HighSchool\",\"Master\",\"Master\",\"Bachelor\",\"HighSchool\",\"Bachelor\")\n)\n\nexample_data\n\n   id happiness age income  education\n1   1        NA  25  50000   Bachelor\n2   2         8  NA  55000   Bachelor\n3   3         5  30   6500     Master\n4   4        NA  40  70000     Master\n5   5         6  22     NA HighSchool\n6   6         9  35     NA   Bachelor\n7   7         4  NA     NA HighSchool\n8   8         7  29  62000     Master\n9   9        NA  31     NA   Bachelor\n10 10         5  28  45000 HighSchool\n11 11         6  34  52000     Master\n12 12         8  NA     NA     Master\n13 13         7  27  58000   Bachelor\n14 14        NA  33     NA HighSchool\n15 15         4  26  61000   Bachelor\nThere are a number of ways to handle missing data. Below I will discuss some of the most common ways of addressing missing data.\nListwise deletion (also called complete-case analysis) is one of the simplest methods for handling missing data in a dataset.\nWhen performing listwise deletion we remove any row that has one or more missing values across any variable used in the analysis.\nAfter deletion, only rows that are complete for all variables remain.\nFor example, our example GSS data has 3,309 rows before we address the missing data. If we only kept rows that contained no missing data we would have 2,780 observations. You can perform listwise deletion on your data using the complete.cases() function as demonstrated below. Then you can visualize the missing data to ensure there is no missingness on the new dataset.\n# nrow(data) # 3,3309 rows\n\ndata_cc &lt;- data[complete.cases(data),]\nnrow(data_cc)\n\n[1] 2746\n\nnaniar::vis_miss(data_cc)\nNow, listwise deletion should really only be used if data is missing completely at random (MCAR). In this case it can still provide unbiased results, although they can be less efficient (reduced power and more uncertainty).\nImputation is the process of filling in missing values in a dataset with estimated or predicted values so that you can perform analyses without dropping incomplete cases.\nUnlike listwise deletion, imputation retains all observations, reducing data loss. Imputation can be simple (deterministic, like a mean) or more involved (stochastic, model-based).\nEstimated Statistic\nA simple approach to imputing missing values for a feature is to compute descriptive statistics such as the mean, median, or mode (for categorical) and use that value to replace the NA values.\nAlthough computationally efficient, this approach does not consider any other attributes for a given observation when imputing.\nThe tidymodels (Kuhn and Wickham 2020) R package has a number of useful functions for machine learning. Here we use the package to perform mean imputation on our dataset.\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.4.1 ──\n\n\n✔ broom        1.0.9     ✔ rsample      1.3.1\n✔ dials        1.4.2     ✔ tailor       0.1.0\n✔ ggplot2      3.5.2     ✔ tidyr        1.3.1\n✔ infer        1.0.9     ✔ tune         2.0.0\n✔ modeldata    1.5.1     ✔ workflows    1.3.0\n✔ parsnip      1.3.3     ✔ workflowsets 1.1.1\n✔ purrr        1.1.0     ✔ yardstick    1.3.2\n✔ recipes      1.3.1     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::discard() masks scales::discard()\n✖ dplyr::filter()  masks stats::filter()\n✖ dplyr::lag()     masks stats::lag()\n✖ recipes::step()  masks stats::step()\n\n# create a recipe\nrec &lt;- recipe(happy ~ ., data = data) %&gt;%\n  step_impute_mean(all_numeric_predictors())  %&gt;% \n  step_impute_mode(all_factor_predictors())\n  # apply mean imputation to predictors\n\n# prep the recipe (estimates imputation values)\nrec_prep &lt;- prep(rec)\n\n# check the imputed values\ndata_mi &lt;- bake(rec_prep, new_data = NULL)\n\nnaniar::vis_miss(data_mi)\nK-Nearest Neighbor\nAnother popular method for performing imputation is k-nearest neighbor.\nNow, instead of just filling in a missing value with a simple number like the mean, KNN looks for other respondents who are most similar (the “nearest neighbors”) and uses their information to fill in the blank.\nHow KNN Imputation Works\nBetter than Mean Imputation?\nThink of guessing someone’s favorite pizza topping:\nlibrary(tidymodels)\n\n# create a recipe\nrec &lt;- recipe(happy ~ ., data = data) %&gt;%\n  step_impute_knn(all_predictors())   \n  # apply mean imputation to predictors\n\n# prep the recipe (estimates imputation values)\nrec_prep &lt;- prep(rec)\n\n# check the imputed values\ndata_knn &lt;- bake(rec_prep, new_data = NULL)\n\nnaniar::vis_miss(data_knn)\nTree-Based Imputation\nTree-based imputation methods use decision trees (or random forests) to predict missing values based on the other variables in the dataset. Similar to KNN methods, tree-based methods make a tailored prediction using patterns in the data.\nTree-based methods are especially nice for imputation as they handle non-linearities and interactions, and can accomodate mixed data types, like continuous and categorical variables.\nHow Tree-Based Imputation Works\nBetter than Mean Imputation?\nThink of tree-based imputation like asking:\nThis is smarter than saying “everyone makes the same average income” (mean imputation) or even “let’s just look at your 3 closest neighbors” (KNN).\nFeature filtering is a feature selection technique in machine learning where features are evaluated prior to the model fitting based on statistical or heuristic criteria. Features are then kept or discarded based on those criteria.\nIt is called filtering because it is a preprocessing step, part of a feature engineering pipeline, done before we actually train the model.\nTypically, the goals of feature filtering are:\nIn practice, one of the filtering tasks we will typically conduct involves weeding out low variance features.\nZero and near-zero variance variables are low-hanging fruit to eliminate.\nZero and near-zero variance variables typically offer little to no information for model building. Furthermore, resampling (data-splitting) further complicates this picture because a given fold or sample may only contain a single value if the variable itself only contains a few unique values.\nBoehmke and Greenwell (2019) suggest the following rule-of-thumb for removing low-variance features:\nRemove a variable if:\nWe can use the caret (Kuhn and Max 2008) package in R to look at these different metrics for our example data.\ncaret::nearZeroVar(data, saveMetrics = TRUE) %&gt;% \n  tibble::rownames_to_column() \n\n   rowname freqRatio percentUnique zeroVar   nzv\n1    happy  2.683688    0.09066183   FALSE FALSE\n2      age  1.014085    2.17588395   FALSE FALSE\n3      sex  1.242672    0.06044122   FALSE FALSE\n4     race  3.986014    0.09066183   FALSE FALSE\n5     educ  1.191964    0.63463282   FALSE FALSE\n6   income 12.948571    0.36264733   FALSE FALSE\n7   childs  1.209166    0.27198549   FALSE FALSE\n8  wrkstat  1.834171    0.24176488   FALSE FALSE\n9  marital  1.287063    0.15110305   FALSE FALSE\n10    born  6.618056    0.06044122   FALSE FALSE\n11 partyid  1.558271    0.24176488   FALSE FALSE\n12  adults  1.075205    0.18132366   FALSE FALSE\n13  earnrs  1.560570    0.12088244   FALSE FALSE\nFeature engineering on continuous or numeric features involves transforming the raw data to make it more useful for modeling. Common motivations include capturing nonlinear relationships (e.g., log or polynomial transforms), improving distributional properties (reducing skew or outlier impact), and putting features on comparable scales through standardization.\nIn some cases, continuous variables are binned into categories or combined into interactions to highlight patterns. Overall, these transformations help models detect structure in the data more effectively and improve both performance and interpretability.\nParametric models with distributional assumptions (e.g., GLMs, and some regularized models) can benefit from minimizing the skewness of numeric features. One popular transformation is the Yeo-Johnson transformation.\nThe Yeo-Johnsontransformation is a statistical technique used to stabilize variance and make data more normally distributed, similar to the Box-Cox transformation but more flexible. Unlike Box-Cox, it can handle both positive and negative values, making it useful for real-world data that span zero.\nrecipe(happy ~ ., data = data) %&gt;%\n  step_YeoJohnson(all_numeric()) \n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 12\n\n\n\n\n\n── Operations \n\n\n• Yeo-Johnson transformation on: all_numeric()\nWe will often want to standardize variables prior to our model fitting to put them on a common scale, typically with mean of zero and a standard deviation of one.\nThis prevents features with larger numeric ranges (e.g., income in dollars vs. age in years) from dominating distance-based methods (like k-NN, SVMs, or clustering). It can also help with optimization problems, may improve convergence and can help ensure that regularization penalties (like in ridge or lasso regression) are applied fairly across predictors.\ndata %&gt;%\n  step_center(all_numeric(), -all_outcomes()) %&gt;%\n  step_scale(all_numeric(), -all_outcomes())\nMany models require that the predictors take numeric form. There are exceptionssuch as tree-based models, however, even tree-based methods can benefit from preprocessing categorical features. The following sections will discuss a few of the more common approaches to engineer categorical features.\nSometimes features will contain levels that have very few observations. For example, take a look at the work status variable wrkstat. There are 8 unique levels and some have relatively few observations. For example, With A Job, But Not At Work Because Of Temporary Illness, Vacation, Strike.\ncount(data, wrkstat) %&gt;% arrange(n)\n\n# A tibble: 9 × 2\n  wrkstat                                                                      n\n  &lt;fct&gt;                                                                    &lt;int&gt;\n1 &lt;NA&gt;                                                                        10\n2 With A Job, But Not At Work Because Of Temporary Illness, Vacation, Str…    61\n3 In School                                                                   90\n4 Other                                                                      113\n5 Unemployed, Laid Off, Looking For Work                                     168\n6 Keeping House                                                              292\n7 Working Part Time                                                          319\n8 Retired                                                                    796\n9 Working Full Time                                                         1460\nSometimes we can benefit from collapsing, or “lumping” these into a lesser number of categories. In the above examples, we may want to collapse all levels that are observed in less than 5% of the training sample into an Other category. We can use step_other() to do so.\nlumping &lt;- recipe(happy ~ ., data = data) %&gt;%\n  step_other(wrkstat, threshold = 0.05, \n             other = \"Other\")\n\n# Apply this blue print --&gt; you will learn about this at \n# the end of the chapter\napply_2_training &lt;- prep(lumping, training = data) %&gt;%\n  bake(data)\n\n# New distribution of Neighborhood\ncount(apply_2_training, wrkstat) %&gt;% arrange(n)\n\n# A tibble: 7 × 2\n  wrkstat                                    n\n  &lt;fct&gt;                                  &lt;int&gt;\n1 &lt;NA&gt;                                      10\n2 Unemployed, Laid Off, Looking For Work   168\n3 Other                                    264\n4 Keeping House                            292\n5 Working Part Time                        319\n6 Retired                                  796\n7 Working Full Time                       1460\nAs mentioned previously many models require that all features be numeric. Consequently, we need to intelligently transform any categorical variables into numeric representations so that these algorithms will work.\nThere are many ways to recode categorical variables as numeric (e.g., one-hot, ordinal, binary, sum, and Helmert).\nOne-hot encoding is a common method for converting categorical variables into a numerical format that machine learning algorithms can work with. Instead of assigning arbitrary numbers to categories (which could incorrectly imply an order), one-hot encoding creates a new binary (0/1) column for each category level. For a given observation, the column corresponding to its category is set to 1, and all others are set to 0.\nHowever, this creates perfect collinearity which causes problems with some predictive modeling algorithms (e.g., ordinary linear regression and neural networks). Alternatively, we can create a full-rank encoding by dropping one of the levels (level blue has been dropped). This is referred to as dummy coding.\nBelow is an example of one-hot encoding the predictors in our model.\nrecipe(happy ~ ., data = data) %&gt;%\n  step_dummy(all_factor_predictors(), one_hot = TRUE)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 12\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_factor_predictors()\nWhen setting up your feature engineering workflow there is often a sensible order in which to perform these steps.\nOne possible sequence that avoids headaches is suggested by Boehmke and Greenwell (2019):\nTo illustrate how this process works together in R code, let’s do a simple analysis using our example data, starting from scratch.\nThe steps below simply re-downloads our data, selects the variables we want to keep, cleans up the missing data codes and does some basic relabeling.\nlibrary(gssr)\n\ngss24 &lt;- gss_get_yr(2024)\n\nFetching: https://gss.norc.org/documents/stata/2024_stata.zip\n\ntarget &lt;- \"happy\"\n\nfeatures &lt;- c(\n  \"age\",     # age of participant\n  \"sex\",     # sex of participant\n  \"race\",    # race of participant\n  \"educ\",    # highest education completed by participant\n  \"income\",  # income of participant\n  \"childs\",  # number of children participant has\n  \"wrkstat\", # work force status\n  \"marital\", # marital status\n  \"born\",    # whether or not participant was born in USA\n  \"partyid\", # political party of participant\n  \"adults\",  # num of fam members 18 or older\n  \"earnrs\"   # number of earners in family  \n)\n\ndata &lt;- gss24[,c(target, features)]\n\n\n# define which varibles are categorical and continuous\ncat_vars &lt;- c(\"sex\",\"race\", \"educ\", \"wrkstat\", \"marital\", \"born\", \"partyid\")\ncon_vars &lt;- c(\"age\",\"income\",\"childs\",\"adults\",\"earnrs\",\"happy\")\n\ncapwords &lt;- function(x, strict = FALSE) {\n    cap &lt;- function(x) paste(toupper(substring(x, 1, 1)),\n                  {x &lt;- substring(x, 2); if(strict) tolower(x) else x},\n                             sep = \"\", collapse = \" \" )\n    sapply(strsplit(x, split = \" \"), cap, USE.NAMES = !is.null(names(x)))\n}\n\ndata &lt;- data |&gt;\n  mutate(\n    # Convert all missing to NA\n    across(everything(), haven::zap_missing), \n    # Make all categorical variables factors and relabel nicely\n    across(all_of(cat_vars), forcats::as_factor),\n    across(all_of(con_vars), as.numeric),\n    across(all_of(cat_vars), \\(x) forcats::fct_relabel(x, capwords, strict = TRUE))\n  )\n\ndata &lt;- data[!is.na(data$happy),]\nWe can now separate our data into a training and test set.\n# install.packages(\"rsample\")\nlibrary(rsample)\n# Stratified sampling with the rsample package\nset.seed(123)\nsplit &lt;- initial_split(data, prop = 0.7, strata = \"happy\")\ndata_train  &lt;- training(split)\ndata_test   &lt;- testing(split)\nNow, we will formally introduce the recipes package.\nThe recipes package is part of the tidymodels framework and is designed for feature engineering. In machine learning, raw data usually isn’t ready to be used directly in a model—you might need to do all the things we discussed in these notes.\nInstead of doing all these steps manually, recipes lets us define a sequence of preprocessing steps (called a recipe) that can be applied consistently to training and test data.\nA recipe typically goes through three main stages:",
    "crumbs": [
      "Introduction to Machine Learning",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "week2_5.html#example-data",
    "href": "week2_5.html#example-data",
    "title": "Feature Engineering",
    "section": "Example Data",
    "text": "Example Data",
    "crumbs": [
      "Introduction to Machine Learning",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "week2_5.html#addressing-missing-data",
    "href": "week2_5.html#addressing-missing-data",
    "title": "Feature Engineering",
    "section": "Addressing Missing Data",
    "text": "Addressing Missing Data",
    "crumbs": [
      "Introduction to Machine Learning",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "week2_5.html#feature-filtering",
    "href": "week2_5.html#feature-filtering",
    "title": "Feature Engineering",
    "section": "Feature Filtering",
    "text": "Feature Filtering",
    "crumbs": [
      "Introduction to Machine Learning",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "week2_5.html#numeric-feature-engineering",
    "href": "week2_5.html#numeric-feature-engineering",
    "title": "Feature Engineering",
    "section": "Numeric Feature Engineering",
    "text": "Numeric Feature Engineering",
    "crumbs": [
      "Introduction to Machine Learning",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "week2_5.html#categorical-feature-engineering",
    "href": "week2_5.html#categorical-feature-engineering",
    "title": "Feature Engineering",
    "section": "Categorical Feature Engineering",
    "text": "Categorical Feature Engineering",
    "crumbs": [
      "Introduction to Machine Learning",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "week2_5.html#workflow",
    "href": "week2_5.html#workflow",
    "title": "Feature Engineering",
    "section": "Workflow",
    "text": "Workflow",
    "crumbs": [
      "Introduction to Machine Learning",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "PSYC559: APPLIED MACHINE LEARNING IN PSYCHOLOGY",
    "section": "",
    "text": "Welcome to Applied Machine Learning in Psychology.",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "introduction.html#the-syllabus",
    "href": "introduction.html#the-syllabus",
    "title": "PSYC559: APPLIED MACHINE LEARNING IN PSYCHOLOGY",
    "section": "The Syllabus",
    "text": "The Syllabus\nStudents may review the syllabus on Canvas.",
    "crumbs": [
      "Table of Contents"
    ]
  }
]