[
  {
    "objectID": "week5_2.html",
    "href": "week5_2.html",
    "title": "Bagging",
    "section": "",
    "text": "As we discussed previously, a limitation of standard decision trees is their high variance. This means that small changes in the training data can lead to significantly different tree structures, which can result in overfitting and poor generalization to new data.\nBagging, or Bootstrap Aggregating, is an ensemble learning technique designed to reduce the variance of high-variance models like decision trees.\nThe core idea behind bagging is to create multiple versions of a predictor (in this case, decision trees) and use these to get an aggregated predictor.\nBootstrapping is a statistical method that involves resampling with replacement from a dataset to create multiple “bootstrap” samples.\nEach bootstrap sample is of the same size as the original dataset but may contain duplicate observations due to the sampling with replacement.",
    "crumbs": [
      "Tree Methods",
      "Bagging"
    ]
  },
  {
    "objectID": "week5_2.html#bagging-in-r",
    "href": "week5_2.html#bagging-in-r",
    "title": "Bagging",
    "section": "Bagging in R",
    "text": "Bagging in R",
    "crumbs": [
      "Tree Methods",
      "Bagging"
    ]
  },
  {
    "objectID": "week5_2.html#code-for-in-class-activity",
    "href": "week5_2.html#code-for-in-class-activity",
    "title": "Bagging",
    "section": "Code for In-Class Activity",
    "text": "Code for In-Class Activity",
    "crumbs": [
      "Tree Methods",
      "Bagging"
    ]
  },
  {
    "objectID": "week4_1.html",
    "href": "week4_1.html",
    "title": "Regularized Regression",
    "section": "",
    "text": "In this chapter we will introduce regularized regression methods, which are powerful tools for feature selection and prediction in high-dimensional settings.\nWe will cover the following topics:\nIn social science contexts, big data often means one or more of the following:\nCrucially for modeling, the problem is not just size — it’s that the number of candidate predictors (p) can be large relative to, or even exceed, the number of observations (n).\nFeature selection is the process of choosing a subset of predictor variables for use in a model. Goals typically include:\nTwo broad strategies exist:\nRegularized regression methods integrate feature selection into the model fitting process by adding a penalty term to the loss function. This encourages simpler models that generalize better.\nRegularization is a modern solution that keeps the model simpler and more stable by shrinking regression coefficients toward zero.\nThink of it like adding a gentle rule to your model:\nThis way, unimportant predictors get down-weighted or even dropped, leaving a model that generalizes better.\nImagine a researcher studying well-being. They collect data on 1,000 participants and measure 100 different predictors:\nThe goal is to predict a well-being score using these 100 predictors.\nEstimating The Model with Classic Multiple Regression\nIf you estimate a regression with all 100 predictors using OLS:\nThe model fits well in terms of variance explained, but:\nEstimating The Model with Regularized Regression\nIf you estimate the same model using Lasso or Ridge (regularized) regression:\nExample of Output from Both Approaches\nCode\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# --- Setup ---\nset.seed(123)\nn &lt;- 200   # participants\np &lt;- 100   # predictors\n\n# Generate realistic variable names\ndemographics &lt;- c(\"age\", \"gender\", \"income\", \"education\", \"marital_status\")\npsych_scales &lt;- paste0(\"stress_item\", 1:20)\nbehavior &lt;- paste0(\"sleep_hours\", 1:10)\nsocial &lt;- paste0(\"social_item\", 1:15)\n# Fill out the rest with generic names to reach 100\nother &lt;- paste0(\"var\", 1:(p - length(demographics) - length(psych_scales) - length(behavior) - length(social)))\n\npredictor_names &lt;- c(demographics, psych_scales, behavior, social, other)\n\n\n# Simulate predictors\nX &lt;- matrix(rnorm(n * p), n, p)\ncolnames(X) &lt;- predictor_names\n\n# True coefficients: let's say age, income, stress_item1, stress_item2, sleep_hours1 are true predictors\ntrue_beta &lt;- rep(0, p)\ntrue_indices &lt;- match(c(\"age\", \"income\", \"stress_item1\", \"stress_item2\", \"sleep_hours1\"), predictor_names)\ntrue_beta[true_indices] &lt;- c(0.8, 1.2, 1.0, 0.9, 0.7)\n\n# Simulate outcome: well-being\ny &lt;- X %*% true_beta + rnorm(n, sd=1.5)\n\n# --- OLS ---\nols_fit &lt;- lm(y ~ X)\nols_coefs &lt;- coef(ols_fit)[-1]  # drop intercept\n\n# --- Lasso ---\nlibrary(glmnet)\ncv_fit &lt;- cv.glmnet(X, y, alpha=1)  # lasso\nbest_lambda &lt;- cv_fit$lambda.min\nlasso_fit &lt;- glmnet(X, y, alpha=1, lambda=best_lambda)\nlasso_coefs &lt;- as.vector(coef(lasso_fit)[-1])  # drop intercept\n\n# --- Table of first 30 predictors ---\ntable_df &lt;- data.frame(\n  Predictor = predictor_names[1:30],\n  OLS = round(ols_coefs[1:30], 2),\n  Lasso = round(lasso_coefs[1:30], 2)\n)\n\ntable_df |&gt;\n    mutate(\n      Lasso = cell_spec(\n        Lasso, \n        \"html\",\n        background = ifelse(Lasso == 0, \"red\", \"black\"))\n      ) |&gt;\n      kable(\n        \"html\", \n        escape = FALSE,\n        row.names = FALSE, \n        caption = \"Comparison of OLS and Lasso Coefficients for First 30 Predictors\") |&gt;\n      kable_styling(full_width = FALSE)\n\n\n\nComparison of OLS and Lasso Coefficients for First 30 Predictors\n\n\nPredictor\nOLS\nLasso\n\n\n\n\nage\n0.69\n0.52\n\n\ngender\n0.02\n0\n\n\nincome\n1.02\n0.99\n\n\neducation\n-0.18\n-0.03\n\n\nmarital_status\n-0.01\n0\n\n\nstress_item1\n1.04\n0.86\n\n\nstress_item2\n0.77\n0.65\n\n\nstress_item3\n-0.04\n0\n\n\nstress_item4\n-0.15\n0\n\n\nstress_item5\n-0.13\n-0.08\n\n\nstress_item6\n-0.09\n0\n\n\nstress_item7\n-0.01\n0\n\n\nstress_item8\n-0.14\n0\n\n\nstress_item9\n0.03\n0\n\n\nstress_item10\n-0.06\n0\n\n\nstress_item11\n0.12\n0\n\n\nstress_item12\n-0.21\n-0.01\n\n\nstress_item13\n-0.10\n0\n\n\nstress_item14\n0.04\n0\n\n\nstress_item15\n-0.08\n0\n\n\nstress_item16\n-0.01\n0\n\n\nstress_item17\n-0.13\n-0.05\n\n\nstress_item18\n-0.17\n0\n\n\nstress_item19\n0.14\n0\n\n\nstress_item20\n0.25\n0.06\n\n\nsleep_hours1\n0.61\n0.53\n\n\nsleep_hours2\n0.00\n0\n\n\nsleep_hours3\n0.14\n0\n\n\nsleep_hours4\n0.23\n0.05\n\n\nsleep_hours5\n0.00\n0\nObservations\nHopefully, this table illustrates how regularization performs variable selection, giving a cleaner, interpretable model for predicting well-being.\nSo how does this regularization (or shrinkage) actually work?\nThe Lasso (Least Absolute Shrinkage and Selection Operator) adds an L1 penalty to the loss function, which encourages sparsity in the coefficients.\nFor example, remember our ordinary least squares (OLS) regression problem, we estimate coefficients \\(\\beta = (\\beta_1, \\beta_2, \\dots, \\beta_p)\\) by minimizing the residual sum of squares:\n\\[\\begin{equation}\n\\widehat{\\beta}^{OLS} = \\arg\\min_{\\beta} \\sum_{i=1}^n \\left( y_i - \\hat{y_i} \\right)^2.\n\\end{equation}\\]\nOLS works well when the number of predictors \\(p\\) is small and predictors are not highly correlated. However, when \\(p\\) is large or predictors are correlated, OLS estimates can become unstable, coefficients can have high variance, and the model can overfit the data.\nLasso regression addresses these problems by adding a penalty on the sum of the absolute values of the coefficients:\n\\[\\begin{equation}\n\\widehat{\\beta}^{Lasso} = \\arg\\min_{\\beta}  \n\\sum_{i=1}^n \\left( y_i - \\hat{y_i} \\right)^2\n+ \\lambda \\sum_{j=1}^p |\\beta_j|\n,\n\\end{equation}\\]\nwhere \\(\\lambda \\ge 0\\) is a hyperparameter that controls the strength of the penalty.\nNotice the penalty term \\(\\lambda \\sum_{j=1}^p |\\beta_j|\\) is taking the sum of the absolute values of the coefficients. This L1 penalty has the effect of shrinking some coefficients exactly to zero when \\(\\lambda\\) is sufficiently large, effectively performing variable selection.\nIntuitively, Lasso performs two things:\nThe key hyperparameter in Lasso regression is \\(\\lambda\\), which controls the amount of regularization.\nChoosing the right \\(\\lambda\\) is crucial for balancing bias and variance.\nFor example, suppose we are trying to predict well-being using several psychological and demographic predictors. We can fit a Lasso regression model across a range of \\(\\lambda\\) values and visualize how the coefficients change.\nCode\n# --- Load libraries ---\nif(!require(glmnet)) install.packages(\"glmnet\")\nlibrary(glmnet)\n\n# --- Simulate a small psychology dataset ---\nset.seed(123)\nn &lt;- 200\n\n# Predictors\nstress &lt;- rnorm(n, mean=5, sd=2)\nsleep &lt;- rnorm(n, mean=7, sd=1.5)\nsocial_support &lt;- rnorm(n, mean=50, sd=10)\nexercise &lt;- rnorm(n, mean=3, sd=1)\nincome &lt;- rnorm(n, mean=50000, sd=15000)\n\nX &lt;- cbind(stress, sleep, social_support, exercise, income)\n\n# True coefficients for simulation\nbeta_true &lt;- c(-0.8, 0.5, 0.7, 0, 0)  # only stress, sleep, social_support matter\ny &lt;- X %*% beta_true + rnorm(n, sd=1.5)\n\ncolnames(X) &lt;- c(\"Stress\", \"Sleep\", \"SocialSupport\", \"Exercise\", \"Income\")\n\n# --- Fit Lasso regression ---\nlasso_fit &lt;- glmnet(X, y, alpha=1)\n\n# --- Plot coefficient paths ---\nplotmo::plot_glmnet(lasso_fit, xvar=\"lambda\", main=\"Lasso Coefficient Paths\", label =3)\nabline(h=0, col=\"gray\", lty=2)\n\n# --- Optional: cross-validated lambda ---\ncv_fit &lt;- cv.glmnet(X, y, alpha=1)\nbest_lambda &lt;- cv_fit$lambda.min\nabline(v=log(best_lambda), col=\"red\", lty=2)\nWe typically use cross-validation to select the optimal \\(\\lambda\\). The process involves: 1. Splitting the data into training and validation sets. 2. Fitting Lasso models with different \\(\\lambda\\) values on the training set. 3. Evaluating prediction error on the validation set. 4. Selecting the \\(\\lambda\\) that minimizes validation error.\nRidge regression is another form of regularization that, like the Lasso, aims to prevent overfitting in high-dimensional regression problems and improve predictive performance.\nRidge regression modifies this by adding an , which penalizes the sum of squared coefficients:\n\\[\\begin{equation}\n\\widehat{\\beta}^{Ridge} = \\arg\\min_{\\beta}\n\\sum_{i=1}^n \\left( y_i - \\hat{y_i}\\right)^2\n+ \\lambda \\sum_{j=1}^p \\beta_j^2,\n\\end{equation}\\]\nwhere \\(\\lambda \\ge 0\\) controls the strength of the penalty. Notice, instead of the absolute values of the coefficients (as in Lasso), Ridge uses the squares of the coefficients. This means Ridge regression does not set coefficients exactly to zero. Instead, it shrinks all coefficients toward zero, but none are eliminated entirely.\nThis means Ridge regression may be better suited for situations where many predictors have small but nonzero effects (a “dense” model).\nAnother penalty, related to the Lasso and Ridge, is the Elastic Net, which combines both L1 and L2 penalties. The Elastic Net is particularly useful when there are multiple correlated predictors.\n\\[\\begin{equation}\n\\widehat{\\beta}^{EN} = \\arg\\min_{\\beta}\n\\sum_{i=1}^n \\left( y_i - \\hat{y_i} \\right)^2\n+ \\lambda \\left[ \\alpha \\sum_{j=1}^p |\\beta_j| + \\frac{1-\\alpha}{2} \\sum_{j=1}^p \\beta_j^2 \\right],\n\\end{equation}\\]\nwhere we now have two hyperparameters: \\(\\alpha\\) and \\(\\lambda\\):\nThinking about the \\(\\alpha\\) hyperparameter it becomes clear that:\nRemember the Ames Housing dataset used in our previous chapters? The Ames Housing dataset is a widely used dataset in regression and machine learning tutorials. It contains detailed information about residential homes in Ames, Iowa, and is often used to predict sale prices of homes based on various characteristics. For the regression chapters we will use the Ames Housing dataset to illustrate regularized regression.\nlibrary(tidymodels)\nset.seed(123)\n\n# Load the Ames Housing dataset\ndata(\"ames\", package = \"modeldata\")\n\n# Split the data into training and testing sets\nsplit &lt;- initial_split(\n  ames, \n  prop = 0.7, \n  strata = \"Sale_Price\"\n)\names_train  &lt;- training(split)\names_test   &lt;- testing(split)\n\n\n# Create training  feature matrices\n# we use model.matrix(...)[, -1] to discard the intercept\nX &lt;- model.matrix(Sale_Price ~ ., ames_train)[, -1]\n\n# transform y with log transformation\nY &lt;- log(ames_train$Sale_Price)\nIt is important to standardize your features before applying regularized regression techniques like Lasso or Ridge. This is because these methods are sensitive to the scale of the predictors. Standardization ensures that all features contribute equally to the penalty term. If we didn’t standardize, then predictors with naturally larger values (e.g., total square footage) would be penalized more than predictors with naturally smaller values (e.g., total number of rooms).\nThe glmnet package, which we will use for fitting Lasso and Ridge models, automatically standardizes the predictors by default. However, if you are using other packages or methods, you may need to standardize your data manually.\nLet’s fit both Lasso and Ridge regression models to the Ames Housing data and compare their results. We will use cross-validation to select the optimal \\(\\lambda\\) for each model.\nNote that setting alpha = 1 specifies Lasso regression, while alpha = 0 specifies Ridge regression.\nThe figure below shows the 10-fold cross-validated mean squared error (MSE) across all \\(\\lambda\\) values. The numbers displayed along the top of the plot indicate the number of features included in the model.\nIn both Ridge and Lasso models, we observe a modest improvement in MSE as the penalty \\(\\text{log}(\\lambda)\\) increases, indicating that an unregularized OLS model may overfit the training data.\nHowever, when the penalty (\\(\\lambda\\)) becomes too large, the MSE begins to get larger.\nlibrary(glmnet)\n\n# Apply CV lasso regression to Ames data\nlasso &lt;- cv.glmnet(\n  x = X,\n  y = Y,\n  alpha = 1 # Lasso penalty\n)\n\n# Apply CV ridge regression to Ames data\nridge &lt;- cv.glmnet(\n  x = X,\n  y = Y,\n  alpha = 0 # Ridge penalty\n)\n\n# plot results\npar(mfrow = c(1, 2))\nplot(lasso, main = \"Lasso penalty\\n\\n\")\nplot(ridge, main = \"Ridge penalty\\n\\n\")\nWe can also look at a plot of the coefficients as a function of \\(\\text{log}(\\lambda)\\) for both Lasso and Ridge regression.\n# Lasso model\nlasso_min &lt;- glmnet(\n  x = X,\n  y = Y,\n  alpha = 1\n)\n\n# Ridge model\nridge_min &lt;- glmnet(\n  x = X,\n  y = Y,\n  alpha = 0\n)\n\npar(mfrow = c(1, 2))\n\n# plot lasso model\nplot(lasso_min, xvar = \"lambda\", main = \"Lasso penalty\\n\\n\")\nabline(v = -1*log(lasso$lambda.min), col = \"red\", lty = \"dashed\")\n\n\n# plot ridge model\nplot(ridge_min, xvar = \"lambda\", main = \"Ridge penalty\\n\\n\")\nabline(v = log(ridge$lambda.min), col = \"red\", lty = \"dashed\")\nSo far we’ve implemented a pure Ridge and pure Lasso model.\nHowever, we can implement an elastic net the same way as the ridge and lasso models, by adjusting the alpha parameter. Any alpha value between 0–1 will perform an elastic net.\nWhen alpha = 0.5 we perform an equal combination of penalties whereas alpha &lt;0.5 will have a heavier ridge penalty applied and alpha &gt;0.5 will have a heavier lasso penalty.\nlibrary(caret)\n\n# for reproducibility\nset.seed(123)\n\n# grid search across \ncv_glmnet &lt;- train(\n  x = X,\n  y = Y,\n  method = \"glmnet\",\n  preProc = c(\"zv\", \"center\", \"scale\"),\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneLength = 10\n)\n\n# model with lowest RMSE\ncv_glmnet$bestTune\n\n  alpha     lambda\n7   0.1 0.02006835\n\n##   alpha     lambda\n## 7   0.1 0.02007035\n\n# results for model with lowest RMSE\ncv_glmnet$results %&gt;%\n  filter(alpha == cv_glmnet$bestTune$alpha, lambda == cv_glmnet$bestTune$lambda)\n\n  alpha     lambda      RMSE Rsquared       MAE     RMSESD RsquaredSD\n1   0.1 0.02006835 0.1525861 0.864404 0.0915093 0.03473999 0.05631073\n        MAESD\n1 0.007590839\n\n# plot cross-validated RMSE\nggplot(cv_glmnet)\nNow, let’s consider three different models we may want to compare:\nWe can also compare the best fitting multiple regression model from the previous chapter.\n# model 1 Lasso\nlasso_grid &lt;- expand.grid(\n  alpha = 1,\n  lambda = 10^seq(-3, 1, length.out = 100)\n)\n\nlibrary(caret)\n# Train model using 10-fold cross-validation\nset.seed(123)  # for reproducibility\ncv_lasso &lt;- train(\n  x = X,\n  y = Y,\n  method = \"glmnet\",\n  preProc = c(\"zv\", \"center\", \"scale\"),\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneGrid = lasso_grid\n)\n# model 1 Ridge\nridge_grid &lt;- expand.grid(\n  alpha = 0,\n  lambda = 10^seq(-3, 1, length.out = 100)\n)\n\nlibrary(caret)\n# Train model using 10-fold cross-validation\nset.seed(123)  # for reproducibility\ncv_ridge &lt;- train(\n  x = X,\n  y = Y,\n  method = \"glmnet\",\n  preProc = c(\"zv\", \"center\", \"scale\"),\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneGrid = ridge_grid\n)\n# model 3 Elastic Net\nen_grid &lt;- expand.grid(\n  alpha = seq(0, 1, length.out = 100),\n  lambda = 10^seq(-3, 1, length.out = 100)\n)\n\nlibrary(caret)\n# Train model using 10-fold cross-validation\nset.seed(123)  # for reproducibility\ncv_en &lt;- train(\n  x = X,\n  y = Y,\n  method = \"glmnet\",\n  preProc = c(\"zv\", \"center\", \"scale\"),\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneGrid = en_grid\n)\n# model 4 Multiple Regression\n\nlibrary(caret)\n# Train model using 10-fold cross-validation\nset.seed(123)  # for reproducibility\ncv_lm &lt;- train(\n  x = X[,c(\"Gr_Liv_Area\", \"Year_Built\")],\n  y = Y,\n  method = \"lm\",\n  preProc = c(\"zv\", \"center\", \"scale\"),\n  trControl = trainControl(method = \"cv\", number = 10)\n)\n# Extract out of sample performance measures\nsummary(resamples(list(\n  model1 = cv_lasso, \n  model2 = cv_ridge, \n  model3 = cv_en,\n  model4 = cv_lm\n)))\n\n\nCall:\nsummary.resamples(object = resamples(list(model1 = cv_lasso, model2 =\n cv_ridge, model3 = cv_en, model4 = cv_lm)))\n\nModels: model1, model2, model3, model4 \nNumber of resamples: 10 \n\nMAE \n             Min.    1st Qu.     Median       Mean    3rd Qu.      Max. NA's\nmodel1 0.08038917 0.08433727 0.09312387 0.09202270 0.09694918 0.1063881    0\nmodel2 0.08501825 0.08891447 0.09353836 0.09401517 0.09806096 0.1040211    0\nmodel3 0.08242807 0.08591793 0.09211082 0.09211488 0.09701318 0.1035652    0\nmodel4 0.15509642 0.15711335 0.16181235 0.16363699 0.16582158 0.1789533    0\n\nRMSE \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.1096969 0.1308161 0.1485084 0.1546435 0.1759209 0.2309049    0\nmodel2 0.1163637 0.1311007 0.1451538 0.1519503 0.1678878 0.2107337    0\nmodel3 0.1125961 0.1288568 0.1453616 0.1511919 0.1689449 0.2138720    0\nmodel4 0.2072819 0.2137013 0.2296104 0.2315454 0.2461504 0.2637897    0\n\nRsquared \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.7431201 0.8338188 0.8704200 0.8603702 0.9053137 0.9293168    0\nmodel2 0.7760509 0.8361733 0.8830171 0.8657564 0.9032942 0.9196449    0\nmodel3 0.7723279 0.8399192 0.8817999 0.8669996 0.9069052 0.9249382    0\nmodel4 0.6469015 0.6573610 0.6823364 0.6915678 0.7257558 0.7500846    0\nRemember in our original model we log-transformed the outcome variable Sale_Price. Therefore, when calculating RMSE, we need to exponentiate the predictions and the actual values to bring them back to the original scale. This way we can compare the models in terms of the actual sale prices.\npred_lasso &lt;- predict(cv_lasso, X)\npred_ridge &lt;- predict(cv_ridge, X)\npred_en    &lt;- predict(cv_en, X)\npred_lm    &lt;- predict(cv_lm, X)\n\ncat(\"RMSE for Lasso:\", RMSE(exp(pred_lasso), exp(Y)))\n\nRMSE for Lasso: 25577.17\n\ncat(\"RMSE for Ridge:\", RMSE(exp(pred_ridge), exp(Y)))\n\nRMSE for Ridge: 25412.25\n\ncat(\"RMSE for Elastic Net:\", RMSE(exp(pred_en), exp(Y)))\n\nRMSE for Elastic Net: 25400.99\n\ncat(\"RMSE for Multiple Regression:\", RMSE(exp(pred_lm), exp(Y)))\n\nRMSE for Multiple Regression: 56259.74\nThe resulting cross-validated RMSE for Elastic Net appears the smallest. How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about $25,400 off from the actual sale price. This is an improvement over the other models, which have higher RMSE values, indicating that the Elastic Net model provides more accurate predictions, on average.\nlibrary(vip)\nvip(cv_en, num_features = 20, geom = \"point\")",
    "crumbs": [
      "Regression Methods",
      "Regularized Regression"
    ]
  },
  {
    "objectID": "week4_1.html#background-on-feature-selection",
    "href": "week4_1.html#background-on-feature-selection",
    "title": "Regularized Regression",
    "section": "Background on Feature Selection",
    "text": "Background on Feature Selection",
    "crumbs": [
      "Regression Methods",
      "Regularized Regression"
    ]
  },
  {
    "objectID": "week4_1.html#regularized-regression",
    "href": "week4_1.html#regularized-regression",
    "title": "Regularized Regression",
    "section": "Regularized Regression",
    "text": "Regularized Regression",
    "crumbs": [
      "Regression Methods",
      "Regularized Regression"
    ]
  },
  {
    "objectID": "week4_1.html#comparing-regularization-methods",
    "href": "week4_1.html#comparing-regularization-methods",
    "title": "Regularized Regression",
    "section": "Comparing Regularization Methods",
    "text": "Comparing Regularization Methods",
    "crumbs": [
      "Regression Methods",
      "Regularized Regression"
    ]
  },
  {
    "objectID": "week3_3.html",
    "href": "week3_3.html",
    "title": "Simple Logistic Regression",
    "section": "",
    "text": "Simple logistic regression is used when the target (outcome) variable is binary (e.g., success/failure, yes/no, 0/1) and we have a single feature (predictor). Instead of modeling the outcome directly, logistic regression models the probability of the event occurring.\nSuppose we want to model a binary outcome \\(Y \\in {0,1}\\) (e.g., pass/fail, success/failure). At first, we might think of using linear regression:\n\\[\n\\hat{Y} = \\beta_0 + \\beta_1 X\n\\]\nand interpret \\(\\hat{Y}\\) as the probability that \\(Y=1\\).\nPredicted probabilities outside [0,1]:\nLinear regression does not constrain predictions. For some values of \\(X\\), \\(\\hat{Y}\\) might be negative or greater than 1, which is impossible for probabilities.\nLet’s fit a regression to binary data to make this concrete.\n# Load libraries\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nset.seed(123)\n\n# Simulate some data\nn &lt;- 100\nx &lt;- runif(n, -3, 3)             # predictor\np &lt;- 1 / (1 + exp(-(-0.5 + 1*x))) # true logistic relationship\ny &lt;- rbinom(n, 1, p)             # binary outcome\n\ndata &lt;- data.frame(x, y)\n\n# Fit linear regression (treat y as numeric 0/1)\nlm_fit &lt;- lm(y ~ x, data = data)\n\n# Create grid of x values for prediction\nx_grid &lt;- data.frame(x = seq(-3, 3, length.out = 200))\n\n# Predictions\nx_grid$lm_pred &lt;- predict(lm_fit, newdata = x_grid)       # linear regression predictions\n\n# Plot: Linear regression\np1 &lt;- ggplot(data, aes(x, y)) +\n  geom_point(alpha = 0.6) +\n  geom_line(data = x_grid, aes(x, lm_pred), color = \"red\", size = 1) +\n  labs(title = \"Linear Regression (flawed probabilities)\",\n       y = \"Predicted / Observed\",\n       x = \"X\") +\n  theme_minimal() +\n  ylim(-0.2, 1.2)   # show that predictions can fall outside [0,1]\n\np1\nNon-constant variance (heteroskedasticity):\nIn binary data, the variance of \\(Y\\) is\n\\[\n   \\mathrm{Var}(Y) = p(1-p)\n\\] which depends on \\(p\\). Importantly, the spread of residuals will be smallest near \\(p=0\\) or \\(p=1\\) and largest near \\(p=0.5\\). That’s exactly heteroskedasticity. Linear regression assumes constant variance (homoskedasticity), so its assumptions are violated.\nLet’s show this visually.\n# Extract fitted values and residuals\nfitted_vals &lt;- lm_fit$fitted.values\nresiduals &lt;- lm_fit$residuals\n  \n  # Plot residuals vs fitted values\n  plot(fitted_vals, residuals,\n       xlab = \"Fitted values\",\n       ylab = \"Residuals\",\n       main = \"Residuals vs Fitted (Linear Regression on Binary Outcome)\",\n       pch = 19, col = \"darkblue\")\n  abline(h = 0, col = \"red\", lwd = 2)\n  \nlegend(\"topright\", legend = c(\"Residuals\"),\n         col = c(\"darkblue\", \"darkgreen\"), pch = c(19, NA), lty = c(NA, 2))\nNonlinear relationship between predictors and probability:\nIn reality, probabilities often follow an S-shaped curve — they change slowly when predictors are very small or very large, and change more quickly in the middle. Linear regression cannot capture this shape.\nCan you think of a real-world example where this might happen?\nThe Predict Students’ Dropout and Academic Success dataset was contributed to the UCI Machine Learning Repository in December 2021.\nIt was collected from multiple databases at a higher education institution in Portugal and focuses on undergraduate students across diverse study programs (e.g., agronomy, education, nursing, journalism, management, social service, and technologies).\nThe dataset is designed for classification tasks:\n- Target: Predict whether a student will eventually graduate, remain enrolled without graduating, or drop out.\n- Motivation: Enable early identification of students at risk of failure or dropout, supporting targeted interventions.\n- Challenge: The outcome classes are imbalanced, which makes predictive modeling more complex.\nDataset link: UCI Repository\nThe key characteristics of the dataset are:\nAll features describe student status at admission. They include:\nThe target variable represents the final student status:\ndropout &lt;- read.csv(\"data/dropout.csv\", sep = \";\")\n\ndropout$Dropout &lt;- ifelse(dropout$Target == \"Dropout\", 1, 0)\nFirst, let’s fit an intercept-only model to the data.\nIn our example the variable Dropout indicates whether a subject dropped out. Let’s start with the simplest model for predicting Dropout, the intercept-only model.\nMore specifically, we have $ logit(_i) = b_0(1_i)$where \\(\\pi_i = P(grad_i = 1)\\).\nWe can use the glm() function to fit the model to the data\nfit_intercept &lt;- glm(\n  Dropout ~ 1, \n  family = \"binomial\", \n  data = dropout\n)\n\nsummary(fit_intercept)\n\n\nCall:\nglm(formula = Dropout ~ 1, family = \"binomial\", data = dropout)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.7482     0.0322  -23.24   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5554.5  on 4423  degrees of freedom\nResidual deviance: 5554.5  on 4423  degrees of freedom\nAIC: 5556.5\n\nNumber of Fisher Scoring iterations: 4\nWithout wanting to get to detailed we don’t need to specify the logit link here because it is the canonical link function for the binomial distribution. This essentially means there is a direct correspondence between the predicted mean and the distribution’s canonical location parameter.\nIn the intercept-only model, the intercept, \\(b_0\\), reflects\nexp(-0.7482)\n\n[1] 0.4732176\n\\[ P(Dropout_i = 1) = \\pi_i = \\frac{e^{b_0}}{1+e^{b_0}} \\] or, equivalently, in R\nexp(-0.7482)/(1 + exp(-0.7482))\n\n[1] 0.3212136\nWe can also confirm that the backward transformed parameter from this intercept-only logistic regression matches the expectation we get from the descriptives of the raw data.\nmean(dropout$Dropout)\n\n[1] 0.3212025\nNote: If \\(\\beta_j &gt; 0\\) then \\(\\mathrm{exp}(b_j) &gt; 1\\), indicating a positive relationship between \\(X_{j}\\) and the probability of the event occurring. If \\(\\beta_j &lt; 0\\), the opposite relationship holds.\nOK, let’s include a predictor in our logistic regression model. Let’s start with Age.at.enrollment such that\n\\[ logit(\\pi_i) = b_0 + b_1Age^{*}_{1i} + \\epsilon_i \\] where \\(\\pi_i = P(Dropout_i = 1)\\). Here, \\(Age^{*}\\) is the mean-centered amount of money one spends on themselves in a month (in units of \\(100\\) dollars).\nLet’s fit the model in R.\ndropout$Age_star &lt;- scale(dropout$Age.at.enrollment, center = TRUE, scale = FALSE)\n\nfit_pred &lt;- glm(\n  Dropout ~ 1 + Age_star, \n  family = \"binomial\", \n  data = dropout, \n)\nsummary(fit_pred)\n\n\nCall:\nglm(formula = Dropout ~ 1 + Age_star, family = \"binomial\", data = dropout)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.776123   0.033364  -23.26   &lt;2e-16 ***\nAge_star     0.068854   0.004364   15.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5554.5  on 4423  degrees of freedom\nResidual deviance: 5283.1  on 4422  degrees of freedom\nAIC: 5287.1\n\nNumber of Fisher Scoring iterations: 4\nAgain, There are essentially three ways to interpret coefficients from a logistic regression model:\nLog-Odds\nThe parameter estimate \\(b_0\\) reflects the expected log-odds (\\(-0.776123\\)) of dropping out for an individual who is approximately 23 years old (the average age at admission).\nThe estimate for \\(b_1\\) indicates the expected difference of the log-odds of dropping out for a 1-year difference in age. Therefore, we expect a \\(0.07\\) difference in the log-odds of dropping out for a \\(1\\) year difference in age.\nOdds\nParameter estimates from a logistic regression are often reported in terms of odds rather than log-odds. To obtain parameters in odds units, we simply exponentiate the coefficients. Note that this is just one of the steps of the inverse link function (which would take us all the way to probability units).\nexp(cbind(OR = coef(fit_pred), confint(fit_pred)))\n\n                   OR     2.5 %    97.5 %\n(Intercept) 0.4601866 0.4309351 0.4911538\nAge_star    1.0712794 1.0622291 1.0805629\nIn other words, the odds of dropping out for an average-aged student \\(exp(-0.776123) = 0.46\\).\nIn regard to the slope coefficient, for 1-year difference in age, we expect to see about \\(7\\%\\) increase in the odds of dropping out. This increase does not depend on the age of the individual. Note this is significant and we would likely report this interpretation. Essentially, if the odds ratio is equal to one, the predictor did not have an impact on the outcome.",
    "crumbs": [
      "Regression Methods",
      "Simple Logistic Regression"
    ]
  },
  {
    "objectID": "week3_3.html#logistic-regression-solution",
    "href": "week3_3.html#logistic-regression-solution",
    "title": "Simple Logistic Regression",
    "section": "Logistic Regression Solution",
    "text": "Logistic Regression Solution\nLogistic regression fixes these problems by modeling the log-odds of success as a linear function:\n\\[\n\\text{logit}(p(x)) = \\ln \\left(\\frac{p(x)}{1-p(x)}\\right) = \\beta_0 + \\beta_1 X\n\\]\nThis ensures:\n\nPredicted probabilities are always between 0 and 1: \\[\np(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}}\n\\]\nThe variance is modeled correctly.\n\nThe S-shaped curve naturally captures how probabilities change with \\(X\\).",
    "crumbs": [
      "Regression Methods",
      "Simple Logistic Regression"
    ]
  },
  {
    "objectID": "week3_3.html#example-data-dropout-data",
    "href": "week3_3.html#example-data-dropout-data",
    "title": "Simple Logistic Regression",
    "section": "Example Data: Dropout Data",
    "text": "Example Data: Dropout Data",
    "crumbs": [
      "Regression Methods",
      "Simple Logistic Regression"
    ]
  },
  {
    "objectID": "week3_3.html#simple-logistic-regression-in-r",
    "href": "week3_3.html#simple-logistic-regression-in-r",
    "title": "Simple Logistic Regression",
    "section": "Simple Logistic Regression in R",
    "text": "Simple Logistic Regression in R",
    "crumbs": [
      "Regression Methods",
      "Simple Logistic Regression"
    ]
  },
  {
    "objectID": "week3_1.html",
    "href": "week3_1.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Simple linear regression is a statistical method used to model the relationship between two variables:\n- one feature (also called the predictor or independent variable),\n- one target (also called the outcome or dependent variable).\nThe general form of a simple linear regression model is:\n\\[\nY_i = \\beta_0 + \\beta_1 X_i + \\varepsilon_i, \\quad \\quad \\text{for}\\:i=1,2,\\dots,n\n\\]\nwhere:\n- \\(Y_i\\) : the ith observation on the target variable we want to predict.\n- \\(X\\) : the ith observation of the feature (or predictor) variable.\n- \\(\\beta_0\\): the intercept, which represents the expected value of \\(Y\\) when \\(X=0\\).\n- \\(\\beta_1\\): the slope, which tells us how much \\(Y\\) changes for a one-unit increase in \\(X\\)\n- \\(\\varepsilon_i\\): the ith observation of the error term, which captures random variation not explained by \\(X\\).\nIn practice, we estimate the parameters \\(\\beta_0\\) and \\(\\beta_1\\) using data by minimizing the residual sum of squares (RSS). It can be helpful to visualize what is meant by residual sum of squares and the fitted or optimal regression line.\nOnce we have estimated the coefficients by finding the coefficients that minimize the RSS, the fitted equation can then be written as:\n\\[\n\\hat{Y}_i = b_0 + b_1 X_i\n\\]\nwhere \\(\\hat{Y}_i\\) is the predicted value of \\(Y\\) for the ith case (or person), and \\(b_0, b_1\\) are the estimated coefficients from the sample data.\nThis simple framework allows us to understand how two variables are related and to make predictions about \\(Y\\) given new values of \\(X\\).\nThe Ames Housing dataset is a widely used dataset in regression and machine learning tutorials. It contains detailed information about residential homes in Ames, Iowa, and is often used to predict sale prices of homes based on various characteristics. For the regression chapters we will use the Ames Housing dataset to illustrate concepts in linear regression.\nThe dataset includes a mix of numerical, categorical, and ordinal variables covering:\n# Install and load package\n# install.packages(\"AmesHousing\")\nlibrary(AmesHousing)\n\n# Load data\names &lt;- make_ames()\n\n# View first few rows\nhead(ames)\n\n# A tibble: 6 × 81\n  MS_SubClass             MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape\n  &lt;fct&gt;                   &lt;fct&gt;            &lt;dbl&gt;    &lt;int&gt; &lt;fct&gt;  &lt;fct&gt; &lt;fct&gt;    \n1 One_Story_1946_and_New… Resident…          141    31770 Pave   No_A… Slightly…\n2 One_Story_1946_and_New… Resident…           80    11622 Pave   No_A… Regular  \n3 One_Story_1946_and_New… Resident…           81    14267 Pave   No_A… Slightly…\n4 One_Story_1946_and_New… Resident…           93    11160 Pave   No_A… Regular  \n5 Two_Story_1946_and_New… Resident…           74    13830 Pave   No_A… Slightly…\n6 Two_Story_1946_and_New… Resident…           78     9978 Pave   No_A… Slightly…\n# ℹ 74 more variables: Land_Contour &lt;fct&gt;, Utilities &lt;fct&gt;, Lot_Config &lt;fct&gt;,\n#   Land_Slope &lt;fct&gt;, Neighborhood &lt;fct&gt;, Condition_1 &lt;fct&gt;, Condition_2 &lt;fct&gt;,\n#   Bldg_Type &lt;fct&gt;, House_Style &lt;fct&gt;, Overall_Qual &lt;fct&gt;, Overall_Cond &lt;fct&gt;,\n#   Year_Built &lt;int&gt;, Year_Remod_Add &lt;int&gt;, Roof_Style &lt;fct&gt;, Roof_Matl &lt;fct&gt;,\n#   Exterior_1st &lt;fct&gt;, Exterior_2nd &lt;fct&gt;, Mas_Vnr_Type &lt;fct&gt;,\n#   Mas_Vnr_Area &lt;dbl&gt;, Exter_Qual &lt;fct&gt;, Exter_Cond &lt;fct&gt;, Foundation &lt;fct&gt;,\n#   Bsmt_Qual &lt;fct&gt;, Bsmt_Cond &lt;fct&gt;, Bsmt_Exposure &lt;fct&gt;, …\n\n# Summary of key variables\nsummary(ames$Sale_Price)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12789  129500  160000  180796  213500  755000 \n\nsummary(ames$Gr_Liv_Area)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    334    1126    1442    1500    1743    5642 \n\ntable(ames$Neighborhood)\n\n\n                             North_Ames                           College_Creek \n                                    443                                     267 \n                               Old_Town                                 Edwards \n                                    239                                     194 \n                               Somerset                      Northridge_Heights \n                                    182                                     166 \n                                Gilbert                                  Sawyer \n                                    165                                     151 \n                         Northwest_Ames                             Sawyer_West \n                                    131                                     125 \n                               Mitchell                               Brookside \n                                    114                                     108 \n                               Crawford                  Iowa_DOT_and_Rail_Road \n                                    103                                      93 \n                             Timberland                              Northridge \n                                     72                                      71 \n                            Stone_Brook South_and_West_of_Iowa_State_University \n                                     51                                      48 \n                            Clear_Creek                          Meadow_Village \n                                     44                                      37 \n                              Briardale                     Bloomington_Heights \n                                     30                                      28 \n                                Veenker                         Northpark_Villa \n                                     24                                      23 \n                                Blueste                                  Greens \n                                     10                                       8 \n                            Green_Hills                                Landmark \n                                      2                                       1 \n                            Hayden_Lake \n                                      0\nWe can fit a simple linear regression model using the lm() function in R. Note we are not using cross-validation here, simply fitting the training data to illustrate the concept of simple linear regression.\nFor example, if we want to predict happiness (Sale_Price) based on house square footage (Gr_Liv_Area), we can use the following code:\nfit_slr &lt;- lm(\n  Sale_Price ~ Gr_Liv_Area, \n  data = ames\n)\nsummary(fit_slr)\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-483467  -30219   -1966   22728  334323 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13289.634   3269.703   4.064 4.94e-05 ***\nGr_Liv_Area   111.694      2.066  54.061  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56520 on 2928 degrees of freedom\nMultiple R-squared:  0.4995,    Adjusted R-squared:  0.4994 \nF-statistic:  2923 on 1 and 2928 DF,  p-value: &lt; 2.2e-16\nWe can interpret the results from summary in the following way:\nThe estimated coefficients from our model are \\(\\beta_0=13,289\\) and \\(\\beta_1=111.7\\).\nWe interpret the intercept, \\(\\beta_0=13,289\\), as the estimated mean sale price when the square footage is 0. While this interpretation may not be meaningful in a real-world context (since a house with 0 square footage does not exist), it serves as a baseline for our model.\nWe interpret the slope coefficient, \\(\\beta_1=111.7\\), as the estimated change in mean sale price for each additional square foot of living area. Specifically, for each one-unit increase in square footage, the mean sale price is estimated to increase by $111.7, holding all else constant. This indicates a positive relationship between square footage and sale price in our sample.\nIn addition to estimating the coefficients, we can also perform hypothesis tests and construct confidence intervals to assess the statistical significance and precision of our estimates.\nIn the simple linear regression model\n\\[\nY_i = \\beta_0 + \\beta_1 X + \\varepsilon_i\n\\]\nthe error term \\(\\varepsilon\\) is at the heart of the model.\nClassical linear regression (the Gauss–Markov framework) makes several key assumptions about \\(\\varepsilon\\):\nIn practice, \\(\\sigma^2\\) is unknown, so we estimate it with the residual variance:\n\\[\ns^2 = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n - 2}\n\\]\nwhere:\n- \\(y_i\\): the observed outcome,\n- \\(\\hat{y}_i\\): the predicted outcome,\n- \\(n\\): the sample size,\n- denominator \\(n-2\\) accounts for the two estimated parameters (\\(\\beta_0, \\beta_1\\)).",
    "crumbs": [
      "Regression Methods",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "week3_1.html#example-data-ames-housing",
    "href": "week3_1.html#example-data-ames-housing",
    "title": "Simple Linear Regression",
    "section": "Example Data: Ames Housing",
    "text": "Example Data: Ames Housing",
    "crumbs": [
      "Regression Methods",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "week3_1.html#simple-linear-regression-in-r",
    "href": "week3_1.html#simple-linear-regression-in-r",
    "title": "Simple Linear Regression",
    "section": "Simple Linear Regression in R",
    "text": "Simple Linear Regression in R",
    "crumbs": [
      "Regression Methods",
      "Simple Linear Regression"
    ]
  },
  {
    "objectID": "week2_5f.html",
    "href": "week2_5f.html",
    "title": "Categorical Feature Engineering",
    "section": "",
    "text": "Many models require that the predictors take numeric form. There are exceptionssuch as tree-based models, however, even tree-based methods can benefit from preprocessing categorical features. The following sections will discuss a few of the more common approaches to engineer categorical features.\n\nLumping\n\nSometimes features will contain levels that have very few observations. For example, take a look at the work status variable wrkstat. There are 8 unique levels and some have relatively few observations. For example, With A Job, But Not At Work Because Of Temporary Illness, Vacation, Strike.\n\ncount(data, wrkstat) %&gt;% arrange(n)\n\n# A tibble: 9 × 2\n  wrkstat                                                                      n\n  &lt;fct&gt;                                                                    &lt;int&gt;\n1 &lt;NA&gt;                                                                        10\n2 With A Job, But Not At Work Because Of Temporary Illness, Vacation, Str…    61\n3 In School                                                                   90\n4 Other                                                                      113\n5 Unemployed, Laid Off, Looking For Work                                     168\n6 Keeping House                                                              292\n7 Working Part Time                                                          319\n8 Retired                                                                    796\n9 Working Full Time                                                         1460\n\n\nSometimes we can benefit from collapsing, or “lumping” these into a lesser number of categories. In the above examples, we may want to collapse all levels that are observed in less than 5% of the training sample into an Other category. We can use step_other() to do so.\n\nlumping &lt;- recipe(happy ~ ., data = data) %&gt;%\n  step_other(wrkstat, threshold = 0.05, \n             other = \"Other\")\n\n# Apply this blue print --&gt; you will learn about this at \n# the end of the chapter\napply_2_training &lt;- prep(lumping, training = data) %&gt;%\n  bake(data)\n\n# New distribution of Neighborhood\ncount(apply_2_training, wrkstat) %&gt;% arrange(n)\n\n# A tibble: 7 × 2\n  wrkstat                                    n\n  &lt;fct&gt;                                  &lt;int&gt;\n1 &lt;NA&gt;                                      10\n2 Unemployed, Laid Off, Looking For Work   168\n3 Other                                    264\n4 Keeping House                            292\n5 Working Part Time                        319\n6 Retired                                  796\n7 Working Full Time                       1460\n\n\n\nOne-Hot and Dummy Encoding\n\nAs mentioned previously many models require that all features be numeric. Consequently, we need to intelligently transform any categorical variables into numeric representations so that these algorithms will work.\nThere are many ways to recode categorical variables as numeric (e.g., one-hot, ordinal, binary, sum, and Helmert).\nOne-hot encoding is a common method for converting categorical variables into a numerical format that machine learning algorithms can work with. Instead of assigning arbitrary numbers to categories (which could incorrectly imply an order), one-hot encoding creates a new binary (0/1) column for each category level. For a given observation, the column corresponding to its category is set to 1, and all others are set to 0.\n\n\n\nhttps://towardsdatascience.com/encoding-categorical-variables-one-hot-vs-dummy-encoding-6d5b9c46e2db/\n\n\nHowever, this creates perfect collinearity which causes problems with some predictive modeling algorithms (e.g., ordinary linear regression and neural networks). Alternatively, we can create a full-rank encoding by dropping one of the levels (level blue has been dropped). This is referred to as dummy coding.\n\n\n\nhttps://towardsdatascience.com/encoding-categorical-variables-one-hot-vs-dummy-encoding-6d5b9c46e2db/\n\n\nBelow is an example of one-hot encoding the predictors in our model.\n\nrecipe(happy ~ ., data = data) %&gt;%\n  step_dummy(all_factor_predictors(), one_hot = TRUE)",
    "crumbs": [
      "Feature Engineering",
      "Categorical Feature Engineering"
    ]
  },
  {
    "objectID": "week2_5d.html",
    "href": "week2_5d.html",
    "title": "Feature Filtering",
    "section": "",
    "text": "Feature filtering is a feature selection technique in machine learning where features are evaluated prior to the model fitting based on statistical or heuristic criteria. Features are then kept or discarded based on those criteria.\nIt is called filtering because it is a preprocessing step, part of a feature engineering pipeline, done before we actually train the model.\nTypically, the goals of feature filtering are:\nIn practice, one of the filtering tasks we will typically conduct involves weeding out low variance features.\nZero and near-zero variance variables are low-hanging fruit to eliminate.\nZero and near-zero variance variables typically offer little to no information for model building. Furthermore, resampling (data-splitting) further complicates this picture because a given fold or sample may only contain a single value if the variable itself only contains a few unique values.\nBoehmke and Greenwell (2019) suggest the following rule-of-thumb for removing low-variance features:\nRemove a variable if:\nWe can use the caret (Kuhn and Max 2008) package in R to look at these different metrics for our example data.\nlibrary(dplyr)\nlibrary(caret)\ncaret::nearZeroVar(data, saveMetrics = TRUE) %&gt;% \n  tibble::rownames_to_column() \n\n   rowname freqRatio percentUnique zeroVar   nzv\n1    happy  2.683688    0.09066183   FALSE FALSE\n2      age  1.014085    2.17588395   FALSE FALSE\n3      sex  1.242672    0.06044122   FALSE FALSE\n4     race  3.986014    0.09066183   FALSE FALSE\n5     educ  1.191964    0.63463282   FALSE FALSE\n6   income 12.948571    0.36264733   FALSE FALSE\n7   childs  1.209166    0.27198549   FALSE FALSE\n8  wrkstat  1.834171    0.24176488   FALSE FALSE\n9  marital  1.287063    0.15110305   FALSE FALSE\n10    born  6.618056    0.06044122   FALSE FALSE\n11 partyid  1.558271    0.24176488   FALSE FALSE\n12  adults  1.075205    0.18132366   FALSE FALSE\n13  earnrs  1.560570    0.12088244   FALSE FALSE",
    "crumbs": [
      "Feature Engineering",
      "Feature Filtering"
    ]
  },
  {
    "objectID": "week2_5d.html#removing-low-variance-features",
    "href": "week2_5d.html#removing-low-variance-features",
    "title": "Feature Filtering",
    "section": "Removing Low-Variance Features",
    "text": "Removing Low-Variance Features",
    "crumbs": [
      "Feature Engineering",
      "Feature Filtering"
    ]
  },
  {
    "objectID": "week2_5b.html",
    "href": "week2_5b.html",
    "title": "Example Data",
    "section": "",
    "text": "To demonstrate feature engineering we will use some example data from the The General Social Survey (GSS).\nGSS is a long-running, nationally representative survey of adults in the United States that has been conducted almost every two years since 1972 by the National Opinion Research Center (NORC) at the University of Chicago. The GSS data is often used to measure American’s attitudes, behaviors, and beliefs on a wide range of topics—such as politics, religion, crime, race relations, family, work, and technology.\nThere are two ways to access the GSS data. One is using the GSS Data Explorer. Another option is using the gssr (Healy 2023) R package. Here we will use the gssr (Healy 2023) package to download some example data.\nWe will start by choosing happy as our target variable. This comes from Question 157 of the 2024 GSS where respondents were asked:\nResponses were coded such that 1 indicates “very happy”, 2 indicates “pretty happy”, and 3 indicates “not too happy”, while NA indicates “don’t know.”\nWe can also identify ad bunch of features we think predict self-reported happiness and save our final dataset.\nlibrary(gssr)\n\ngss24 &lt;- gss_get_yr(2024)\n\ntarget &lt;- \"happy\"\n\nfeatures &lt;- c(\n  \"age\",     # age of participant\n  \"sex\",     # sex of participant\n  \"race\",    # race of participant\n  \"educ\",    # highest education completed by participant\n  \"income\",  # income of participant\n  \"childs\",  # number of children participant has\n  \"wrkstat\", # work force status\n  \"marital\", # marital status\n  \"born\",    # whether or not participant was born in USA\n  \"partyid\", # political party of participant\n  \"adults\",  # num of fam members 18 or older\n  \"earnrs\"   # number of earners in family  \n)\n\ndata &lt;- gss24[,c(target, features)]\n\ntable(data$happy, useNA = \"always\")\n\n\n   1    2    3 &lt;NA&gt; \n 684 1892  705   28\nNext, let’s clean up data a bit, discarding some of the labels and missing value information we don’t need. The data in gss24 retains the labeling structure provided by the GSS. Variables are stored numerically with labels attached to them. Often, when using the data in R, it will be convenient to convert the categorical variables we are interested in to character or factor type instead.\nHere we can use code from the gssr package introduction to simplify this recoding. The only thing we need to do is define the categorical variables in our data.\nlibrary(dplyr)\n\ncat_vars &lt;- c(\"sex\",\"race\", \"educ\", \"wrkstat\", \"marital\", \"born\", \"partyid\")\ncont_vars &lt;- c(\"happy\", \"age\", \"income\", \"childs\", \"adults\", \"earnrs\")\n\ncapwords &lt;- function(x, strict = FALSE) {\n    cap &lt;- function(x) paste(toupper(substring(x, 1, 1)),\n                  {x &lt;- substring(x, 2); if(strict) tolower(x) else x},\n                             sep = \"\", collapse = \" \" )\n    sapply(strsplit(x, split = \" \"), cap, USE.NAMES = !is.null(names(x)))\n}\n\ndata &lt;- data |&gt;\n  mutate(\n    # Convert all missing to NA\n    across(everything(), haven::zap_missing), \n    across(all_of(cont_vars), as.numeric),\n    # Make all categorical variables factors and relabel nicely\n    across(all_of(cat_vars), forcats::as_factor),\n    across(all_of(cat_vars), \\(x) forcats::fct_relabel(x, capwords, strict = TRUE))\n  )",
    "crumbs": [
      "Feature Engineering",
      "Example Data"
    ]
  },
  {
    "objectID": "week2_4.html",
    "href": "week2_4.html",
    "title": "Bias and Variance",
    "section": "",
    "text": "Prediction errors can be broken down into two main parts: bias and variance.\nThese represent different sources of mistakes a model can make.\nUsually, there’s a tradeoff between keeping bias low and keeping variance low. It can be exceptionally hard to minimize both at the same time.\nBy understanding where bias and variance come from, we can make better choices when building models and end up with more accurate predictions.\nLet’s approach each concept in turn.\nBias is the difference between the average prediction from our model and the true value which we are trying to predict.\nIn machine learning, bias often refers to the systematic error in a model.\nBias in ML can lead to incorrect or unfair outcomes, possibly from flawed or incomplete training data, or perhaps from poor assumptions made by the algorithm.\nA biased model fails to accurately reflect the true relationship in the data, leading to poor generalization and skewed predictions.\nIn statistics, variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value.\nIn machine learning, variance typically refers to the sensitivity of a model’s predictions to small changes in the training data.\nSo, error due to variance is defined as the variability of a model prediction for a given data point.\nBias and variance are often introduced together in the context of the bias-variance tradeoff that occurs when models overfit, or underfit the data.\nWhen a model overfits the data, it means the model has learned too much from the training data, including random noise or minor fluctuations, rather than just the true underlying patterns.\nWhen a model underfits the data, it means the model is too simple to capture the underlying patterns in the data.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_4.html#variance-in-machine-learning",
    "href": "week2_4.html#variance-in-machine-learning",
    "title": "Bias and Variance",
    "section": "Variance in Machine Learning",
    "text": "Variance in Machine Learning",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_4.html#the-bias-variance-tradeoff",
    "href": "week2_4.html#the-bias-variance-tradeoff",
    "title": "Bias and Variance",
    "section": "The Bias Variance Tradeoff",
    "text": "The Bias Variance Tradeoff",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_2.html",
    "href": "week2_2.html",
    "title": "Problem Classes in ML",
    "section": "",
    "text": "Machine learning applications correspond to a wide variety of learning problems. Some major classes include:\nWe will now complete a short in-class exercise demonstrating how one might build a binary classifier to solve a classic classification problem.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#classification",
    "href": "week2_2.html#classification",
    "title": "Problem Classes in ML",
    "section": "Classification",
    "text": "Classification\n\nThe goal of classification is often toassign a category or label to each observation.\nThe music genre problem is a good example of a classification problem\nNote: The number of categories can be small, large, or even unbounded (e.g., optical character recognition, text classification, speech recognition).\n\n\n\n\nhttps://datahacker.rs/008-machine-learning-multiclass-classification-and-softmax-function/\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve classification problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#regression",
    "href": "week2_2.html#regression",
    "title": "Problem Classes in ML",
    "section": "Regression",
    "text": "Regression\n\nIn regression problems we are typically concerned with predicting a real-valued number for each item.\nFor example, we might be interested in predicting stock prices, or how .\nstressed out someone is.\nNotes: The penalty for prediction errors depends on the magnitude of the difference between true and predicted values, unlike classification where categories are discrete and there is often no notion of distance between various categories.\n\n\n\nhttps://builtin.com/data-science/regression-machine-learning\n\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve regression-type problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#ranking",
    "href": "week2_2.html#ranking",
    "title": "Problem Classes in ML",
    "section": "Ranking",
    "text": "Ranking\n\nWith ranking problems we are often tasked with ordering items according to a specific criterion.\nA well-known example of a ranking algorithm is Google’s PageRank algorithm.\nNotes: Ranking problems focus on relative order rather than exact category or numeric value.\n\n\n\n\nhttps://towardsdatascience.com/wp-content/uploads/2023/08/1ZFmd2Q-G95ArY93od20Adw.png\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve ranking problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#clustering",
    "href": "week2_2.html#clustering",
    "title": "Problem Classes in ML",
    "section": "Clustering",
    "text": "Clustering\n\nIn clustering problems we are typically looking topartition items into similar groups or regions.\nFor example, in social network analysis we are interested in identifying communities within large groups of people\nNotes: Clustering is often applied to very large datasets to reveal underlying structure.\n\n\n\n\nhttps://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\n\n\nHow do the classification and clustering problems discussed thus far differ?\nCan you think of any examples of algorithms you encounter in your everyday life that solve clustering problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#dimensionality-reduction",
    "href": "week2_2.html#dimensionality-reduction",
    "title": "Problem Classes in ML",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nWhen looking to solve dimension reduction problems we are typically interested in transforming high-dimensional data into a lower-dimensional representation while preserving important properties.\nCommon examples in psychology include identifying constructs in survey data or scales.\nNotes: Dimension reduction is also useful for feature extraction, noise reduction, and speeding up downstream learning tasks.\n\n\n\n\nhttps://www.sthda.com/english/sthda-upload/figures/principal-component-methods/006-principal-component-analysis-pca-variable-cos2-corrplot-1.png\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve dimension reduction problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week1_4.html",
    "href": "week1_4.html",
    "title": "Describing Data Visually",
    "section": "",
    "text": "This section will briefly introduce you to visualizing data using the ggplot2 package. R has a number of systems for making graphs but ggplot2 is by far the most developed option. ggplot2 implements the grammar of graphics, and if you’d like to learn more about the motivation for this work take a look at The Layered Grammar of Graphics.",
    "crumbs": [
      "Introduction to R",
      "Describing Data Visually"
    ]
  },
  {
    "objectID": "1_2.html",
    "href": "1_2.html",
    "title": "Reading in Data",
    "section": "",
    "text": "One of the most important initial tasks you’ll face in R is reading in data. Let’s walk through some basics.\nBy reading in data we are generally refering to the process of importing data from a local directory on your computer, or from the web, into R. When we read a data file into R, we often read it in as a tabularobject where columns representing variables and rows representing cases. This is not always the case but covers the most basic use case.\nMany different data file formats can be read into R as data frames, such as .csv (comma separated values), .xlsx (Excel workbook), .txt (text), .sas7bdat (SAS), and .sav (SPSS) can be read into R. We will mostly focus on the .csv case in this class.",
    "crumbs": [
      "Introduction to R",
      "Reading in Data"
    ]
  },
  {
    "objectID": "1_1.html",
    "href": "1_1.html",
    "title": "Basic R Programming",
    "section": "",
    "text": "Before getting started we will need to install R and RStudio.",
    "crumbs": [
      "Introduction to R",
      "Basic R Programming"
    ]
  },
  {
    "objectID": "1_1b.html",
    "href": "1_1b.html",
    "title": "R Packages",
    "section": "",
    "text": "One of the main reasons to be excited about R is the package library system. In R, packages are like apps. R packages extend the functionality of base R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded freely without much hassle (compared to many other package libraries). For example, we will often use the ggplot2 package for plotting and visualizing data and the psych package for describing data numerically.\nTo install R packages you can use the install.packages() function directly in the console as follows:\ninstall.packages(\"ggplot2\")\nor you can do so from the Packages tab of the Files pane in RStudio using the following steps:\nOnce a package is installed you can load it into your environment using the library() function. Once loaded you have access to all the R functions supplied by that package.\nlibrary(\"ggplot2\")\nThere are many ways to find useful R packages on the internet simply by googling the type of data or analysis you are interested in. Another way is to look at the CRAN Task Views. For example, there is an interesting Task View for Machine Learning that contains many relevant packages you might be interested in using. Some class members expressed interest in F1 racing. In the Sports Analytics Task View there is a f1dataR package that provides historical data from the beginning of Formula 1.",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "1_1b.html#using-r-packages",
    "href": "1_1b.html#using-r-packages",
    "title": "R Packages",
    "section": "Using R Packages",
    "text": "Using R Packages",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "1_1b.html#finding-useful-r-packages",
    "href": "1_1b.html#finding-useful-r-packages",
    "title": "R Packages",
    "section": "Finding Useful R Packages",
    "text": "Finding Useful R Packages",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "week1_3.html",
    "href": "week1_3.html",
    "title": "Describing Data Numerically",
    "section": "",
    "text": "Once you have read in a data frame it can be useful to know how the variables are understood by R. For example, let’s look at some Kaggle Marketing Analytics Data. You can download the raw data here.",
    "crumbs": [
      "Introduction to R",
      "Describing Data Numerically"
    ]
  },
  {
    "objectID": "week2_1.html",
    "href": "week2_1.html",
    "title": "Important Terms and Distinctions",
    "section": "",
    "text": "The amount of jargon one finds when digging into the machine learning (ML) literature is a common source of confusion for new learners. Many ML terms are borrowed from statistics, computer science, or everyday language, but they can carry subtly or even radically different meanings in the context of ML.\nIn the following overview I will provide a high-level summary of some common terms used in machine learning, and try to highlight the differences between concepts that may seem familiar to you, but can represent distinct ideas.\nTwo important distinction I will try to highlight are the differences between:\nArtificial Intelligence (AI) and Machine Learning (ML) are related but distinct concepts in computer science and data analysis.\nML approaches rely heavily on accurate and efficient prediction algorithms.\nSo, what exactly are algorithms?\nAn algorithm is a step-by-step, well-defined procedure for solving a problem or completing a task.\nIn computing and machine learning, algorithms are generally a finite sequence of instructions that takes some input, follows a set of rules, and produces an output.\nA model in machine learning is the learned representation of patterns in data, produced by applying a learning algorithm to a dataset.\nA model in ML is the vehicle to make predictions, classify new examples, or infer relationships.\nUnderstanding what algorithm to choose for a specific problem or application is often difficult. In this class we will devote a lot of time on how to make these decisions, and communicate the results of an analysis. That said, we often have minimal knowledge of the problem or data at hand when we first approach an applied problem, and it is difficult to know which ML method will perform best. This idea is generally known as the no free lunch theorem\nAn algorithm is just a set of step-by-step instructions to solve a problem. Machine learning algorithms are no different—they’re just systematic ways of finding patterns in data. Let’s explore what that means.\nIn groups of 3–4, imagine you are writing an algorithm for a simple real-world problem:\nEach group should discuss (and prepare to share):",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#artificial-intelligence-ai",
    "href": "week2_1.html#artificial-intelligence-ai",
    "title": "Important Terms and Distinctions",
    "section": "Artificial Intelligence (AI)",
    "text": "Artificial Intelligence (AI)\n\nAI is the broad field of creating systems that can perform tasks that normally require human intelligence.\nTypically, the goal of AI is to create programs that can simulate intelligent behavior, whether or not they learn from data.\nCommon Examples of AI include:\n\nDigital assistants, LLMs\n\n\nCan you think of any examples of AI you encounter in your everyday life?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#machine-learning-ml",
    "href": "week2_1.html#machine-learning-ml",
    "title": "Important Terms and Distinctions",
    "section": "Machine Learning (ML)",
    "text": "Machine Learning (ML)\n\nML is a branch of AI focused on systems that learn patterns from data and improve automatically from experience.\nWhen doing ML we are generally interested in creating models that generalize well enough to make accurate predictions on unseen inputs.\nCommon use cases for ML in everyday life include:\n\nRecommendation systems, image recognition\n\n\nCan you think of any examples of ML you encounter in your everyday life?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#differences-between-ai-and-ml",
    "href": "week2_1.html#differences-between-ai-and-ml",
    "title": "Important Terms and Distinctions",
    "section": "Differences Between AI and ML",
    "text": "Differences Between AI and ML\n\n\n\n\n\n\n\n\n\n\nAspect\nArtificial Intelligence (AI)\nMachine Learning (ML)\n\n\n\n\nFocus\nSimulating intelligent behavior\nLearning patterns from data\n\n\nGoal\nBroader human-like intelligence\nSpecific predictive or decision-making tasks\n\n\nExamples\nChess engines, self-driving cars, expert systems\nRegression, neural networks, clustering\n\n\n\nBroadly speaking, ML is a branch of AI focused on training models to recognize patterns and make predictions using algorithms. AI encompasses a broader set of techniques, some of which do not involve learning from data at all.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#characteristics-of-an-algorithm",
    "href": "week2_1.html#characteristics-of-an-algorithm",
    "title": "Important Terms and Distinctions",
    "section": "Characteristics of an Algorithm",
    "text": "Characteristics of an Algorithm\n\nInput – Data the algorithm operates on\nOutput – The result produced after execution\nFiniteness – Must finish in a finite number of steps\nDefiniteness – Each step is clear and unambiguous\nEffectiveness – Each operation can actually be carried out",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#algorithms-in-machine-learning",
    "href": "week2_1.html#algorithms-in-machine-learning",
    "title": "Important Terms and Distinctions",
    "section": "Algorithms in Machine Learning",
    "text": "Algorithms in Machine Learning\n\nIn ML, an algorithm is the procedure used to train a model from data.\nExample: Linear regression algorithm finds the best-fit line by minimizing error.\nExample: Decision tree algorithm splits data into branches by checking features step by step.\n\nOne can think of an algorithm as the recipe one follows when cooking a meal. The ingredients of the dish are analogous to the input data, and the finished dish is equivalent to the model.\n\n\n\nhttps://xkcd.com/1667/\n\n\nSo, how does an algorithm differ from a model? What is a model in the context of ML?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#definition",
    "href": "week2_1.html#definition",
    "title": "Important Terms and Distinctions",
    "section": "Definition",
    "text": "Definition\nA model is the outcome of training a machine learning algorithm on data.It encapsulates the patterns, relationships, or structure discovered in the training data.\n\nInput: Features from the training data\n\nProcess: Algorithm (e.g., linear regression, decision tree, neural network)\n\nOutput: Parameters, structure, or rules that allow prediction",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#examples",
    "href": "week2_1.html#examples",
    "title": "Important Terms and Distinctions",
    "section": "Examples",
    "text": "Examples\n\n\n\n\n\n\n\n\nAlgorithm\nResulting Model\nWhat It Represents\n\n\n\n\nLinear Regression\nA line (y = mx + b)\nRelationship between input and output variables\n\n\nDecision Tree\nTree of splits\nHow features split to predict classes or values\n\n\nNeural Network\nLayers of neurons & weights\nComplex non-linear mappings between inputs and outputs\n\n\nk-Means Clustering\nCluster centroids\nGrouping of data points into clusters",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#things-to-remember",
    "href": "week2_1.html#things-to-remember",
    "title": "Important Terms and Distinctions",
    "section": "Things to Remember",
    "text": "Things to Remember\n\nThe algorithm is the procedure for learning\n\nThe model is the trained artifact used for prediction or inference\nDifferent algorithms can produce different models from the same data\nA model generalizes patterns from training data to unseen data\nAlgorithm = recipe (instructions for learning)\nModel = finished dish (learned representation ready to use)\n\nThe model is the end product of learning — the part that actually “knows” something about the data and can be used to make predictions. In ML: Data + Algorithm → Model → Predictions.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_3.html",
    "href": "week2_3.html",
    "title": "Data Splitting",
    "section": "",
    "text": "Data splitting (e.g., dividing data into training, validation, and test samples) is crucial because it protects us from fooling ourselves about how well a model actually performs.\nIn our discussion of the training and validation samples we introduce the idea of parameters and hyperparameters. The distinction between these terms can often can trip people up, so it’s worth explaining the difference.\nIn groups of 3–4, discuss:\nSo, what does all this data splitting mean for us in practice? Let’s take a look at one of the more popular data-splitting methods used in practice: K-Folds Cross-Validation",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#parameters-vs.-hyperparameters",
    "href": "week2_3.html#parameters-vs.-hyperparameters",
    "title": "Data Splitting",
    "section": "Parameters vs. Hyperparameters",
    "text": "Parameters vs. Hyperparameters\n\nParameters are values learned automatically (or estimated) from the training data. The model parameters are often the thing we are interested in estimating in a given algorithm.\nFor example. the slope of the line in a simple linear regression prediction problem relating amount of treatment (feature) to depression score (target).\nHyperparameters are set before training begins, and control how the learning process works. The selection of hyperparameters is often critical to model performance, as in procedures like k-folds cross-validation, playing a critical role in how the model generalizes.\nFor example, the the number of clusters used to partition the data",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#role-of-parameters-in-data-splitting",
    "href": "week2_3.html#role-of-parameters-in-data-splitting",
    "title": "Data Splitting",
    "section": "Role of Parameters in Data Splitting",
    "text": "Role of Parameters in Data Splitting\nWhen we split data, each portion plays a distinct role:\n\nTraining set → Fit the model parameters.\n\nValidation set → Used to compare different hyperparameter choices.\n\nTest set → Used only once, at the very end, to estimate generalization performance.\n\nImportant note: Hyperparameters should be tuned using the validation set, not the test set. If we adjust hyperparameters based on the test set, we are indirectly training on it and lose the ability to measure real-world performance.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#example-k-means-clustering",
    "href": "week2_3.html#example-k-means-clustering",
    "title": "Data Splitting",
    "section": "Example: k-Means Clustering",
    "text": "Example: k-Means Clustering\n\nParameters (learned from the data)\n\nCluster centroids: the coordinates of the cluster centers.\n\nThese are calculated by the algorithm during training and adjusted iteratively until convergence.\n\nYou do not set them manually — the algorithm figures them out.\n\n\n\nHyperparameters (set before training)\n\nNumber of clusters (k): chosen by the user before running the algorithm.\n\nInitialization method (e.g., random, k-means++).\n\nMaximum number of iterations allowed.\n\nThese control how the algorithm runs, but are not learned from the data itself.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#prevents-overfitting-to-the-training-data",
    "href": "week2_3.html#prevents-overfitting-to-the-training-data",
    "title": "Data Splitting",
    "section": "Prevents Overfitting to the Training Data",
    "text": "Prevents Overfitting to the Training Data\n\nWhen a model is trained, it adapts to patterns in the training set.\n\nIf we only check performance on that same data, we might think the model is excellent — but it may just be memorizing instead of generalizing.\n\nA separate test set lets us see how the model behaves on new, unseen data.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#simulates-real-world-performance",
    "href": "week2_3.html#simulates-real-world-performance",
    "title": "Data Splitting",
    "section": "Simulates Real-World Performance",
    "text": "Simulates Real-World Performance\n\nIn real life, the model will be applied to data it hasn’t seen before.\n\nBy holding out a test set, we simulate that scenario, giving us a better sense of expected performance in practice.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#helps-with-model-selection-tuning",
    "href": "week2_3.html#helps-with-model-selection-tuning",
    "title": "Data Splitting",
    "section": "Helps with Model Selection & Tuning",
    "text": "Helps with Model Selection & Tuning\n\nA validation set (or cross-validation) is used to choose hyperparameters (like tree depth, learning rate, regularization strength).\n\nIf we tuned on the test set, we’d essentially be “leaking” information, and performance estimates would be biased upward.\n\nProper splitting ensures that the test set remains untouched until the very end.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#detects-data-leakage",
    "href": "week2_3.html#detects-data-leakage",
    "title": "Data Splitting",
    "section": "Detects Data Leakage",
    "text": "Detects Data Leakage\n\nSometimes information from the future or from labels sneaks into the features.\n\nIf this happens, the model might look perfect on the training set but fail on a clean split.\n\nData splitting may not actually help data leakage though. When might it help and when might it not?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#provides-a-fair-benchmark",
    "href": "week2_3.html#provides-a-fair-benchmark",
    "title": "Data Splitting",
    "section": "Provides a Fair Benchmark",
    "text": "Provides a Fair Benchmark\n\nIn research and industry, different models need to be compared on a common, untouched test set.\n\nWithout splitting, comparisons aren’t meaningful, since each model might overfit differently.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_5a.html",
    "href": "week2_5a.html",
    "title": "Overview",
    "section": "",
    "text": "Feature engineering in machine learning typically describes the process of creating, transforming, or selecting variables (features) from raw data to improve a model’s performance.\nFor example, one aspect of feature engineering is feature creation. This often means transforming raw data into meaningful inputs that better capture the underlying patterns the model needs to learn. For example, suppose we are trying to predict medicine adherence (whether or not someone takes their medicine). Our raw data may contain timestamps of when someone actually takes their medicine. We might use this timestamp to create new features for our model. For example, we might add features representing day of the week, holiday or weekend to provide additional context to the model.\nA number of domains fall under feature engineering:\nData leakage occurs when information from outside the training data set is used to create the model.\nData leakage often occurs during feature engineering.\nTo minimize data leaking, we will often want to do our feature engineering during the resampling, or data splitting, procedure we are using. To visualize this take a look at the graphic below from Boehmke and Greenwell (2019) where the pre-processing, or data engineering tasks, occur during each iteration. Keep this in mind as we introduce each feature engineering task.",
    "crumbs": [
      "Feature Engineering",
      "Overview"
    ]
  },
  {
    "objectID": "week2_5a.html#feature-engineering-and-data-leakage",
    "href": "week2_5a.html#feature-engineering-and-data-leakage",
    "title": "Overview",
    "section": "Feature Engineering and Data Leakage",
    "text": "Feature Engineering and Data Leakage",
    "crumbs": [
      "Feature Engineering",
      "Overview"
    ]
  },
  {
    "objectID": "week2_5c.html",
    "href": "week2_5c.html",
    "title": "Missing Data",
    "section": "",
    "text": "Dealing with missing data in a consistent manner is one of the most important aspects of feature engineering.\nTo understand modern approaches to handle missing data it is critical to have a basic understanding of missing data mechanisms, and the methodological approaches used for addressing each mechanism.\nMuch attention is paid to the mechanisms producing missing data in the statistics and applied sciences literature.\nKnowing how the missing data came about is critical for knowing how to handle it in a subsequent analysis.\nAlthough attention to missing data mechanisms has historically not been a primary focus in the ML literature, this has been changing.\nA common framework for understanding missing data mechanisms was described by Rubin (1976). Here we will briefly describe these mechanisms in the context of our current data example.\nIn MCAR, the probability of a value being missing is unrelated to the value itself or any other observed or unobserved variable. This is a purely random and unsystematic process.\nImagine a cat opening up your dataset in Excel and walking across the keyboard, randomly deleting different cells.\nDefinition: Missingness in self-reported happiness, for example, is unrelated to the respondent’s true happiness level or any other variables.\nExample: Let’s say there is a glitch in the online GSS survey and for some respondent’s questions are randomly skipped. This means the probability of missingness is purely random, and it does not depend on other variables (e.g. income, age, or happiness levels).\nImplication: Dropping these cases (listwise deletion) or using simple imputation will not bias the results, although efficiency is reduced.\nIn MAR, the probability of a value being missing is systematically related to other observed variables in the dataset, but not to the unobserved value itself.\nDefinition: Missingness in happiness depends on other observed variables but not directly on happiness itself.\nExample: Suppose in the GSS, individuals with higher incomes were less likely to answer the happiness question. Missingness on happiness is explained by income, which is observed. Conditional on income (having income in our model as a predictor), the probability of missingness does not depend how happy one is.\nImplication: Methods like multiple imputation (MICE), missForest, or regression-based imputation can use income (and other observed covariates like marital status, education) to predict and impute missing happiness values without bias.\nDefinition: Missingness in happiness depends on the unobserved happiness score itself.\nExample: Respondents who are very unhappy may avoid answering the happiness question because it feels too personal, while those who are extremely happy may skip it because they consider it obvious. Missingness is directly tied to the unreported happiness level.\nImplication: Standard imputation methods will be biased. Handling MNAR requires explicitly modeling the missingness mechanism (e.g., selection models, pattern-mixture models)\nOften a first step in handling missing data involves recoding missing values as NA. Writing bespoke code to handle the different types of missing data one might encounter is tedious and unnecessary.\nThe naniar (Tierney and Cook 2023) package in R contains many convenience functions for managing missing data in R. Here we demonstrate some of that functionality.\nNow that we have a dataset with missing values we can use naniar to recode these values to NA. In our current data example this is already done, but this code might be useful for other projects where you import data\nlibrary(naniar)\ngss_na_codes &lt;- c(-99, -999, \"NA\")\n  \ndata &lt;- naniar::replace_with_na_all(\n  data, condition = ~.x %in% gss_na_codes\n)\nSee the naniar vignette on recoding NA values for more detailed information on the package functionality.\nBelow is a small dataset looking at predictors of happiness. Some values are missing.\nIn small groups please speculate on why each value might be missing and which type of missing data mechanism it represents: MCAR, MAR, or MNAR.\n# Columns: id, happiness (target), age (feature), income (feature), education (feature)\n# NA indicates missing values\n\nexample_data &lt;- data.frame(\n  id = 1:15,\n  happiness = c(NA, 8, 5, NA, 6, 9, 4, 7, NA, 5, 6, 8, 7, NA, 4),\n  age = c(25, NA, 30, 40, 22, 35, NA, 29, 31, 28, 34, NA, 27, 33, 26),\n  income = c(50000, 55000, 6500, 70000, NA, NA, NA, 62000, NA, 45000, 52000, NA, 58000, NA,61000),\n  education = c(\"Bachelor\",\"Bachelor\",\"Master\",\"Master\",\"HighSchool\",\"Bachelor\",\"HighSchool\",\"Master\",\"Bachelor\",\"HighSchool\",\"Master\",\"Master\",\"Bachelor\",\"HighSchool\",\"Bachelor\")\n)\n\nexample_data\n\n   id happiness age income  education\n1   1        NA  25  50000   Bachelor\n2   2         8  NA  55000   Bachelor\n3   3         5  30   6500     Master\n4   4        NA  40  70000     Master\n5   5         6  22     NA HighSchool\n6   6         9  35     NA   Bachelor\n7   7         4  NA     NA HighSchool\n8   8         7  29  62000     Master\n9   9        NA  31     NA   Bachelor\n10 10         5  28  45000 HighSchool\n11 11         6  34  52000     Master\n12 12         8  NA     NA     Master\n13 13         7  27  58000   Bachelor\n14 14        NA  33     NA HighSchool\n15 15         4  26  61000   Bachelor\nThere are a number of ways to handle missing data. Below I will discuss some of the most common ways of addressing missing data.\nListwise deletion (also called complete-case analysis) is one of the simplest methods for handling missing data in a dataset.\nWhen performing listwise deletion we remove any row that has one or more missing values across any variable used in the analysis.\nAfter deletion, only rows that are complete for all variables remain.\nFor example, our example GSS data has 3,309 rows before we address the missing data. If we only kept rows that contained no missing data we would have 2,780 observations. You can perform listwise deletion on your data using the complete.cases() function as demonstrated below. Then you can visualize the missing data to ensure there is no missingness on the new dataset.\n# nrow(data) # 3,3309 rows\n\ndata_cc &lt;- data[complete.cases(data),]\nnrow(data_cc)\n\n[1] 2746\n\nnaniar::vis_miss(data_cc)\nNow, listwise deletion should really only be used if data is missing completely at random (MCAR). In this case it can still provide unbiased results, although they can be less efficient (reduced power and more uncertainty).\nImputation is the process of filling in missing values in a dataset with estimated or predicted values so that you can perform analyses without dropping incomplete cases.\nUnlike listwise deletion, imputation retains all observations, reducing data loss. Imputation can be simple (deterministic, like a mean) or more involved (stochastic, model-based).\nA simple approach to imputing missing values for a feature is to compute descriptive statistics such as the mean, median, or mode (for categorical) and use that value to replace the NA values.\nAlthough computationally efficient, this approach does not consider any other attributes for a given observation when imputing.\nThe tidymodels (Kuhn and Wickham 2020) R package has a number of useful functions for machine learning. Here we use the package to perform mean imputation on our dataset.\nlibrary(tidymodels)\n\n# create a recipe\nrec &lt;- recipe(happy ~ ., data = data) %&gt;%\n  step_impute_mean(all_numeric_predictors())  %&gt;% \n  step_impute_mode(all_factor_predictors())\n  # apply mean imputation to predictors\n\n# prep the recipe (estimates imputation values)\nrec_prep &lt;- prep(rec)\n\n# check the imputed values\ndata_mi &lt;- bake(rec_prep, new_data = NULL)\n\nnaniar::vis_miss(data_mi)\nAnother popular method for performing imputation is k-nearest neighbor.\nNow, instead of just filling in a missing value with a simple number like the mean, KNN looks for other respondents who are most similar (the “nearest neighbors”) and uses their information to fill in the blank.\nHow KNN Imputation Works\nBetter than Mean Imputation?\nThink of guessing someone’s favorite pizza topping:\nlibrary(tidymodels)\n\n# create a recipe\nrec &lt;- recipe(happy ~ ., data = data) %&gt;%\n  step_impute_knn(all_predictors())   \n  # apply mean imputation to predictors\n\n# prep the recipe (estimates imputation values)\nrec_prep &lt;- prep(rec)\n\n# check the imputed values\ndata_knn &lt;- bake(rec_prep, new_data = NULL)\n\nnaniar::vis_miss(data_knn)\nTree-based imputation methods use decision trees (or random forests) to predict missing values based on the other variables in the dataset. Similar to KNN methods, tree-based methods make a tailored prediction using patterns in the data.\nTree-based methods are especially nice for imputation as they handle non-linearities and interactions, and can accomodate mixed data types, like continuous and categorical variables.\nHow Tree-Based Imputation Works\nBetter than Mean Imputation?\nThink of tree-based imputation like asking:\nThis is smarter than saying “everyone makes the same average income” (mean imputation) or even “let’s just look at your 3 closest neighbors” (KNN).",
    "crumbs": [
      "Feature Engineering",
      "Missing Data"
    ]
  },
  {
    "objectID": "week2_5c.html#missing-data-mechanisms",
    "href": "week2_5c.html#missing-data-mechanisms",
    "title": "Missing Data",
    "section": "Missing Data Mechanisms",
    "text": "Missing Data Mechanisms",
    "crumbs": [
      "Feature Engineering",
      "Missing Data"
    ]
  },
  {
    "objectID": "week2_5c.html#handling-missing-data-in-r",
    "href": "week2_5c.html#handling-missing-data-in-r",
    "title": "Missing Data",
    "section": "Handling Missing Data in R",
    "text": "Handling Missing Data in R",
    "crumbs": [
      "Feature Engineering",
      "Missing Data"
    ]
  },
  {
    "objectID": "week2_5c.html#missing-data-activity",
    "href": "week2_5c.html#missing-data-activity",
    "title": "Missing Data",
    "section": "Missing Data Activity",
    "text": "Missing Data Activity",
    "crumbs": [
      "Feature Engineering",
      "Missing Data"
    ]
  },
  {
    "objectID": "week2_5c.html#handling-missing-data",
    "href": "week2_5c.html#handling-missing-data",
    "title": "Missing Data",
    "section": "Handling Missing Data",
    "text": "Handling Missing Data",
    "crumbs": [
      "Feature Engineering",
      "Missing Data"
    ]
  },
  {
    "objectID": "week2_5e.html",
    "href": "week2_5e.html",
    "title": "Numeric Feature Engineering",
    "section": "",
    "text": "Feature engineering on continuous or numeric features involves transforming the raw data to make it more useful for modeling. Common motivations include capturing nonlinear relationships (e.g., log or polynomial transforms), improving distributional properties (reducing skew or outlier impact), and putting features on comparable scales through standardization.\nIn some cases, continuous variables are binned into categories or combined into interactions to highlight patterns. Overall, these transformations help models detect structure in the data more effectively and improve both performance and interpretability.\nParametric models with distributional assumptions (e.g., GLMs, and some regularized models) can benefit from minimizing the skewness of numeric features. One popular transformation is the Yeo-Johnson transformation.\nThe Yeo-Johnsontransformation is a statistical technique used to stabilize variance and make data more normally distributed, similar to the Box-Cox transformation but more flexible. Unlike Box-Cox, it can handle both positive and negative values, making it useful for real-world data that span zero.\nrecipe(happy ~ ., data = data) %&gt;%\n  step_YeoJohnson(all_numeric())\nWe will often want to standardize variables prior to our model fitting to put them on a common scale, typically with mean of zero and a standard deviation of one.\nThis prevents features with larger numeric ranges (e.g., income in dollars vs. age in years) from dominating distance-based methods (like k-NN, SVMs, or clustering). It can also help with optimization problems, may improve convergence and can help ensure that regularization penalties (like in ridge or lasso regression) are applied fairly across predictors.\ndata %&gt;%\n  step_center(all_numeric(), -all_outcomes()) %&gt;%\n  step_scale(all_numeric(), -all_outcomes())",
    "crumbs": [
      "Feature Engineering",
      "Numeric Feature Engineering"
    ]
  },
  {
    "objectID": "week2_5g.html",
    "href": "week2_5g.html",
    "title": "Workflow",
    "section": "",
    "text": "Workflow in R\n\nTo illustrate how this process works together in R code, let’s do a simple analysis using our example data, starting from scratch.\nThe steps below simply re-downloads our data, selects the variables we want to keep, cleans up the missing data codes and does some basic relabeling.\n\nlibrary(gssr)\n\ngss24 &lt;- gss_get_yr(2024)\n\ntarget &lt;- \"happy\"\n\nfeatures &lt;- c(\n  \"age\",     # age of participant\n  \"sex\",     # sex of participant\n  \"race\",    # race of participant\n  \"educ\",    # highest education completed by participant\n  \"income\",  # income of participant\n  \"childs\",  # number of children participant has\n  \"wrkstat\", # work force status\n  \"marital\", # marital status\n  \"born\",    # whether or not participant was born in USA\n  \"partyid\", # political party of participant\n  \"adults\",  # num of fam members 18 or older\n  \"earnrs\"   # number of earners in family  \n)\n\ndata &lt;- gss24[,c(target, features)]\n\n\n# define which varibles are categorical and continuous\ncat_vars &lt;- c(\"sex\",\"race\", \"educ\", \"wrkstat\", \"marital\", \"born\", \"partyid\")\ncon_vars &lt;- c(\"age\",\"income\",\"childs\",\"adults\",\"earnrs\",\"happy\")\n\ncapwords &lt;- function(x, strict = FALSE) {\n    cap &lt;- function(x) paste(toupper(substring(x, 1, 1)),\n                  {x &lt;- substring(x, 2); if(strict) tolower(x) else x},\n                             sep = \"\", collapse = \" \" )\n    sapply(strsplit(x, split = \" \"), cap, USE.NAMES = !is.null(names(x)))\n}\n\ndata &lt;- data |&gt;\n  mutate(\n    # Convert all missing to NA\n    across(everything(), haven::zap_missing), \n    # Make all categorical variables factors and relabel nicely\n    across(all_of(cat_vars), forcats::as_factor),\n    across(all_of(con_vars), as.numeric),\n    across(all_of(cat_vars), \\(x) forcats::fct_relabel(x, capwords, strict = TRUE))\n  )\n\ndata &lt;- data[!is.na(data$happy),]\n\nWe can now separate our data into a training and test set.\n\n# install.packages(\"rsample\")\nlibrary(rsample)\n# Stratified sampling with the rsample package\nset.seed(123)\nsplit &lt;- initial_split(data, prop = 0.7, strata = \"happy\")\ndata_train  &lt;- training(split)\ndata_test   &lt;- testing(split)\n\nNow, we will formally introduce the recipes package.\nThe recipes package is part of the tidymodels framework and is designed for feature engineering. In machine learning, raw data usually isn’t ready to be used directly in a model—you might need to do all the things we discussed in these notes.\nInstead of doing all these steps manually, recipes lets us define a sequence of preprocessing steps (called a recipe) that can be applied consistently to training and test data.\nA recipe typically goes through three main stages:\n\n1. Define the recipe\n\nWrite down the blueprint of preprocessing steps you want to apply.\n\nExample: “Impute missing values, standardize numeric predictors, and one-hot encode categorical variables.”\n\nAt this stage, the recipe only records what to do, not how to do it.\n\n\n\n2. Prep\n\nUse the training data to learn any parameters needed for preprocessing.\n\nExample: Calculate means and standard deviations for standardization, determine category levels for encoding, or find values to impute.\n\nAfter prepping, the recipe is ready to be applied consistently to new data.\n\n\n\n3. Bake\n\nApply the recipe to a dataset (training, validation, or test).\n\nThis step actually transforms the data using the information learned during the prep stage.\n\nThe output is a processed dataset that can be used directly in a machine learning model.\n\nFor example, the following defines happy as the target variable and then uses all the remaining columns as features based on data_train. We then:\n\nRemove near-zero variance features that are categorical (aka nominal).\nImpute missing data\nDummy encode our categorical features.\nCenter and scale (i.e., standardize) all numeric features\n\n\nblueprint &lt;- recipe(happy ~ ., data = data_train) %&gt;%\n  step_nzv(all_nominal())  %&gt;%\n  step_impute_bag(all_predictors()) %&gt;%\n  step_dummy(all_factor_predictors(), one_hot = FALSE) %&gt;%\n  step_center(all_numeric(), -all_outcomes()) %&gt;%\n  step_scale(all_numeric(), -all_outcomes()) \n\nblueprint\n\n# these are example steps you don't need to run for any reason other\n# than troubleshooting\n\n# prepare &lt;- prep(blueprint, training = data_train)\n# prepare\n\n# baked_train &lt;- bake(prepare, new_data = data_train)\n# baked_test  &lt;- bake(prepare, new_data = data_test)\n# baked_train\n\nNext, we can fit a model using the caret pacakge, using our blueprint as the first argument and then caret takes care of the rest.\n\nlibrary(caret)\n\n# Specify cross-validation plan\ncv &lt;- trainControl(\n  method = \"cv\", # k-folds cross-validation\", \n  number = 10  # 10 folds\n)\n\n# Create grid of hyperparameter values\nhyper_grid &lt;- expand.grid(k = seq(2, 25, by = 1))\n\n# Tune a knn model using grid search\nknn_fit &lt;- train(\n  blueprint, \n  data = data_train, \n  method = \"knn\", \n  trControl = cv, \n  tuneGrid = hyper_grid,\n  metric = \"RMSE\"\n)\n\nknn_fit\n\nk-Nearest Neighbors \n\n2295 samples\n  12 predictor\n\nRecipe steps: nzv, impute_bag, dummy, center, scale \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 2066, 2066, 2066, 2064, 2066, 2065, ... \nResampling results across tuning parameters:\n\n  k   RMSE       Rsquared    MAE      \n   2  0.7418063  0.03097989  0.5719579\n   3  0.7052823  0.03088297  0.5546278\n   4  0.6847273  0.03149323  0.5390046\n   5  0.6771450  0.02897065  0.5329401\n   6  0.6684619  0.03192580  0.5233915\n   7  0.6594812  0.03810636  0.5159846\n   8  0.6550279  0.03945877  0.5112249\n   9  0.6529972  0.03903932  0.5081061\n  10  0.6514761  0.03798039  0.5039809\n  11  0.6495920  0.03878046  0.5006377\n  12  0.6464377  0.04135895  0.4983083\n  13  0.6449913  0.04153260  0.4959262\n  14  0.6457968  0.03833580  0.4952235\n  15  0.6455656  0.03708429  0.4940333\n  16  0.6445642  0.03761307  0.4920747\n  17  0.6428354  0.03999850  0.4895426\n  18  0.6415509  0.04156203  0.4877497\n  19  0.6410869  0.04199466  0.4861644\n  20  0.6408229  0.04204207  0.4847307\n  21  0.6409284  0.04098073  0.4844582\n  22  0.6412205  0.04035344  0.4837011\n  23  0.6408491  0.04051976  0.4822852\n  24  0.6410824  0.03919338  0.4819184\n  25  0.6411540  0.03858298  0.4816823\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was k = 20.\n\nggplot(knn_fit)",
    "crumbs": [
      "Feature Engineering",
      "Workflow"
    ]
  },
  {
    "objectID": "week3_2.html",
    "href": "week3_2.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Multiple linear regression is a statistical method used to model the relationship between two variables. As such, multiple regression extends the simple linear regression framework to include more than one predictor variable:\n- more than one feature (also called the predictors or independent variables),\n- one target (also called the outcome or dependent variable).\nMultiple linear regression allows us to model the relationship between a single outcome variable \\(Y_i\\) and multiple predictors \\(X_{1i}, X_{2i}, \\dots, X_{pi}\\) for each observation \\(i = 1, \\dots, n\\).\nAgain let’s use the Ames dataset to illustrate multiple linear regression. We will look to predict the sale price of a home (Sale_Price) using two predictors: above ground living area (Gr_Liv_Area) and the year the house was built (Year_Built).\n# Install and load package\n# install.packages(\"AmesHousing\")\nlibrary(AmesHousing)\n\n# Load data\names &lt;- make_ames()\nThe general form of the multiple linear regression model is:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_p X_{pi} + \\varepsilon_i\n\\]\nwhere:\n- \\(Y_i\\): the dependent (outcome) variable for observation \\(i\\).\n- \\(X_{ji}\\): the value of predictor \\(j\\) for observation \\(i\\).\n- \\(\\beta_0\\): the intercept (expected value of \\(Y_i\\) when all \\(X_{ji} = 0\\)).\n- \\(\\beta_j\\): the regression coefficient for predictor \\(X_{ji}\\), representing the expected change in \\(Y_i\\) for a one-unit increase in \\(X_{ji}\\), holding all other predictors constant.\n- \\(\\varepsilon_i\\): the error term for observation \\(i\\), capturing variation in \\(Y_i\\) not explained by the predictors.\nThe coefficients \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) are estimated using ordinary least squares (OLS).\nOLS chooses coefficient values that minimize the residual sum of square:\n\\[\n\\text{RSS} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n\\]\nwhere:\n- \\(y_i\\) is the observed outcome for observation \\(i\\),\n- \\(\\hat{y}_i\\) is the predicted value of \\(Y_i\\) from the model.\nIn R, we fit a multiple regression model with the lm() function:\nfit_mlr &lt;- lm(\n  Sale_Price ~ Gr_Liv_Area + Year_Built, \n  data = ames\n)\nsummary(fit_mlr)\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-458172  -26758   -2236   18514  306986 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.106e+06  5.734e+04  -36.74   &lt;2e-16 ***\nGr_Liv_Area  9.597e+01  1.758e+00   54.60   &lt;2e-16 ***\nYear_Built   1.087e+03  2.938e+01   37.01   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 46660 on 2927 degrees of freedom\nMultiple R-squared:  0.6591,    Adjusted R-squared:  0.6588 \nF-statistic:  2829 on 2 and 2927 DF,  p-value: &lt; 2.2e-16\nWe interpret the intercept, \\(\\beta_0=-2,106,000\\), as the estimated mean sale price when both Gr_Liv_Area and Year_Built are 0. While this interpretation may not be meaningful in a real-world context (since a house with 0 living area and built in year 0 is not realistic), it serves as a baseline for our model.\nWe interpret the slope coefficient for Gr_Liv_Area, \\(\\beta_1=96\\), as the estimated change in mean sale price for each additional square foot of living area, holding Year_Built constant. Specifically, for each one-square-foot increase in Gr_Liv_Area, the mean sale price is estimated to increase by \\(96\\) dollars, holding Year_Built constant.\nWe interpret the slope coefficient for Year_Built, \\(\\beta_2=1,087\\), as the estimated change in mean sale price for each additional year the house was built, holding Gr_Liv_Area constant. Specifically, for each one-year increase in Year_Built, the mean sale price is estimated to increase by roughly a thousand dollars, holding Gr_Liv_Area constant.\nIn multiple linear regression, an interaction occurs when the effect of one predictor on the response variable depends on the level of another predictor.\nIn other words, the impact of one variable is modified or moderated by another variable.\nFor two predictors, \\(X_1\\) and \\(X_2\\), an interaction term can be included as:\n\\[\nY_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\beta_3 (X_{1i} \\cdot X_{2i}) + \\varepsilon_i\n\\]\nwhere:\n- \\(\\beta_3\\) captures the interaction effect between \\(X_1\\) and \\(X_2\\).\n- The term \\((X_{1i} \\cdot X_{2i})\\) is the product of the two predictors.\n- If \\(\\beta_3 \\neq 0\\), the effect of \\(X_1\\) on \\(Y\\) changes depending on \\(X_2\\), and vice versa.\nInteractions allow the model to represent non-additive relationships between predictors and the response. Without interactions, the model assumes each predictor contributes independently, which may oversimplify reality.\nfit_interaction &lt;- lm(\n  Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, \n  data = ames\n)\nsummary(fit_interaction)\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area:Year_Built, \n    data = ames)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-541888  -25275   -1849   17227  292527 \n\nCoefficients:\n                         Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            -3.252e+05  1.757e+05  -1.851   0.0643 .  \nGr_Liv_Area            -1.033e+03  1.055e+02  -9.792   &lt;2e-16 ***\nYear_Built              1.825e+02  8.931e+01   2.043   0.0411 *  \nGr_Liv_Area:Year_Built  5.727e-01  5.351e-02  10.703   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45780 on 2926 degrees of freedom\nMultiple R-squared:  0.6719,    Adjusted R-squared:  0.6716 \nF-statistic:  1998 on 3 and 2926 DF,  p-value: &lt; 2.2e-16\nWe could interpret the interaction coefficient, \\(\\beta_3=0.057\\), as follows: For each additional square foot of living area, the effect of Year_Built on Sale_Price increases by about six cents, holding Gr_Liv_Area constant. Conversely, for each additional year the house was built, the effect of Gr_Liv_Area on Sale_Price increases by about six cents, holding Year_Built constant.\nTo assess how accurate our models are we will go back to our cross-validation framework. First, let’s split our data into test and train sets.\nset.seed(123)\nsplit &lt;- initial_split(\n  ames, \n  prop = 0.7, \n  strata = \"Sale_Price\"\n)\names_train  &lt;- training(split)\names_test   &lt;- testing(split)\nNow, let’s consider three different models we may want to compare: 1. Model 1: A simple linear regression using only Gr_Liv_Area as a predictor.\n2. Model 2: A multiple linear regression using Gr_Liv_Area and Year_Built as predictors.\n3. Model 3: A full model using all available predictors.\nlibrary(caret)\n# Train model using 10-fold cross-validation\nset.seed(123)  # for reproducibility\n(cv_model1 &lt;- train(\n  form = Sale_Price ~ Gr_Liv_Area, \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nLinear Regression \n\n2049 samples\n   1 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1843, 1844, 1844, 1844, 1844, 1844, ... \nResampling results:\n\n  RMSE      Rsquared  MAE     \n  56644.76  0.510273  38851.99\n\nTuning parameter 'intercept' was held constant at a value of TRUE\nThe resulting cross-validated RMSE is $56,644.76 (this is the average RMSE across the 10 CV folds). How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about $56,644.76 off from the actual sale price.\n# model 2 CV\nset.seed(123)\n(cv_model2 &lt;- train(\n  Sale_Price ~ Gr_Liv_Area + Year_Built, \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nLinear Regression \n\n2049 samples\n   2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1843, 1844, 1844, 1844, 1844, 1844, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  46865.68  0.6631008  31695.48\n\nTuning parameter 'intercept' was held constant at a value of TRUE\nThe resulting cross-validated RMSE is now a bit lower at $46,865.68 (this is the average RMSE across the 10 CV folds). How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about $46,865.68 off from the actual sale price.\n# model 3 CV\nset.seed(123)\n(cv_model3 &lt;- train(\n  Sale_Price ~ Gr_Liv_Area + Year_Built + Gr_Liv_Area*Year_Built, \n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nLinear Regression \n\n2049 samples\n   2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1843, 1844, 1844, 1844, 1844, 1844, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  46523.38  0.6691386  30746.99\n\nTuning parameter 'intercept' was held constant at a value of TRUE\nAgain we see a (very) small improvement when adding all the available features. The resulting cross-validated RMSE is now lower at $46,523.38 (this is the average RMSE across the 10 CV folds). How should we interpret this? When applied to unseen data, the predictions this model makes are, on average, about $46,523.38 off from the actual sale price.\n# Extract out of sample performance measures\nsummary(resamples(list(\n  model1 = cv_model1, \n  model2 = cv_model2, \n  model3 = cv_model3\n)))\n\n\nCall:\nsummary.resamples(object = resamples(list(model1 = cv_model1, model2\n = cv_model2, model3 = cv_model3)))\n\nModels: model1, model2, model3 \nNumber of resamples: 10 \n\nMAE \n           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nmodel1 34076.73 37656.23 39785.18 38851.99 40200.92 42058.68    0\nmodel2 29227.14 30885.17 32003.59 31695.48 32710.41 33942.26    0\nmodel3 27811.34 30036.68 31084.19 30746.99 31639.78 33142.02    0\n\nRMSE \n           Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's\nmodel1 45604.65 55896.58 57000.74 56644.76 59544.08 66198.59    0\nmodel2 37174.26 42650.00 46869.84 46865.68 51155.14 55780.47    0\nmodel3 35825.06 41986.84 46598.42 46523.38 50897.45 58150.97    0\n\nRsquared \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.4230788 0.4621034 0.5090642 0.5102730 0.5681246 0.5996400    0\nmodel2 0.5829425 0.6075293 0.6865871 0.6631008 0.6976664 0.7254572    0\nmodel3 0.5653745 0.6087785 0.6898757 0.6691386 0.7134740 0.7453157    0\nAs discussed earlier, linear regression remains one of the most widely used modeling techniques, primarily because the regression coefficients are straightforward to interpret. Each coefficient quantifies the expected change in the response variable for a one-unit change in a predictor, holding all other predictors constant.\nHowever, linear regression relies on several strong assumptions, and these assumptions are often violated as we increase the number of predictors in the model. Violations can lead to biased estimates, misleading interpretations of coefficients, and inaccurate predictions.\nLinear regression assumes a linear relationship between each predictor and the response variable. This means that the change in the response associated with a one-unit change in a predictor is constant across all levels of that predictor.\nIn practice, many relationships are non-linear. Fortunately, we can often address this issue by applying transformations to the response and/or predictors to make the relationship approximately linear.\nFor example, the relationship between home sale price and the year the home was built is shown:\np1 &lt;- ggplot(ames_train, aes(Year_Built, Sale_Price)) + \n  geom_point(size = 1, alpha = .4) +\n  geom_smooth(se = FALSE) +\n  scale_y_continuous(\"Sale price\", labels = scales::dollar) +\n  xlab(\"Year built\") +\n  ggtitle(paste(\"Non-transformed variables with a\\n\",\n                \"non-linear relationship.\"))\n\np2 &lt;- ggplot(ames_train, aes(Year_Built, Sale_Price)) + \n  geom_point(size = 1, alpha = .4) + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_y_log10(\"Sale price\", labels = scales::dollar, \n                breaks = seq(0, 400000, by = 100000)) +\n  xlab(\"Year built\") +\n  ggtitle(paste(\"Transforming variables can provide a\\n\",\n                \"near-linear relationship.\"))\n\ngridExtra::grid.arrange(p1, p2, nrow = 1)\nHeteroskedasticity occurs when the variance of the errors is not constant across all levels of the predictors. This violates the homoscedasticity assumption of linear regression.\nLooking at the plot of residuals versus fitted values for Model 1, we can see a funnel shape, indicating that the variance of the residuals increases with the fitted values. This suggests heteroskedasticity, which can lead to inefficient estimates and invalid inference.\ndf1 &lt;- broom::augment(cv_model1$finalModel, data = ames_train)\n\nggplot(df1, aes(.fitted, .std.resid)) + \n  geom_point(size = 1, alpha = .4) +\n  xlab(\"Predicted values\") +\n  ylab(\"Residuals\") +\n  ggtitle(\"Model 1\", subtitle = \"Sale_Price ~ Gr_Liv_Area\")\nTo address heteroskedasticity, we can consider transforming the response variable (e.g., using a log transformation) or using robust standard errors that adjust for non-constant variance.\nWe might also be concerned with checking the residuals for autocorrelation, especially if our data has a time component. Autocorrelation occurs when the residuals are correlated with each other, violating the independence assumption. This can lead to underestimated standard errors and overly optimistic p-values. Since our current data does not have a time component, we will not explore this further here.\nThe vip() function from the vip package in R (Variable Importance Plots) is commonly used to visualize the relative importance of predictors in a model.\n# Load package\nlibrary(vip)\nThe vip() function takes any fitted model object, such as lm() for linear regression, glm(), random forests, or xgboost.\nFor linear regression models, vip() typically uses the absolute value of the t-statistics of the coefficients as a measure of importance. Predictors with larger absolute t-values are considered more influential on the response.\n# model 4 CV\nset.seed(1234)\n(cv_model4 &lt;- train(\n  Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Overall_Qual + Total_Bsmt_SF + Garage_Area + Full_Bath + TotRms_AbvGrd + Fireplaces + Garage_Cars + Wood_Deck_SF + Open_Porch_SF + Year_Remod_Add ,\n  data = ames_train, \n  method = \"lm\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nLinear Regression \n\n2049 samples\n  13 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 1844, 1845, 1843, 1844, 1844, 1844, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  32145.44  0.8392331  19868.31\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\nvip(cv_model4, num_features = 10, method = \"model\")",
    "crumbs": [
      "Regression Methods",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#example-data",
    "href": "week3_2.html#example-data",
    "title": "Multiple Linear Regression",
    "section": "Example Data",
    "text": "Example Data",
    "crumbs": [
      "Regression Methods",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#the-model",
    "href": "week3_2.html#the-model",
    "title": "Multiple Linear Regression",
    "section": "The Model",
    "text": "The Model",
    "crumbs": [
      "Regression Methods",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#estimating-the-model",
    "href": "week3_2.html#estimating-the-model",
    "title": "Multiple Linear Regression",
    "section": "Estimating the Model",
    "text": "Estimating the Model",
    "crumbs": [
      "Regression Methods",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#interpreting-coefficients",
    "href": "week3_2.html#interpreting-coefficients",
    "title": "Multiple Linear Regression",
    "section": "Interpreting Coefficients",
    "text": "Interpreting Coefficients",
    "crumbs": [
      "Regression Methods",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_2.html#implementation-in-r",
    "href": "week3_2.html#implementation-in-r",
    "title": "Multiple Linear Regression",
    "section": "Implementation in R",
    "text": "Implementation in R",
    "crumbs": [
      "Regression Methods",
      "Multiple Linear Regression"
    ]
  },
  {
    "objectID": "week3_4.html",
    "href": "week3_4.html",
    "title": "Multiple Logistic Regression",
    "section": "",
    "text": "Multiple logistic regression extends the simple logistic regression framework to include more than one predictor variable:\n- more than one feature (also called the predictors or independent variables),\n- one target (also called the outcome or dependent variable).\nMultiple logistic regression allows us to model the relationship between a single outcome variable \\(Y_i\\) and multiple predictors \\(X_{1i}, X_{2i}, \\dots, X_{pi}\\) for each observation \\(i = 1, \\dots, n\\).\nAgain let’s use dropout data.\ndropout &lt;- read.csv(\"data/dropout.csv\", sep = \";\")\ndropout$Dropout &lt;- ifelse(dropout$Target == \"Dropout\", 1, 0)\ndropout$Age_star &lt;- scale(dropout$Age.at.enrollment, center = TRUE, scale = FALSE)\nThe general form of the multiple logistic regression model is:\n\\[\np(Y_i) = \\frac{e^{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_p X_{pi}}}{1 + e^{\\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\dots + \\beta_p X_{pi}}}\n\\]\nThe coefficients \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) are typically estimated by maximum likelihood estimation (MLE), since least squares is not appropriate for binary outcomes.\nIn R, we fit a multiple regression model with the glm() function:\nfit_mult &lt;- glm(\n  Dropout ~ Age_star + Debtor, \n  family = \"binomial\", \n  data = dropout\n)\n\nsummary(fit_mult)\n\n\nCall:\nglm(formula = Dropout ~ Age_star + Debtor, family = \"binomial\", \n    data = dropout)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.948869   0.036623  -25.91   &lt;2e-16 ***\nAge_star     0.065876   0.004414   14.93   &lt;2e-16 ***\nDebtor       1.347617   0.101097   13.33   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 5554.5  on 4423  degrees of freedom\nResidual deviance: 5100.5  on 4421  degrees of freedom\nAIC: 5106.5\n\nNumber of Fisher Scoring iterations: 4\nSo far we have considered two possibilities for interpreting logistic regression results:\nHowever, as we include more covariates in our model, interpretation becomes more difficult. We can only think about “holding other variable constant” in the log-odds and odds scale. For nonlinear model marginal effects provide us with an intuitive and easy to interpret method for understanding and communicating results.\nMarginal effects are partial derivatives of the regression equation with respect to each variable in the model for each unit in the data.\nPut differently, the marginal effect measures the association between a change in an explanatory variable and a change in the response. The marginal effect is the slope of the prediction function, measured at a specific value of the explanatory variable.\nIn linear models the effect of a given change in an independent variable is the same regardless of (1) the value of that variable at the start of its change, and (2) the level of the other variables in the model.\nIn nonlinear models the effect of a given change in an independent variable (1) depends on the values of other variables in the model, and (2) is no longer equal to the parameter itself.\nConsider a linear and nonlinear model for happiness as a function of personal spending and a dummy variable indicating whether someone is rich.\nA Few Observations\nFor the linear model: - Whether one is rich or poor does no impact the relationship between happiness and personal spending. - Differences in happiness levels between rich and poor are not dependent on the amount of money one spends.\nFrom the nonlinear model: - Whether one is rich or poor does impact the relationship between happiness and personal spending. - Differences in happiness levels between rich and poor are dependent on the amount of money one spends.\nTo look at marginal effects we will use the marginaleffects package.\n# install.packages(\"marginaleffects\")\nlibrary(marginaleffects)\nFor example, let’s look at the impact of Debtor on the probability of dropping out.\nmarginaleffects::plot_predictions(\n  fit_mult, \n  condition = c(\"Debtor\"), \n  conf.int = TRUE\n)\nWhat if we were interested in the relationship between Age and Debtor on the probability of dropping out.\nmarginaleffects::plot_predictions(\n  fit_mult, \n  condition = c(\"Age_star\",\"Debtor\")\n)\nThus, marginal effects provide a way to understand and communicate the results of nonlinear models.\nTo assess how accurate our models are we will go back to our cross-validation framework. First, let’s split our data into test and train sets.\nset.seed(123)\n\ndropout$Dropout &lt;- as.factor(dropout$Dropout)\ndropout$Age.at.enrollment &lt;- NULL\n\nsplit &lt;- initial_split(\n  dropout, \n  prop = 0.7, \n  strata = \"Dropout\"\n)\ndropout_train  &lt;- training(split)\ndropout_test   &lt;- testing(split)\nNow, let’s consider three different models we may want to compare:\nlibrary(caret)\n# Train model using 10-fold cross-validation\nset.seed(123)  # for reproducibility\n(cv_model1 &lt;- train(\n  form = Dropout ~ Age_star, \n  data = dropout_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nGeneralized Linear Model \n\n3096 samples\n   1 predictor\n   2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 2786, 2787, 2786, 2786, 2787, 2787, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.6824965  0.1098779\n# model 2 CV\nset.seed(123)\n(cv_model2 &lt;- train(\n  form = Dropout ~ Age_star + Debtor, \n  data = dropout_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nGeneralized Linear Model \n\n3096 samples\n   2 predictor\n   2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 2786, 2787, 2786, 2786, 2787, 2787, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7070554  0.2323849\n# model 3 CV\nset.seed(123)\n(cv_model3 &lt;- train(\n  form = Dropout ~ ., \n  data = dropout_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nGeneralized Linear Model \n\n3096 samples\n  37 predictor\n   2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 2786, 2787, 2786, 2786, 2787, 2787, ... \nResampling results:\n\n  Accuracy  Kappa\n  1         1\nWhy did we get warnings for our full model and seeminly perfect accuracy?\nThe warnings are likely due to perfect separation in the data, which can occur when a predictor variable perfectly predicts the outcome variable. This can lead to issues with model convergence and unreliable estimates.\ndropout_train$Target &lt;- NULL\n\n# model 3 CV\nset.seed(123)\n(cv_model3 &lt;- train(\n  form = Dropout ~ ., \n  data = dropout_train, \n  method = \"glm\",\n  family = \"binomial\",\n  trControl = trainControl(method = \"cv\", number = 10)\n))\n\nGeneralized Linear Model \n\n3096 samples\n  36 predictor\n   2 classes: '0', '1' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 2786, 2787, 2786, 2786, 2787, 2787, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.8769326  0.7053757\n# Extract out of sample performance measures\nsummary(resamples(list(\n  model1 = cv_model1, \n  model2 = cv_model2, \n  model3 = cv_model3\n)))\n\n\nCall:\nsummary.resamples(object = resamples(list(model1 = cv_model1, model2\n = cv_model2, model3 = cv_model3)))\n\nModels: model1, model2, model3 \nNumber of resamples: 10 \n\nAccuracy \n            Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.6548387 0.6771845 0.6860841 0.6824965 0.6910694 0.7022654    0\nmodel2 0.6903226 0.6956576 0.7032258 0.7070554 0.7170608 0.7378641    0\nmodel3 0.8381877 0.8673139 0.8774194 0.8769326 0.8894248 0.9223301    0\n\nKappa \n             Min.    1st Qu.    Median      Mean   3rd Qu.      Max. NA's\nmodel1 0.05504587 0.08128886 0.1071698 0.1098779 0.1256038 0.1807021    0\nmodel2 0.16730491 0.20248171 0.2328163 0.2323849 0.2637005 0.3031822    0\nmodel3 0.61400090 0.67115168 0.7080421 0.7053757 0.7336519 0.8187418    0\nWhen building a classification model, we need to assess how well it performs.\nDifferent metrics emphasize different aspects of performance, depending on the context (e.g., balanced vs. imbalanced data, cost of false positives vs. false negatives).\nBelow is an overview of common evaluation metrics.\nAll classification measures are based on the confusion matrix:\nWe can look at a confusion matrix in R using the confusionMatrix() function from the caret package.\n# Make predictions on the test set      \npred_class &lt;- predict(cv_model3, dropout_train)\n\n# create confusion matrix\nconfusionMatrix(\n  data = relevel(pred_class, ref = \"1\"), \n  reference = relevel(dropout_train$Dropout, ref = \"1\")\n)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction    1    0\n         1  732  105\n         0  262 1997\n                                          \n               Accuracy : 0.8815          \n                 95% CI : (0.8696, 0.8926)\n    No Information Rate : 0.6789          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7163          \n                                          \n Mcnemar's Test P-Value : 3.852e-16       \n                                          \n            Sensitivity : 0.7364          \n            Specificity : 0.9500          \n         Pos Pred Value : 0.8746          \n         Neg Pred Value : 0.8840          \n             Prevalence : 0.3211          \n         Detection Rate : 0.2364          \n   Detection Prevalence : 0.2703          \n      Balanced Accuracy : 0.8432          \n                                          \n       'Positive' Class : 1\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\n\\[\n\\text{Specificity} = \\frac{TN}{TN + FP}\n\\]\n\\[\n\\kappa = \\frac{P_o - P_e}{1 - P_e}\n\\]\nwhere:\n\\[\n\\text{MCC} = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\n\\]\nThe vip() function from the vip package in R (Variable Importance Plots) is commonly used to visualize the relative importance of predictors in a model.\n# Load package\nlibrary(vip)\nThe vip() function takes any fitted model object, such as lm() for linear regression, glm(), random forests, or xgboost.\nFor linear regression models, vip() typically uses the absolute value of the t-statistics of the coefficients as a measure of importance. Predictors with larger absolute t-values are considered more influential on the response.\nvip(cv_model3, num_features = 20, method = \"model\")",
    "crumbs": [
      "Regression Methods",
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "week3_4.html#example-data",
    "href": "week3_4.html#example-data",
    "title": "Multiple Logistic Regression",
    "section": "Example Data",
    "text": "Example Data",
    "crumbs": [
      "Regression Methods",
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "week3_4.html#the-model",
    "href": "week3_4.html#the-model",
    "title": "Multiple Logistic Regression",
    "section": "The Model",
    "text": "The Model",
    "crumbs": [
      "Regression Methods",
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "week3_4.html#estimating-the-model",
    "href": "week3_4.html#estimating-the-model",
    "title": "Multiple Logistic Regression",
    "section": "Estimating the Model",
    "text": "Estimating the Model",
    "crumbs": [
      "Regression Methods",
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "week3_4.html#implementation-in-r",
    "href": "week3_4.html#implementation-in-r",
    "title": "Multiple Logistic Regression",
    "section": "Implementation in R",
    "text": "Implementation in R",
    "crumbs": [
      "Regression Methods",
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "week3_4.html#interpretation-of-coefficients",
    "href": "week3_4.html#interpretation-of-coefficients",
    "title": "Multiple Logistic Regression",
    "section": "Interpretation of Coefficients",
    "text": "Interpretation of Coefficients",
    "crumbs": [
      "Regression Methods",
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "week3_4.html#outcome-measures-for-classification-tasks",
    "href": "week3_4.html#outcome-measures-for-classification-tasks",
    "title": "Multiple Logistic Regression",
    "section": "Outcome Measures for Classification Tasks",
    "text": "Outcome Measures for Classification Tasks",
    "crumbs": [
      "Regression Methods",
      "Multiple Logistic Regression"
    ]
  },
  {
    "objectID": "week5_1.html",
    "href": "week5_1.html",
    "title": "Decision Trees",
    "section": "",
    "text": "A decision tree is a predictive model that uses a tree-like structure to make decisions.\nIt splits data into smaller and smaller groups based on feature values, ultimately leading to a prediction at the “leaves” (the end points of the tree).\nThe name comes from the fact that the model looks like a flowchart: you start at the root (the top of the tree), follow branches based on yes/no or threshold-based questions about the features, and end up at a leaf with a prediction.\nThe tree is built by repeatedly asking:\nFor classification tasks, we want splits that make the groups as “pure” as possible (mostly one class).\nFor regression tasks, we want splits that minimize the variability within each group, often using mean squared error (MSE).\nAt each step, the algorithm chooses the best feature and split point, divides the data, and repeats until:\nIn the context of a decision tree, a node is pure if it contains only examples from a single class.\nFor example, suppose you’re predicting whether students pass or fail a project based on whether they attend class.\nStudent Attended Outcome\n1       A      Yes    Pass\n2       B      Yes    Pass\n3       C      Yes    Fail\n4       D       No    Fail\n5       E       No    Fail\n6       F       No    Pass\nStep 1: The Roote Node\nAt the root node, we have all 6 students: 3 “pass” and 3 “fail” (50% pure).\nIf a node has 20 students and all of them are “pass”, that node is 100% pure.\nSince the node has 3 “pass” and 3 “fail,” it’s very impure.\nA common measure of impurity is the Gini impurity, which quantifies how often a randomly chosen element would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n\\[\nGini = 1 - p(Pass)^2 + p(Fail)^2\n\\]\nor more generally,\n\\[  \nGini = 1 - \\sum (p_i)^2\n\\]\nStep 2: Splitting on Attendance\nNow let’s split our data based on attendance.\nGini_Yes   Gini_No \n0.4444444 0.4444444\nNow, after the split:\nBoth groups are still impure, but we have better separated the students based on attendance.\nOther common measures of impurity include:\nEntropy:\nMeasures the uncertainty in a dataset. Lower entropy means higher purity.\n\\[\n  Entropy = - \\sum p_i \\log_2(p_i)\n  \\] Misclassification Error:\nThe fraction of samples that do not belong to the majority class in a node.\n\\[\n  Misclassification\\ Error = 1 - \\max(p_i)\n  \\]\nLike the Gini coefficient, these measures help the algorithm decide the best splits to create purer nodes.\nAt each node, the algorithm evaluates all possible splits across all features and selects the one that results in the highest increase in purity (or the largest decrease in impurity).\nAt each step, a decision tree faces the question:\nTo answer this, it evaluates all possible splits and picks the one that produces the biggest increase in purity of the child nodes compared to the parent node.\nFor each candidate feature and possible split point:\n\\[\nG\\_{\\text{split}} = \\sum_{j=1}^{m} \\frac{N_j}{N} , G_j,\n\\] where,\nOur goal is to minimize this quantity \\(G_{\\text{split}}\\). The split that results in the lowest weighted impurity is chosen.\nFor every feature we can then measure the “goodness” of a split as the reduction in impurity\n\\[\n\\Delta G = G_{\\text{parent}} - G_{\\text{split}},\n\\] where, - \\(\\Delta G\\) is the reduction in impurity (information gain). - \\(G_{\\text{parent}}\\) is the impurity of the parent node. - \\(G_{\\text{split}}\\) is the weighted impurity after the split.\nDecision trees can grow very deep, creating many splits to perfectly classify the training data. However, this can lead to overfitting, where the model captures noise instead of the underlying pattern. To prevent this, we can set limits on the tree’s growth, such as:\nBy controlling the tree’s complexity, we can improve its ability to generalize to new, unseen data. This balance between fitting the training data and maintaining generalization is crucial for building effective decision tree models.\nTo prevent overfitting and ensure that the decision tree generalizes well to unseen data, we can implement early stopping criteria during the tree-building process. Common early stopping criteria include:\nMaximum Depth\nLimit the maximum depth of the tree. A shallower tree is less likely to overfit. For example, we may set a maximum depth of 5 or 10 levels. If we choose too shallow a tree, it may underfit the data, however, the variance will likely be lower. If we choose too deep a tree, it may overfit the data, but, the variance will likely be higher.\nMinimum Samples per Leaf\nWe can also set a minimum number of samples that must be present in a leaf node. This prevents the tree from creating leaves that are too specific to the training data.\nOn one hand we might allow for a single observation in a leaf, which would likely lead to overfitting. On the other hand, we might require at least 10 or 20 samples in each leaf, which could help the model generalize better.\nMinimum Impurity Decrease\nWe could also only allow a split if it results in a significant decrease in impurity. This ensures that splits are meaningful and contribute to the model’s predictive power.\nAnother approach to controlling tree complexity is pruning, which involves removing branches that do not provide significant predictive power.\nPruning can be done in two main ways:\nPost-Pruning\nThis involves growing the full tree and then removing branches that do not improve performance on a validation set. This can be done by evaluating the tree’s performance after removing certain branches and keeping the changes that lead to better generalization.\nPre-Pruning\nWe could also add a penalty for tree complexity to the loss function used to evaluate splits. This encourages the model to prefer simpler trees that still perform well. Smaller penalties will lead to larger trees, while larger penalties will lead to smaller trees.\nAs with regularization, we would evaluate multiple models across a spectrum of hyperparamaters (penalty parameters) and then use cross-validation to identify the optimal value that generalizes best to unseen data.\nIn the rpart package, which we will use later, the complexity parameter cp controls how large a decision tree can grow before it is pruned. It is the main regularization tool that balances tree complexity against generalizability.\nHow cp Works\nMathematically, the cost-complexity criterion is:\n\\[\nR_{alpha(T)} \\;=\\; R(T) + \\alpha |T|,\n\\] where,\nn rpart, decision trees are fit and pruned via cost–complexity (weakest-link) pruning. The optimization target is\n\\[\nC_{\\alpha}(T) = R(T) + \\alpha\\,|T|,\n\\]\nwhere,\nWhat is \\(R(T)\\)?\nThe impurity \\(R(T)\\) depends on the learning task; it is always computed as a sum over the leaves \\(t\\) of \\(T\\).\nRegression trees (method=\"anova\")\n\\[\nR(T)\n=\n\\sum_{t \\in \\mathrm{leaves}(T)}\n\\;\\sum_{i \\in t}\n\\bigl(y_i - \\bar{y}_t\\bigr)^2,\n\\qquad\n\\bar{y}_t = \\frac{1}{N_t}\\sum_{i \\in t} y_i.\n\\]\nRegression trees (method=\"class\")\n\\[\nR(T)\n=\n\\sum_{t \\in \\mathrm{leaves}(T)}\nN_t \\Bigl( 1 - \\max_{k} p_{tk} \\Bigr),\n\\qquad\np_{tk} = \\frac{1}{N_t}\\sum_{i \\in t} \\mathbf{1}\\{y_i = k\\}.\n\\]\nIn their standard form, decision trees have several limitations:\nOverfitting and Lack of Generalization\n- Trees can keep splitting until each leaf is pure, perfectly fitting the training data.\n- This often captures noise and leads to poor generalization.\nHigh Variance / Instability\n- Small changes in training data can lead to very different tree structures.\n- This instability makes them unreliable when used alone.\nBias Toward Features with Many Levels\n- Features with many unique categories (e.g., IDs, zip codes) can dominate splits.\n- The tree may overvalue these even when they’re not meaningful.\nGreedy, Locally Optimal Splitting\n- Trees choose the best split at each step, but this is only a local optimum.\n- They can miss better global solutions.\nIn R we can use the rpart package to create decision trees. The rpart package implements the widely popular CART (Classification and Regression Trees) algorithm for building decision trees.\nWe will use the following libraries:\nlibrary(rpart)\nlibrary(caret)\nlibrary(rsample)\nlibrary(tidyverse)\nlibrary(recipes)\nlibrary(rattle)\nlibrary(titanic)\nlibrary(psych)\nLet’s try to predict the variable OPN8 using a decision tree. Remember, OPN8 is a dichotomous variable indicating whether a respondent strongly agrees (1) or does not strongly agree (0) with the statement “I use difficult words.” We will use the same data preprocessing steps as before and predict our outcome using all the personality items included in our example data.\ndata &lt;- read.csv(\"data/data-final-shuffled.csv\")\ndichotomous_vars &lt;- colnames(data)[sapply(data, max) == 1]\ncharacter_vars   &lt;- colnames(data)[sapply(data, class) == \"character\"]\ncat_vars         &lt;- c(character_vars,dichotomous_vars)\ndata[cat_vars ]  &lt;- lapply(data[cat_vars ], as.factor)\n\ntarget &lt;- \"OPN8\"\ndata &lt;- data[!is.na(data[,target]),]\n\nset.seed(123)\nsplit &lt;- initial_split(data, prop = 0.7, strata = target)\ndata_train  &lt;- training(split)\ndata_test   &lt;- testing(split)\nAgain, we can recreate our recipe from the in-class activity.\nformula_string &lt;- as.formula(paste(target, \"~ .\"))\n\nblueprint &lt;- recipe(formula_string, data = data_train) %&gt;%\n  step_impute_bag(all_predictors()) %&gt;%\n  step_dummy(all_factor_predictors(), one_hot = FALSE)\nNow, let’s fit a decision tree model using 10-fold cross-validation to tune the complexity parameter (cp). The cp parameter controls the size of the decision tree and helps prevent overfitting by penalizing more complex trees. Here, tuneLength specifies the number of different cp values to try during the tuning process.\ncv_model_1 &lt;- train(\n  blueprint,\n  data = data_train,\n  method = \"rpart\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneLength = 10\n)\nWe can summarize the model to see the results of the cross-validation and the best cp value found.\ncv_model_1\n\nCART \n\n699 samples\n 49 predictor\n  2 classes: '0', '1' \n\nRecipe steps: impute_bag, dummy \nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 629, 629, 629, 629, 629, 629, ... \nResampling results across tuning parameters:\n\n  cp           Accuracy   Kappa    \n  0.000000000  0.6666874  0.3251901\n  0.001618123  0.6666874  0.3251901\n  0.004854369  0.7024224  0.3952978\n  0.008090615  0.7195445  0.4312424\n  0.009708738  0.7152588  0.4236310\n  0.011326861  0.7166874  0.4261848\n  0.012297735  0.7166874  0.4261848\n  0.022653722  0.7253416  0.4517328\n  0.025889968  0.7196273  0.4447376\n  0.378640777  0.6093789  0.1565585\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.02265372.\nWe can also visualize the cross-validation results to see how the model’s performance varies with different values of the complexity parameter (cp). Here we can again see that the model achieves the best accuracy with a cp value of approximately 0.02.\nggplot(cv_model_1)\nTo look at the final model tree structure in text form we can use the following command.\ncv_model_1$finalModel\n\nn= 699 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 699 309 0 (0.5579399 0.4420601)  \n   2) OPN1&lt; 3.5 268  35 0 (0.8694030 0.1305970) *\n   3) OPN1&gt;=3.5 431 157 1 (0.3642691 0.6357309)  \n     6) OPN1&lt; 4.5 245 116 1 (0.4734694 0.5265306)  \n      12) EXT9_X1&lt; 0.5 156  70 0 (0.5512821 0.4487179) *\n      13) EXT9_X1&gt;=0.5 89  30 1 (0.3370787 0.6629213) *\n     7) OPN1&gt;=4.5 186  41 1 (0.2204301 0.7795699) *\nWe can also visualize the final decision tree using the fancyRpartPlot function from the rattle package. This function provides a clear and informative visualization of the decision tree structure, including the splits, nodes, and predicted classes.\nfancyRpartPlot(cv_model_1$finalModel)\nEach rounded square in the plot above represents a decision node, showing the feature used for the split, the threshold value, and the distribution of classes in that node. The leaves (terminal nodes) show the predicted class and the probability of each class.\nThe top number in each node is the predicted class, while the bottom number is the probability of that class. For example, in the root node, the model predicts class “0” with a probability of 0.56. This is simply the proportion of people who don’t like using fancy words in the training data.\nThe first split of the data occurs on the feature OPN1 (I have a rich vocabulary).\nIf a respondent (strongly) disagrees with this statement (OPN1 &lt; 3.5), they are directed to the left branch of the tree, forming a leaf. This leaf contains about 38% of the respondents and in this leaf 0.87 of respondents do not agree with the statement “I use difficult words” (OPN8 = 0).\nIf a respondent (strongly) agrees with this statement (OPN1 &gt; 3.5), they are directed to the right branch of the tree. In this node 0.64 of respondents agree with the statement “I use difficult words” (OPN8 = 0).\nThe right branch of the tree is then further split on the feature OPN1, separating respondents who very strongly agree with the statement “I have a rich vocabulary” (OPN1 &gt; 4.5) in the right branch, from those who are more neutral or slightly agree (3.5 &lt; OPN1 &lt; 4.5) in the left branch.\nThe right branch of the tree ends in a terminal node (leaf) where 0.78 of respondents strongly agree with the statement “I use difficult words” (OPN8 = 1).\nThe left branch of the tree is further split on the feature EXT9 (“I don’t mind being the center of attention”).\nThose who agree at all with the statement (EXT9 &lt; 0.5) are directed to the left branch, where 22% of the sample reside and the predicted response is 0 for “I use difficult words” with a probability of 0.55.\nThose who endorse (EXT9 &lt; 0.5) at all are directed to the right branch, where 13% of the sample remain and the predicted response is 1 for “I use difficult words” with a probability of 0.66.\n# Make predictions on the test set      \npred_class &lt;- predict(cv_model_1, data_train)\n\n# create confusion matrix\nconfusionMatrix(\n  data = relevel(pred_class, ref = \"1\"), \n  reference = relevel(data_train[,target], ref = \"1\")\n)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   1   0\n         1 204  71\n         0 105 319\n                                        \n               Accuracy : 0.7482        \n                 95% CI : (0.7143, 0.78)\n    No Information Rate : 0.5579        \n    P-Value [Acc &gt; NIR] : &lt; 2e-16       \n                                        \n                  Kappa : 0.4837        \n                                        \n Mcnemar's Test P-Value : 0.01287       \n                                        \n            Sensitivity : 0.6602        \n            Specificity : 0.8179        \n         Pos Pred Value : 0.7418        \n         Neg Pred Value : 0.7524        \n             Prevalence : 0.4421        \n         Detection Rate : 0.2918        \n   Detection Prevalence : 0.3934        \n      Balanced Accuracy : 0.7391        \n                                        \n       'Positive' Class : 1\nLet’s read in the Titanic training data from the titanic package and take a look at the data.\nThis package comes with data that is pre-split into training and test sets.\ntitanic_train &lt;- titanic::titanic_train\nHere, the features included in titanic_train:\nWe can get basic desciptives using the psych package.\ndescribe(titanic_train)\n\n            vars   n   mean     sd median trimmed    mad  min    max  range\nPassengerId    1 891 446.00 257.35 446.00  446.00 330.62 1.00 891.00 890.00\nSurvived       2 891   0.38   0.49   0.00    0.35   0.00 0.00   1.00   1.00\nPclass         3 891   2.31   0.84   3.00    2.39   0.00 1.00   3.00   2.00\nName*          4 891 446.00 257.35 446.00  446.00 330.62 1.00 891.00 890.00\nSex*           5 891   1.65   0.48   2.00    1.68   0.00 1.00   2.00   1.00\nAge            6 714  29.70  14.53  28.00   29.27  13.34 0.42  80.00  79.58\nSibSp          7 891   0.52   1.10   0.00    0.27   0.00 0.00   8.00   8.00\nParch          8 891   0.38   0.81   0.00    0.18   0.00 0.00   6.00   6.00\nTicket*        9 891 339.52 200.83 338.00  339.65 268.35 1.00 681.00 680.00\nFare          10 891  32.20  49.69  14.45   21.38  10.24 0.00 512.33 512.33\nCabin*        11 891  18.63  38.14   1.00    8.29   0.00 1.00 148.00 147.00\nEmbarked*     12 891   3.53   0.80   4.00    3.66   0.00 1.00   4.00   3.00\n             skew kurtosis   se\nPassengerId  0.00    -1.20 8.62\nSurvived     0.48    -1.77 0.02\nPclass      -0.63    -1.28 0.03\nName*        0.00    -1.20 8.62\nSex*        -0.62    -1.62 0.02\nAge          0.39     0.16 0.54\nSibSp        3.68    17.73 0.04\nParch        2.74     9.69 0.03\nTicket*      0.00    -1.28 6.73\nFare         4.77    33.12 1.66\nCabin*       2.09     3.07 1.28\nEmbarked*   -1.27    -0.16 0.03\nNow, let’s fit our regression tree model to the Titanic data. We will predict the Survived variable using all other features in the dataset.\ntitanic_train$PassengerId &lt;- NULL\ntitanic_train$Name &lt;- NULL\ntitanic_train$Ticket &lt;- NULL\ntitanic_train$Cabin &lt;- NULL\ntitanic_train$Survived &lt;- as.factor(titanic_train$Survived)\ntitanic_train$Sex &lt;- as.factor(titanic_train$Sex)\n\nblueprint_titanic &lt;- recipe(Survived ~., data = titanic_train) %&gt;%\n  step_impute_bag(all_predictors()) %&gt;%\n  step_dummy(all_factor_predictors(), one_hot = FALSE) \n\ncv_model_titanic &lt;- train(\n  blueprint_titanic,\n  data = titanic_train,\n  method = \"rpart\",\n  trControl = trainControl(method = \"cv\", number = 10),\n  tuneLength = 20\n)\nggplot(cv_model_titanic)\nWe can also visualize the final decision tree using the fancyRpartPlot function from the rattle package.\nfancyRpartPlot(cv_model_titanic$finalModel)\nNow, let’s evaluate the model’s performance using cross-validation results and a confusion matrix.\n# Make predictions on the test set      \npred_class &lt;- predict(cv_model_titanic, titanic_train)\n\n# create confusion matrix\nconfusionMatrix(\n  data = relevel(pred_class, ref = \"1\"), \n  reference = relevel(titanic_train[,\"Survived\"], ref = \"1\")\n)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   1   0\n         1 271  48\n         0  71 501\n                                          \n               Accuracy : 0.8664          \n                 95% CI : (0.8423, 0.8881)\n    No Information Rate : 0.6162          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.714           \n                                          \n Mcnemar's Test P-Value : 0.04372         \n                                          \n            Sensitivity : 0.7924          \n            Specificity : 0.9126          \n         Pos Pred Value : 0.8495          \n         Neg Pred Value : 0.8759          \n             Prevalence : 0.3838          \n         Detection Rate : 0.3042          \n   Detection Prevalence : 0.3580          \n      Balanced Accuracy : 0.8525          \n                                          \n       'Positive' Class : 1",
    "crumbs": [
      "Tree Methods",
      "Decision Trees"
    ]
  },
  {
    "objectID": "week5_1.html#implementation-in-r",
    "href": "week5_1.html#implementation-in-r",
    "title": "Decision Trees",
    "section": "Implementation in R",
    "text": "Implementation in R",
    "crumbs": [
      "Tree Methods",
      "Decision Trees"
    ]
  }
]