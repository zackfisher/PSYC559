[
  {
    "objectID": "week1_1b.html",
    "href": "week1_1b.html",
    "title": "R Packages",
    "section": "",
    "text": "One of the main reasons to be excited about R is the package library system. In R, packages are like apps. R packages extend the functionality of base R by providing additional functions, data, and documentation. They are written by a worldwide community of R users and can be downloaded freely without much hassle (compared to many other package libraries). For example, we will often use the ggplot2 package for plotting and visualizing data and the psych package for describing data numerically.\nTo install R packages you can use the install.packages() function directly in the console as follows:\ninstall.packages(\"ggplot2\")\nor you can do so from the Packages tab of the Files pane in RStudio using the following steps:\nOnce a package is installed you can load it into your environment using the library() function. Once loaded you have access to all the R functions supplied by that package.\nlibrary(\"ggplot2\")\nThere are many ways to find useful R packages on the internet simply by googling the type of data or analysis you are interested in. Another way is to look at the CRAN Task Views. For example, there is an interesting Task View for Machine Learning that contains many relevant packages you might be interested in using. Some class members expressed interest in F1 racing. In the Sports Analytics Task View there is a f1dataR package that provides historical data from the beginning of Formula 1.",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "week1_1b.html#using-r-packages",
    "href": "week1_1b.html#using-r-packages",
    "title": "R Packages",
    "section": "Using R Packages",
    "text": "Using R Packages",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "week1_1b.html#finding-useful-r-packages",
    "href": "week1_1b.html#finding-useful-r-packages",
    "title": "R Packages",
    "section": "Finding Useful R Packages",
    "text": "Finding Useful R Packages",
    "crumbs": [
      "Introduction to R",
      "R Packages"
    ]
  },
  {
    "objectID": "week2_2b.html#examples-items-instances-or-cases",
    "href": "week2_2b.html#examples-items-instances-or-cases",
    "title": "Terminology Notes",
    "section": "Examples, Items, Instances or Cases",
    "text": "Examples, Items, Instances or Cases\n\nDefinition: Individual data points used for learning or evaluation.\n\nExample: Each participant in a study, with their responses to a cognitive task or survey questionnaire.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Terminology Notes"
    ]
  },
  {
    "objectID": "week2_2b.html#features",
    "href": "week2_2b.html#features",
    "title": "Terminology Notes",
    "section": "Features",
    "text": "Features\n\nDefinition: Attributes or variables associated with each example, often represented as a vector.\n\nExample: Participant age, gender, response times, scores on survey items, or physiological measures (e.g., heart rate, skin conductance).",
    "crumbs": [
      "Introduction to Machine Learning",
      "Terminology Notes"
    ]
  },
  {
    "objectID": "week2_2b.html#labels",
    "href": "week2_2b.html#labels",
    "title": "Terminology Notes",
    "section": "Labels",
    "text": "Labels\n\nDefinition: The outcome, value, or category assigned to each example.\n\nExample:\n\nClassification: Diagnosed vs. non-diagnosed for depression.\n\nRegression: Continuous depression score on a scale from 0–30.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Terminology Notes"
    ]
  },
  {
    "objectID": "week2_2b.html#training-sample",
    "href": "week2_2b.html#training-sample",
    "title": "Terminology Notes",
    "section": "Training Sample",
    "text": "Training Sample\n\nDefinition: The subset of examples used to train the learning algorithm.\n\nExample: 100 participants’ data with known depression scores used to train a predictive model.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Terminology Notes"
    ]
  },
  {
    "objectID": "week2_2b.html#validation-sample",
    "href": "week2_2b.html#validation-sample",
    "title": "Terminology Notes",
    "section": "Validation Sample",
    "text": "Validation Sample\n\nDefinition: Examples used to tune algorithm parameters (e.g., regularization strength, number of neurons in a network).\n\nExample: 30 participants’ data used to select the optimal model settings or tuning parameters before final evaluation.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Terminology Notes"
    ]
  },
  {
    "objectID": "week2_2b.html#test-sample",
    "href": "week2_2b.html#test-sample",
    "title": "Terminology Notes",
    "section": "Test Sample",
    "text": "Test Sample\n\nDefinition: Examples used to evaluate model performance, separate from training and validation. Examples used to evaluate the performance of a learning algorithm. The test sample is not made available in the learning stage.\nExample: 50 new participants whose depression scores the model predicts; predicted scores are compared to actual scores to assess accuracy.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Terminology Notes"
    ]
  },
  {
    "objectID": "week2_2b.html#loss-function",
    "href": "week2_2b.html#loss-function",
    "title": "Terminology Notes",
    "section": "Loss Function",
    "text": "Loss Function\n\nDefinition: A function that measures the difference between predicted labels and true labels.\n\nExample:\nRegression: Squared loss: L(y, y′) = (y′ − y)², e.g., predicted vs. actual depression score.\n\nClassification: Zero-one loss: L(y, y′) = 1 if prediction ≠ true label, e.g., predicted diagnosis vs. actual diagnosis.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Terminology Notes"
    ]
  },
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "PSYC559: Applied Machine Learning in Psychology",
    "section": "",
    "text": "https://zackfisher.github.io/PSYC559/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSC559: Applied Machine Learning in Psychology",
    "section": "",
    "text": "Welcome to the PSC559: Applied Machine Learning in Psychology!\nAs the class progresses I will update this front matter with a general course overview and important announcements.\nIf you find errors or typos in any of these materials please let me know."
  },
  {
    "objectID": "week2_1.html",
    "href": "week2_1.html",
    "title": "Important Terms and Distinctions",
    "section": "",
    "text": "The amount of jargon one finds when first diving into the machine learning literature is a common source of confusion for new learners. Many ML terms are borrowed from statistics, computer science, or everyday language, but they can carry subtly or even radically different meanings in ML contexts.\nIn the following overview I will provide a high-level summary of some common terms used in machine learning, and try to highlight the differences between concepts that may seem similar, but often represent distinct ideas.\nTwo important distinction I will try to highlight are the differences between:\nArtificial Intelligence (AI) and Machine Learning (ML) are related but distinct concepts in computer science and data analysis.\nML approaches rely heavily on accurate and efficient prediction algorithms.\nSo, what exactly are algorithms?\nAn algorithm is a step-by-step, well-defined procedure for solving a problem or completing a task.\nIn computing and machine learning, algorithms are generally a finite sequence of instructions that takes some input, follows a set of rules, and produces an output.\nA model in machine learning is the learned representation of patterns in data, produced by applying a learning algorithm to a dataset.\nA model in ML is the vehicle to make predictions, classify new examples, or infer relationships.\nUnderstanding what algorithm to choose for a specific problem or application is often difficult. In this class we will devote a lot of time on how to make these decisions, and communicate the results of an analysis. That said, we often have minimal knowledge of the problem or data at hand when we first approach an applied problem, and it is difficult to know which ML method will perform best. This idea is generally known as the no free lunch theorem\nAn algorithm is just a set of step-by-step instructions to solve a problem. Machine learning algorithms are no different—they’re just systematic ways of finding patterns in data. Let’s explore what that means.\nIn groups of 3–4, imagine you are writing an algorithm for a simple real-world problem:\nEach group should discuss (and prepare to share):",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#artificial-intelligence-ai",
    "href": "week2_1.html#artificial-intelligence-ai",
    "title": "Important Terms and Distinctions",
    "section": "Artificial Intelligence (AI)",
    "text": "Artificial Intelligence (AI)\n\nAI is the broad field of creating systems that can perform tasks that normally require human intelligence.\nTypically, the goal of AI is to create programs that can simulate intelligent behavior, whether or not they learn from data.\nCommon Examples of AI include:\n\nDigital assistants, LLMs\n\n\nCan you think of any examples of AI you encounter in your everyday life?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#machine-learning-ml",
    "href": "week2_1.html#machine-learning-ml",
    "title": "Important Terms and Distinctions",
    "section": "Machine Learning (ML)",
    "text": "Machine Learning (ML)\n\nML is a branch of AI focused on systems that learn patterns from data and improve automatically from experience.\nWhen doing ML we are generally interested in creating models that generalize well enough to make accurate predictions on unseen inputs.\nCommon use cases for ML in everyday life include:\n\nRecommendation systems, image recognition\n\n\nCan you think of any examples of ML you encounter in your everyday life?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#differences-between-ai-and-ml",
    "href": "week2_1.html#differences-between-ai-and-ml",
    "title": "Important Terms and Distinctions",
    "section": "Differences Between AI and ML",
    "text": "Differences Between AI and ML\n\n\n\n\n\n\n\n\n\n\nAspect\nArtificial Intelligence (AI)\nMachine Learning (ML)\n\n\n\n\nFocus\nSimulating intelligent behavior\nLearning patterns from data\n\n\nGoal\nBroader human-like intelligence\nSpecific predictive or decision-making tasks\n\n\nExamples\nChess engines, self-driving cars, expert systems\nRegression, neural networks, clustering\n\n\n\nBroadly speaking, ML is a branch of AI focused on training models to recognize patterns and make predictions using algorithms. AI encompasses a broader set of techniques, some of which do not involve learning from data at all.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#characteristics-of-an-algorithm",
    "href": "week2_1.html#characteristics-of-an-algorithm",
    "title": "Important Terms and Distinctions",
    "section": "Characteristics of an Algorithm",
    "text": "Characteristics of an Algorithm\n\nInput – Data the algorithm operates on\nOutput – The result produced after execution\nFiniteness – Must finish in a finite number of steps\nDefiniteness – Each step is clear and unambiguous\nEffectiveness – Each operation can actually be carried out",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#algorithms-in-machine-learning",
    "href": "week2_1.html#algorithms-in-machine-learning",
    "title": "Important Terms and Distinctions",
    "section": "Algorithms in Machine Learning",
    "text": "Algorithms in Machine Learning\n\nIn ML, an algorithm is the procedure used to train a model from data.\nExample: Linear regression algorithm finds the best-fit line by minimizing error.\nExample: Decision tree algorithm splits data into branches by checking features step by step.\n\nOne can think of an algorithm as the recipe one follows when cooking a meal. The ingredients of the dish are analogous to the input data, and the finished dish is equivalent to the model.\n\n\n\nhttps://xkcd.com/1667/\n\n\nSo, how does an algorithm differ from a model? What is a model in the context of ML?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#definition",
    "href": "week2_1.html#definition",
    "title": "Important Terms and Distinctions",
    "section": "Definition",
    "text": "Definition\nA model is the outcome of training a machine learning algorithm on data.It encapsulates the patterns, relationships, or structure discovered in the training data.\n\nInput: Features from the training data\n\nProcess: Algorithm (e.g., linear regression, decision tree, neural network)\n\nOutput: Parameters, structure, or rules that allow prediction",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#examples",
    "href": "week2_1.html#examples",
    "title": "Important Terms and Distinctions",
    "section": "Examples",
    "text": "Examples\n\n\n\n\n\n\n\n\nAlgorithm\nResulting Model\nWhat It Represents\n\n\n\n\nLinear Regression\nA line (y = mx + b)\nRelationship between input and output variables\n\n\nDecision Tree\nTree of splits\nHow features split to predict classes or values\n\n\nNeural Network\nLayers of neurons & weights\nComplex non-linear mappings between inputs and outputs\n\n\nk-Means Clustering\nCluster centroids\nGrouping of data points into clusters",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_1.html#things-to-remember",
    "href": "week2_1.html#things-to-remember",
    "title": "Important Terms and Distinctions",
    "section": "Things to Remember",
    "text": "Things to Remember\n\nThe algorithm is the procedure for learning\n\nThe model is the trained artifact used for prediction or inference\nDifferent algorithms can produce different models from the same data\nA model generalizes patterns from training data to unseen data\nAlgorithm = recipe (instructions for learning)\nModel = finished dish (learned representation ready to use)\n\nThe model is the end product of learning — the part that actually “knows” something about the data and can be used to make predictions. In ML: Data + Algorithm → Model → Predictions.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Important Terms and Distinctions"
    ]
  },
  {
    "objectID": "week2_2.html",
    "href": "week2_2.html",
    "title": "Problem Classes in ML",
    "section": "",
    "text": "Machine learning applications correspond to a wide variety of learning problems. Some major classes include:\nWe will now complete a short in-class exercise demonstrating how one might build a binary classification problem.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#classification",
    "href": "week2_2.html#classification",
    "title": "Problem Classes in ML",
    "section": "Classification",
    "text": "Classification\n\nThe goal of classification is often toassign a category or label to each observation.\nThe music genre problem is a good example of a classification problem\nNote: The number of categories can be small, large, or even unbounded (e.g., optical character recognition, text classification, speech recognition).\n\n\n\n\nhttps://datahacker.rs/008-machine-learning-multiclass-classification-and-softmax-function/\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve classification problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#regression",
    "href": "week2_2.html#regression",
    "title": "Problem Classes in ML",
    "section": "Regression",
    "text": "Regression\n\nIn regression problems we are typically concerned with predicting a real-valued number for each item.\nFor example, we might be interested in predicting stock prices, or how .\nstressed out someone is.\nNotes: The penalty for prediction errors depends on the magnitude of the difference between true and predicted values, unlike classification where categories are discrete and there is often no notion of distance between various categories.\n\n\n\nhttps://builtin.com/data-science/regression-machine-learning\n\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve regression-type problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#ranking",
    "href": "week2_2.html#ranking",
    "title": "Problem Classes in ML",
    "section": "Ranking",
    "text": "Ranking\n\nWith ranking problems we are often tasked with ordering items according to a specific criterion.\nA well-known example of a ranking algorithm is Google’s PageRank algorithm.\nNotes: Ranking problems focus on relative order rather than exact category or numeric value.\n\n\n\n\nhttps://towardsdatascience.com/wp-content/uploads/2023/08/1ZFmd2Q-G95ArY93od20Adw.png\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve ranking problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#clustering",
    "href": "week2_2.html#clustering",
    "title": "Problem Classes in ML",
    "section": "Clustering",
    "text": "Clustering\n\nIn clustering problems we are typically looking topartition items into similar groups or regions.\nFor example, in social network analysis we are interested in identifying communities within large groups of people\nNotes: Clustering is often applied to very large datasets to reveal underlying structure.\n\n\n\n\nhttps://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png\n\n\nHow do the classification and clustering problems discussed thus far differ?\nCan you think of any examples of algorithms you encounter in your everyday life that solve clustering problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week2_2.html#dimensionality-reduction",
    "href": "week2_2.html#dimensionality-reduction",
    "title": "Problem Classes in ML",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\n\nWhen looking to solve dimension reduction problems we are typically interested in transforming high-dimensional data into a lower-dimensional representation while preserving important properties.\nCommon examples in psychology include identifying constructs in survey data or scales.\nNotes: Dimension reduction is also useful for feature extraction, noise reduction, and speeding up downstream learning tasks.\n\n\n\n\nhttps://www.sthda.com/english/sthda-upload/figures/principal-component-methods/006-principal-component-analysis-pca-variable-cos2-corrplot-1.png\n\n\nCan you think of any examples of algorithms you encounter in your everyday life that solve dimension reduction problems?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Problem Classes in ML"
    ]
  },
  {
    "objectID": "week1_1.html",
    "href": "week1_1.html",
    "title": "Basic R Programming",
    "section": "",
    "text": "Before getting started we will need to install R and RStudio.",
    "crumbs": [
      "Introduction to R",
      "Basic R Programming"
    ]
  },
  {
    "objectID": "week1_3.html",
    "href": "week1_3.html",
    "title": "Describing Data Numerically",
    "section": "",
    "text": "Once you have read in a data frame it can be useful to know how the variables are understood by R. For example, let’s look at some Kaggle Marketing Analytics Data. You can download the raw data here.",
    "crumbs": [
      "Introduction to R",
      "Describing Data Numerically"
    ]
  },
  {
    "objectID": "week1_2.html",
    "href": "week1_2.html",
    "title": "Reading in Data",
    "section": "",
    "text": "One of the most important initial tasks you’ll face in R is reading in data. Let’s walk through some basics.\nBy reading in data we are generally refering to the process of importing data from a local directory on your computer, or from the web, into R. When we read a data file into R, we often read it in as a tabularobject where columns representing variables and rows representing cases. This is not always the case but covers the most basic use case.\nMany different data file formats can be read into R as data frames, such as .csv (comma separated values), .xlsx (Excel workbook), .txt (text), .sas7bdat (SAS), and .sav (SPSS) can be read into R. We will mostly focus on the .csv case in this class.",
    "crumbs": [
      "Introduction to R",
      "Reading in Data"
    ]
  },
  {
    "objectID": "week1_4.html",
    "href": "week1_4.html",
    "title": "Describing Data Visually",
    "section": "",
    "text": "This section will briefly introduce you to visualizing data using the ggplot2 package. R has a number of systems for making graphs but ggplot2 is by far the most developed option. ggplot2 implements the grammar of graphics, and if you’d like to learn more about the motivation for this work take a look at The Layered Grammar of Graphics.",
    "crumbs": [
      "Introduction to R",
      "Describing Data Visually"
    ]
  },
  {
    "objectID": "week2_3.html",
    "href": "week2_3.html",
    "title": "Data Splitting",
    "section": "",
    "text": "Data splitting (e.g., dividing data into training, validation, and test samples) is crucial because it protects us from fooling ourselves about how well a model actually performs.\n\n\n\nThe subset of examples used to train the learning algorithm. Here we typically estimate the model’s parameters in the training sample.\nFor example, if we have data from 150 participants, we might keep data from 100 participants (with known depression scores) to train a predictive model.\nNotes: Spending too much in training won’t allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting).\n\n\n\n\n\nSamples used to tune algorithm hyperparameters (e.g., regularization strength, number of neurons in a network).\nFor example, from our 100 individuals in the training sample, we may split the training data into 4 groups of 25 participants. Each of these will be a validation sample, and allow us to select the optimal model settings or tuning parameters before our final evaluation.\n\n\n\n\n\nThe test Sample used to evaluate model performance, separate from training and validation. The test sample is not made available in the learning stage. Sometimes too much spent in testing won’t allow us to get a good assessment of model parameters.\nFor example, for those 50 participants we never looked out may be our test data. The model can now be used to predict depression scores ; predicted scores for the individuals in the test sample. Here we can now compare actual scores to assess accuracy.\n\n\n\n\nhttps://www.statology.org/validation-set-vs-test-set/\nIn groups of 3–4, discuss:\nSo, what does all this data splitting mean for us in practice? Let’s take a look at one of the more popular data-splitting methods used in practice: K-Folds Cross-Validation",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#parameters-vs.-hyperparameters",
    "href": "week2_3.html#parameters-vs.-hyperparameters",
    "title": "Data Splitting",
    "section": "Parameters vs. Hyperparameters",
    "text": "Parameters vs. Hyperparameters\n\nParameters are values learned automatically (or estimated) from the training data. The model parameters are often the thing we are interested in estimating in a given algorithm.\nFor example. the slope of the line in a simple linear regression prediction problem relating amount of treatment (feature) to depression score (target).\nHyperparameters are set before training begins, and control how the learning process works. The selection of hyperparameters is often critical to model performance, as in procedures like k-folds cross-validation, playing a critical role in how the model generalizes.\nFor example, the the number of clusters used to partition the data",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#role-of-parameters-in-data-splitting",
    "href": "week2_3.html#role-of-parameters-in-data-splitting",
    "title": "Data Splitting",
    "section": "Role of Parameters in Data Splitting",
    "text": "Role of Parameters in Data Splitting\nWhen we split data, each portion plays a distinct role:\n\nTraining set → Fit the model parameters.\n\nValidation set → Used to compare different hyperparameter choices.\n\nTest set → Used only once, at the very end, to estimate generalization performance.\n\nImportant note: Hyperparameters should be tuned using the validation set, not the test set. If we adjust hyperparameters based on the test set, we are indirectly training on it and lose the ability to measure real-world performance.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#example-k-means-clustering",
    "href": "week2_3.html#example-k-means-clustering",
    "title": "Data Splitting",
    "section": "Example: k-Means Clustering",
    "text": "Example: k-Means Clustering\n\nParameters (learned from the data)\n\nCluster centroids: the coordinates of the cluster centers.\n\nThese are calculated by the algorithm during training and adjusted iteratively until convergence.\n\nYou do not set them manually — the algorithm figures them out.\n\n\n\nHyperparameters (set before training)\n\nNumber of clusters (k): chosen by the user before running the algorithm.\n\nInitialization method (e.g., random, k-means++).\n\nMaximum number of iterations allowed.\n\nThese control how the algorithm runs, but are not learned from the data itself.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#prevents-overfitting-to-the-training-data",
    "href": "week2_3.html#prevents-overfitting-to-the-training-data",
    "title": "Data Splitting",
    "section": "Prevents Overfitting to the Training Data",
    "text": "Prevents Overfitting to the Training Data\n\nWhen a model is trained, it adapts to patterns in the training set.\n\nIf we only check performance on that same data, we might think the model is excellent — but it may just be memorizing instead of generalizing.\n\nA separate test set lets us see how the model behaves on new, unseen data.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#simulates-real-world-performance",
    "href": "week2_3.html#simulates-real-world-performance",
    "title": "Data Splitting",
    "section": "Simulates Real-World Performance",
    "text": "Simulates Real-World Performance\n\nIn real life, the model will be applied to data it hasn’t seen before.\n\nBy holding out a test set, we simulate that scenario, giving us a better sense of expected performance in practice.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#helps-with-model-selection-tuning",
    "href": "week2_3.html#helps-with-model-selection-tuning",
    "title": "Data Splitting",
    "section": "Helps with Model Selection & Tuning",
    "text": "Helps with Model Selection & Tuning\n\nA validation set (or cross-validation) is used to choose hyperparameters (like tree depth, learning rate, regularization strength).\n\nIf we tuned on the test set, we’d essentially be “leaking” information, and performance estimates would be biased upward.\n\nProper splitting ensures that the test set remains untouched until the very end.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#detects-data-leakage",
    "href": "week2_3.html#detects-data-leakage",
    "title": "Data Splitting",
    "section": "Detects Data Leakage",
    "text": "Detects Data Leakage\n\nSometimes information from the future or from labels sneaks into the features.\n\nIf this happens, the model might look perfect on the training set but fail on a clean split.\n\nData splitting may not actually help data leakage though. When might it help and when might it not?",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_3.html#provides-a-fair-benchmark",
    "href": "week2_3.html#provides-a-fair-benchmark",
    "title": "Data Splitting",
    "section": "Provides a Fair Benchmark",
    "text": "Provides a Fair Benchmark\n\nIn research and industry, different models need to be compared on a common, untouched test set.\n\nWithout splitting, comparisons aren’t meaningful, since each model might overfit differently.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Data Splitting"
    ]
  },
  {
    "objectID": "week2_4.html",
    "href": "week2_4.html",
    "title": "Bias and Variance",
    "section": "",
    "text": "Prediction errors can be broken down into two main parts: bias and variance.\nThese represent different sources of mistakes a model can make.\nUsually, there’s a tradeoff between keeping bias low and keeping variance low. It can be exceptionally hard to minimize both at the same time.\nBy understanding where bias and variance come from, we can make better choices when building models and end up with more accurate predictions.\nLet’s approach each concept in turn.\nBias is the difference between the average prediction from our model and the true value which we are trying to predict.\nIn machine learning, bias often refers to the systematic error in a model.\nBias in ML can lead to incorrect or unfair outcomes, possibly from flawed or incomplete training data, or perhaps from poor assumptions made by the algorithm.\nA biased model fails to accurately reflect the true relationship in the data, leading to poor generalization and skewed predictions.\nIn statistics, variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value.\nIn machine learning, variance typically refers to the sensitivity of a model’s predictions to small changes in the training data.\nSo, error due to variance is defined as the variability of a model prediction for a given data point.\nBias and variance are often introduced together in the context of the bias-variance tradeoff that occurs when models overfit, or underfit the data.\nWhen a model overfits the data, it means the model has learned too much from the training data, including random noise or minor fluctuations, rather than just the true underlying patterns.\nWhen a model underfits the data, it means the model is too simple to capture the underlying patterns in the data.",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_4.html#variance-in-machine-learning",
    "href": "week2_4.html#variance-in-machine-learning",
    "title": "Bias and Variance",
    "section": "Variance in Machine Learning",
    "text": "Variance in Machine Learning",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "week2_4.html#the-bias-variance-tradeoff",
    "href": "week2_4.html#the-bias-variance-tradeoff",
    "title": "Bias and Variance",
    "section": "The Bias Variance Tradeoff",
    "text": "The Bias Variance Tradeoff",
    "crumbs": [
      "Introduction to Machine Learning",
      "Bias and Variance"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "PSYC559: APPLIED MACHINE LEARNING IN PSYCHOLOGY",
    "section": "",
    "text": "Welcome to Applied Machine Learning in Psychology.",
    "crumbs": [
      "Table of Contents"
    ]
  },
  {
    "objectID": "introduction.html#the-syllabus",
    "href": "introduction.html#the-syllabus",
    "title": "PSYC559: APPLIED MACHINE LEARNING IN PSYCHOLOGY",
    "section": "The Syllabus",
    "text": "The Syllabus\nStudents may review the syllabus on Canvas.",
    "crumbs": [
      "Table of Contents"
    ]
  }
]